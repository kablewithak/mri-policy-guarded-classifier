{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5962731,"sourceType":"datasetVersion","datasetId":3419493},{"sourceId":6568541,"sourceType":"datasetVersion","datasetId":3531266},{"sourceId":6959730,"sourceType":"datasetVersion","datasetId":3997912},{"sourceId":7663855,"sourceType":"datasetVersion","datasetId":4469006},{"sourceId":9004923,"sourceType":"datasetVersion","datasetId":5424936},{"sourceId":14816676,"sourceType":"datasetVersion","datasetId":9475143},{"sourceId":14832123,"sourceType":"datasetVersion","datasetId":1608934},{"sourceId":14832371,"sourceType":"datasetVersion","datasetId":9486246},{"sourceId":14837364,"sourceType":"datasetVersion","datasetId":9475450}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# MRI Phase 2 — Training Baseline v1 (Kaggle)\n\nThis notebook trains a **baseline 4‑class brain MRI tumor classifier** using CSV split artifacts created in the Data Phase.\n\n## How to run (important)\n\n1) Attach inputs in Kaggle (right panel → **Add Input**):\n- The raw datasets (image folders + any OOD/non-tumor challenge datasets you used)\n- The split artifacts dataset (e.g. `mri-data-artifacts-v1`) containing:\n  - `split_train_images.csv`, `split_val_images.csv`, `split_test_images.csv`\n  - `split_external_test_npz.csv`\n  - `split_challenge_sampled.csv`\n\n2) **Turn on GPU right before training**\n- Settings → Accelerator → **GPU**\n- Restart session\n- Run top → bottom.\n\nIf you run on CPU, it will work but will be slow.","metadata":{}},{"cell_type":"code","source":"# CELL: 00_ARTIFACTS_PATHS - Resolve split CSV paths from Kaggle inputs\n\nfrom pathlib import Path\n\n# --- 1) Helpful visibility: what is actually mounted? ---\nINPUT_ROOT = Path(\"/kaggle/input\")\nprint(\"Mounted under /kaggle/input:\")\nif INPUT_ROOT.exists():\n    for p in sorted(INPUT_ROOT.iterdir()):\n        if p.is_dir():\n            print(\" -\", p.name)\n\n# --- 2) Candidate locations (explicit + fallback search) ---\nCANDIDATES = [\n    Path(\"/kaggle/working/data_artifacts\"),\n    Path(\"/kaggle/input/mri-data-artifacts-v1/data_artifacts\"),  # <-- explicit artifacts dataset path\n    Path(\"/kaggle/input\"),  # fallback: search under mounted inputs\n]\n\ndef find_file(name: str) -> Path | None:\n    # Prefer direct hits first\n    for base in CANDIDATES:\n        if not base.exists():\n            continue\n        direct = base / name\n        if direct.exists():\n            return direct\n\n    # Fallback: recursive search (can be slow, so last resort)\n    for base in CANDIDATES:\n        if not base.exists():\n            continue\n        hits = list(base.rglob(name))\n        if hits:\n            hits = sorted(hits, key=lambda p: (len(p.parts), str(p)))\n            return hits[0]\n    return None\n\nsplit_train = find_file(\"split_train_images.csv\")\nsplit_val   = find_file(\"split_val_images.csv\")\nsplit_test  = find_file(\"split_test_images.csv\")\nsplit_ext   = find_file(\"split_external_test_npz.csv\")\nsplit_chal  = find_file(\"split_challenge_sampled.csv\")\n\nprint(\"\\nResolved split paths:\")\nprint(\"split_train:\", split_train)\nprint(\"split_val:  \", split_val)\nprint(\"split_test: \", split_test)\nprint(\"split_ext:  \", split_ext)\nprint(\"split_chal: \", split_chal)\n\nassert all([split_train, split_val, split_test, split_ext, split_chal]), (\n    \"Missing one or more split CSVs.\\n\"\n    \"Make sure the dataset 'kabomolefe/mri-data-artifacts-v1' (Version 2) is attached as an Input.\\n\"\n    \"Expected folder: /kaggle/input/mri-data-artifacts-v1/data_artifacts\"\n)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-14T15:43:30.340135Z","iopub.execute_input":"2026-02-14T15:43:30.340682Z","iopub.status.idle":"2026-02-14T15:51:43.278289Z","shell.execute_reply.started":"2026-02-14T15:43:30.340643Z","shell.execute_reply":"2026-02-14T15:51:43.277392Z"}},"outputs":[{"name":"stdout","text":"Mounted under /kaggle/input:\n - datasets\n - mri-demo-artifacts-bundle\n\nResolved split paths:\nsplit_train: /kaggle/input/datasets/kabomolefe/mri-data-artifacts-v1/data_artifacts/split_train_images.csv\nsplit_val:   /kaggle/input/datasets/kabomolefe/mri-data-artifacts-v1/data_artifacts/split_val_images.csv\nsplit_test:  /kaggle/input/datasets/kabomolefe/mri-data-artifacts-v1/data_artifacts/split_test_images.csv\nsplit_ext:   /kaggle/input/datasets/kabomolefe/mri-data-artifacts-v1/data_artifacts/split_external_test_npz.csv\nsplit_chal:  /kaggle/input/datasets/kabomolefe/mri-data-artifacts-v1/data_artifacts/split_challenge_sampled.csv\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# CELL: 01_CONFIG - Reproducibility, device selection, output dirs, env snapshot\n\nfrom pathlib import Path\nimport os, random, json\nimport numpy as np\nimport pandas as pd\nimport torch\n\nSEED = 42\n\ndef seed_everything(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    # Deterministic is slower but stable for baselines\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed_everything(SEED)\n\nUSE_CUDA = torch.cuda.is_available()\nDEVICE = \"cuda\" if USE_CUDA else \"cpu\"\nprint(\"DEVICE:\", DEVICE)\n\nOUT = Path(\"/kaggle/working/train_artifacts\")\nOUT.mkdir(parents=True, exist_ok=True)\nprint(\"OUT:\", OUT)\n\n# Environment snapshot (useful for anti-regression)\nenv = {\n    \"python\": os.sys.version,\n    \"torch\": torch.__version__,\n    \"cuda_available\": USE_CUDA,\n}\ntry:\n    import torchvision\n    env[\"torchvision\"] = torchvision.__version__\nexcept Exception as e:\n    env[\"torchvision\"] = f\"IMPORT_ERROR: {repr(e)}\"\n\nwith open(OUT/\"env_snapshot.json\", \"w\") as f:\n    json.dump(env, f, indent=2)\n\nprint(\"✅ wrote env snapshot:\", OUT/\"env_snapshot.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T15:52:04.493448Z","iopub.execute_input":"2026-02-14T15:52:04.493930Z","iopub.status.idle":"2026-02-14T15:52:04.510190Z","shell.execute_reply.started":"2026-02-14T15:52:04.493900Z","shell.execute_reply":"2026-02-14T15:52:04.509335Z"}},"outputs":[{"name":"stdout","text":"DEVICE: cpu\nOUT: /kaggle/working/train_artifacts\n✅ wrote env snapshot: /kaggle/working/train_artifacts/env_snapshot.json\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# RESTORE: copy trained model artifacts from the model-bundle dataset into OUT\n\nfrom pathlib import Path\nimport shutil\n\nOUT = Path(\"/kaggle/working/train_artifacts\")\nOUT.mkdir(parents=True, exist_ok=True)\n\nCANDIDATES = [\n    Path(\"/kaggle/input/mri-demo-artifacts-bundle/train_artifacts\"),\n    Path(\"/kaggle/input/mri-demo-artifacts-bundle\"),\n    Path(\"/kaggle/input/datasets/mri-demo-artifacts-bundle/train_artifacts\"),\n    Path(\"/kaggle/input/datasets/mri-demo-artifacts-bundle\"),\n]\n\nsrc = None\nfor c in CANDIDATES:\n    if c.exists():\n        # accept either direct train_artifacts folder or dataset root containing train_artifacts\n        if c.name == \"train_artifacts\":\n            src = c\n            break\n        if (c / \"train_artifacts\").exists():\n            src = c / \"train_artifacts\"\n            break\n\nprint(\"Model artifact candidates:\")\nfor c in CANDIDATES:\n    print(\" -\", c, \"exists=\", c.exists())\n\nassert src is not None, \"Could not find model artifacts dataset mount. Make sure 'mri-demo-artifacts-bundle' is attached as Input.\"\n\nprint(\"Using:\", src)\n\n# copy everything (idempotent)\nfor p in src.glob(\"*\"):\n    if p.is_file():\n        shutil.copy2(p, OUT / p.name)\n\nprint(\"Restored into:\", OUT)\nprint(\"Files now in OUT:\")\nfor p in sorted(OUT.iterdir()):\n    if p.is_file():\n        print(\" -\", p.name, f\"{p.stat().st_size/1e6:.2f}MB\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T15:54:49.883748Z","iopub.execute_input":"2026-02-14T15:54:49.884095Z","iopub.status.idle":"2026-02-14T15:54:51.841740Z","shell.execute_reply.started":"2026-02-14T15:54:49.884065Z","shell.execute_reply":"2026-02-14T15:54:51.840820Z"}},"outputs":[{"name":"stdout","text":"Model artifact candidates:\n - /kaggle/input/mri-demo-artifacts-bundle/train_artifacts exists= True\n - /kaggle/input/mri-demo-artifacts-bundle exists= True\n - /kaggle/input/datasets/mri-demo-artifacts-bundle/train_artifacts exists= False\n - /kaggle/input/datasets/mri-demo-artifacts-bundle exists= False\nUsing: /kaggle/input/mri-demo-artifacts-bundle/train_artifacts\nRestored into: /kaggle/working/train_artifacts\nFiles now in OUT:\n - best_model.pth 44.79MB\n - calib_test_logits.pt 0.05MB\n - calib_val_logits.pt 0.05MB\n - calibration_metrics.json 0.00MB\n - calibration_summary_before.csv 0.00MB\n - calibration_summary_before_after.csv 0.00MB\n - challenge_conf_by_pred.csv 0.00MB\n - challenge_confidence_summary.csv 0.00MB\n - challenge_policy_outputs.csv 0.85MB\n - challenge_policy_outputs_demo.csv 1.13MB\n - challenge_policy_outputs_v2.csv 0.95MB\n - challenge_policy_outputs_v3.csv 1.10MB\n - challenge_pred_distribution.csv 0.00MB\n - challenge_predictions.csv 0.77MB\n - domain_guard_tau_sweep.csv 0.00MB\n - env_snapshot.json 0.00MB\n - final_policy_config.json 0.00MB\n - final_policy_report.csv 0.00MB\n - mri_resnet18_baseline_best.pth 44.79MB\n - muaz_policy_outputs.csv 0.07MB\n - muaz_policy_report.csv 0.00MB\n - policy_summary.csv 0.00MB\n - policy_summary_v2.csv 0.00MB\n - policy_summary_v3.csv 0.00MB\n - reliability_bins_test_after.csv 0.00MB\n - reliability_bins_test_before.csv 0.00MB\n - reliability_bins_val_after.csv 0.00MB\n - reliability_bins_val_before.csv 0.00MB\n - reliability_test_after.png 0.04MB\n - reliability_test_before.png 0.04MB\n - reliability_val_after.png 0.03MB\n - reliability_val_before.png 0.03MB\n - temperature_scaling.json 0.00MB\n - train_history.json 0.00MB\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# LOAD CHECKPOINT + build feat_extractor (robust to filename differences)\n\nimport torch\nimport torch.nn as nn\nfrom pathlib import Path\n\nOUT = Path(\"/kaggle/working/train_artifacts\")\n\n# Prefer best_model.pth, fall back to other known names\ncands = [\n    OUT/\"best_model.pth\",\n    OUT/\"mri_resnet18_baseline_best.pth\",\n]\nckpt_path = next((p for p in cands if p.exists()), None)\n\n# last-resort: any .pth\nif ckpt_path is None:\n    any_pth = sorted(OUT.glob(\"*.pth\"))\n    ckpt_path = any_pth[0] if any_pth else None\n\nassert ckpt_path is not None, f\"No .pth checkpoint found in {OUT}. Run the RESTORE cell above.\"\n\nprint(\"Loading checkpoint:\", ckpt_path)\n\nckpt = torch.load(ckpt_path, map_location=DEVICE)\n\n# training notebook checkpoints are dicts with model_state_dict\nif isinstance(ckpt, dict) and \"model_state_dict\" in ckpt:\n    model.load_state_dict(ckpt[\"model_state_dict\"])\nelse:\n    # fallback: plain state_dict\n    model.load_state_dict(ckpt)\n\nmodel.eval()\n\nfeat_extractor = nn.Sequential(*list(model.children())[:-1]).to(DEVICE)\nfeat_extractor.eval()\n\nprint(\"✅ checkpoint loaded + feat_extractor ready\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T15:54:55.797374Z","iopub.execute_input":"2026-02-14T15:54:55.797718Z","iopub.status.idle":"2026-02-14T15:54:55.867255Z","shell.execute_reply.started":"2026-02-14T15:54:55.797688Z","shell.execute_reply":"2026-02-14T15:54:55.866422Z"}},"outputs":[{"name":"stdout","text":"Loading checkpoint: /kaggle/working/train_artifacts/best_model.pth\n✅ checkpoint loaded + feat_extractor ready\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# CELL: 01B_DEVICE_FLAGS - Ensure USE_CUDA + DEVICE are defined for downstream cells\n\nimport torch\n\nUSE_CUDA = torch.cuda.is_available()\nDEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n\nprint(\"USE_CUDA:\", USE_CUDA)\nprint(\"DEVICE:\", DEVICE)\nif USE_CUDA:\n    print(\"GPU:\", torch.cuda.get_device_name(0))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T10:47:36.865337Z","iopub.execute_input":"2026-02-14T10:47:36.866177Z","iopub.status.idle":"2026-02-14T10:47:41.154987Z","shell.execute_reply.started":"2026-02-14T10:47:36.866146Z","shell.execute_reply":"2026-02-14T10:47:41.154309Z"}},"outputs":[{"name":"stdout","text":"USE_CUDA: False\nDEVICE: cpu\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# CELL: 01C_RESTORE_TRAIN_ARTIFACTS - Restore trained artifacts into OUT (robust mount detection)\n\nfrom pathlib import Path\nimport shutil\n\nOUT = Path(\"/kaggle/working/train_artifacts\")\nOUT.mkdir(parents=True, exist_ok=True)\n\nCANDIDATES = [\n    Path(\"/kaggle/input/mri-demo-artifacts-bundle/train_artifacts\"),\n    Path(\"/kaggle/input/mri_demo_artifacts_bundle/train_artifacts\"),\n    Path(\"/kaggle/input/datasets/mri-demo-artifacts-bundle/train_artifacts\"),\n    Path(\"/kaggle/input/datasets/mri_demo_artifacts_bundle/train_artifacts\"),\n]\n\nSRC = next((p for p in CANDIDATES if p.exists()), None)\n\nprint(\"Restore candidates:\")\nfor p in CANDIDATES:\n    print(\" -\", p, \"exists=\", p.exists())\n\nassert SRC is not None, \"Could not find train_artifacts in mounted inputs. Confirm 'mri-demo-artifacts-bundle' is attached.\"\n\nprint(\"Using:\", SRC)\n\nfor p in SRC.glob(\"*\"):\n    if p.is_file():\n        shutil.copy2(p, OUT / p.name)\n\nprint(\"✅ Restored into:\", OUT)\nprint(\"Files now in OUT:\")\nfor p in sorted(OUT.glob(\"*\")):\n    if p.is_file():\n        print(\" -\", p.name, f\"{p.stat().st_size/1e6:.2f}MB\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T11:13:42.557547Z","iopub.execute_input":"2026-02-14T11:13:42.558333Z","iopub.status.idle":"2026-02-14T11:13:42.786332Z","shell.execute_reply.started":"2026-02-14T11:13:42.558298Z","shell.execute_reply":"2026-02-14T11:13:42.785353Z"}},"outputs":[{"name":"stdout","text":"Restore candidates:\n - /kaggle/input/mri-demo-artifacts-bundle/train_artifacts exists= True\n - /kaggle/input/mri_demo_artifacts_bundle/train_artifacts exists= False\n - /kaggle/input/datasets/mri-demo-artifacts-bundle/train_artifacts exists= False\n - /kaggle/input/datasets/mri_demo_artifacts_bundle/train_artifacts exists= False\nUsing: /kaggle/input/mri-demo-artifacts-bundle/train_artifacts\n✅ Restored into: /kaggle/working/train_artifacts\nFiles now in OUT:\n - best_model.pth 44.79MB\n - calib_test_logits.pt 0.05MB\n - calib_val_logits.pt 0.05MB\n - calibration_metrics.json 0.00MB\n - calibration_summary_before.csv 0.00MB\n - calibration_summary_before_after.csv 0.00MB\n - challenge_conf_by_pred.csv 0.00MB\n - challenge_confidence_summary.csv 0.00MB\n - challenge_policy_outputs.csv 0.85MB\n - challenge_policy_outputs_demo.csv 1.13MB\n - challenge_policy_outputs_v2.csv 0.95MB\n - challenge_policy_outputs_v3.csv 1.10MB\n - challenge_pred_distribution.csv 0.00MB\n - challenge_predictions.csv 0.77MB\n - domain_guard_tau_sweep.csv 0.00MB\n - env_snapshot.json 0.00MB\n - final_policy_config.json 0.00MB\n - final_policy_report.csv 0.00MB\n - mri_resnet18_baseline_best.pth 44.79MB\n - muaz_policy_outputs.csv 0.07MB\n - muaz_policy_report.csv 0.00MB\n - policy_summary.csv 0.00MB\n - policy_summary_v2.csv 0.00MB\n - policy_summary_v3.csv 0.00MB\n - reliability_bins_test_after.csv 0.00MB\n - reliability_bins_test_before.csv 0.00MB\n - reliability_bins_val_after.csv 0.00MB\n - reliability_bins_val_before.csv 0.00MB\n - reliability_test_after.png 0.04MB\n - reliability_test_before.png 0.04MB\n - reliability_val_after.png 0.03MB\n - reliability_val_before.png 0.03MB\n - temperature_scaling.json 0.00MB\n - train_history.json 0.00MB\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"from pathlib import Path\n\n# 1) Define the dataset roots we actually have in THIS notebook\nROOTS = {\n    \"brainmri\": Path(\"/kaggle/input/datasets/sabersakin/brainmri\"),\n    \"masoud\":  Path(\"/kaggle/input/datasets/masoudnickparvar/brain-tumor-mri-dataset\"),\n    # add challenge roots here ONLY if df_chal points to them\n    # \"stroke\": Path(\"/kaggle/input/datasets/mitangshu11/brain-stroke-mri-images\"),\n    # \"oasis\":  Path(\"/kaggle/input/datasets/ninadaithal/imagesoasis\"),\n    # \"dicom\":  Path(\"/kaggle/input/datasets/trainingdatapro/dicom-brain-dataset\"),\n}\n\nfor k,v in ROOTS.items():\n    print(k, \"exists:\", v.exists(), \"->\", v)\n\ndef canon(p: str) -> str:\n    p = str(p)\n    # already valid\n    if Path(p).exists():\n        return p\n\n    # If the path contains an identifiable dataset segment, re-root it.\n    # Masoud dataset segment\n    seg = \"/masoudnickparvar/brain-tumor-mri-dataset/\"\n    if seg in p:\n        tail = p.split(seg, 1)[1]\n        cand = ROOTS[\"masoud\"] / tail\n        return str(cand)\n\n    # Sabersakin brainmri segment\n    seg = \"/sabersakin/brainmri/\"\n    if seg in p:\n        tail = p.split(seg, 1)[1]\n        cand = ROOTS[\"brainmri\"] / tail\n        return str(cand)\n\n    # fallback: keep original (debug)\n    return p\n\ndef apply_canon(df, name):\n    df = df.copy()\n    df[\"path\"] = df[\"path\"].apply(canon)\n    missing = (~df[\"path\"].apply(lambda x: Path(x).exists())).sum()\n    print(f\"{name}: rows={len(df)} missing={missing}\")\n    if missing:\n        print(\"Examples:\", df.loc[~df[\"path\"].apply(lambda x: Path(x).exists()), \"path\"].head(3).tolist())\n    return df\n\ndf_train = apply_canon(df_train, \"df_train\")\ndf_val   = apply_canon(df_val,   \"df_val\")\ndf_test  = apply_canon(df_test,  \"df_test\")\ndf_chal  = apply_canon(df_chal,  \"df_chal\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T09:14:55.643178Z","iopub.execute_input":"2026-02-14T09:14:55.643936Z","iopub.status.idle":"2026-02-14T09:14:55.658968Z","shell.execute_reply.started":"2026-02-14T09:14:55.643901Z","shell.execute_reply":"2026-02-14T09:14:55.657668Z"}},"outputs":[{"name":"stdout","text":"brainmri exists: True -> /kaggle/input/datasets/sabersakin/brainmri\nmasoud exists: True -> /kaggle/input/datasets/masoudnickparvar/brain-tumor-mri-dataset\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/4032953103.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_canon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"df_train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0mdf_val\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mapply_canon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_val\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;34m\"df_val\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mdf_test\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mapply_canon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m\"df_test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'df_train' is not defined"],"ename":"NameError","evalue":"name 'df_train' is not defined","output_type":"error"}],"execution_count":7},{"cell_type":"code","source":"# CELL: 02_LOAD_SPLITS - Load split CSVs into DataFrames\n\nfrom pathlib import Path\nimport pandas as pd\n\nsplit_train = Path(split_train)\nsplit_val   = Path(split_val)\nsplit_test  = Path(split_test)\nsplit_ext   = Path(split_ext)\nsplit_chal  = Path(split_chal)\n\nARTIFACTS_DIR = split_train.parent\nprint(\"ARTIFACTS_DIR:\", ARTIFACTS_DIR)\n\ndf_train = pd.read_csv(split_train)\ndf_val   = pd.read_csv(split_val)\ndf_test  = pd.read_csv(split_test)\ndf_ext   = pd.read_csv(split_ext)\ndf_chal  = pd.read_csv(split_chal)\n\nprint(\"loaded: train/val/test:\", len(df_train), len(df_val), len(df_test))\nprint(\"loaded: external_npz:\", len(df_ext), \"challenge:\", len(df_chal))\n\n# Schema sanity (fail early)\nREQ_COLS = {\"path\", \"label_id\", \"label_name\"}\nfor name, df in [(\"train\", df_train), (\"val\", df_val), (\"test\", df_test)]:\n    missing = REQ_COLS - set(df.columns)\n    assert not missing, f\"{name} missing columns: {missing}\"\n\nprint(\"✅ split schemas OK\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T15:52:15.734582Z","iopub.execute_input":"2026-02-14T15:52:15.734959Z","iopub.status.idle":"2026-02-14T15:52:15.903538Z","shell.execute_reply.started":"2026-02-14T15:52:15.734927Z","shell.execute_reply":"2026-02-14T15:52:15.902420Z"}},"outputs":[{"name":"stdout","text":"ARTIFACTS_DIR: /kaggle/input/datasets/kabomolefe/mri-data-artifacts-v1/data_artifacts\nloaded: train/val/test: 16646 1850 2055\nloaded: external_npz: 1311 challenge: 4275\n✅ split schemas OK\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from pathlib import Path\n\ndef missing_count(df, name):\n    m = ~df[\"path\"].apply(lambda p: Path(p).exists())\n    print(f\"{name}: rows={len(df)} missing={m.sum()}\")\n    if m.sum():\n        print(\"Examples:\", df.loc[m, \"path\"].head(3).tolist())\n\nmissing_count(df_train, \"df_train\")\nmissing_count(df_val, \"df_val\")\nmissing_count(df_test, \"df_test\")\nmissing_count(df_chal, \"df_chal\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T10:28:53.989827Z","iopub.execute_input":"2026-02-14T10:28:53.990151Z","iopub.status.idle":"2026-02-14T10:29:11.254428Z","shell.execute_reply.started":"2026-02-14T10:28:53.990121Z","shell.execute_reply":"2026-02-14T10:29:11.253702Z"}},"outputs":[{"name":"stdout","text":"df_train: rows=16646 missing=0\ndf_val: rows=1850 missing=0\ndf_test: rows=2055 missing=0\ndf_chal: rows=4275 missing=0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# CELL: 03_PREFLIGHT - Strong checks (paths exist, labels valid, distributions)\n\nfrom pathlib import Path\nimport numpy as np\n\ndef assert_paths_exist(df, n=50, seed=SEED, label=\"df\"):\n    sample = df[\"path\"].sample(min(n, len(df)), random_state=seed).tolist()\n    missing = [p for p in sample if not Path(p).exists()]\n    assert not missing, f\"{label}: Missing {len(missing)} files; example: {missing[0]}\"\n\nassert_paths_exist(df_train, n=50, label=\"train\")\nassert_paths_exist(df_val,   n=30, label=\"val\")\nassert_paths_exist(df_test,  n=30, label=\"test\")\nprint(\"✅ sample paths exist\")\n\n# Label sanity\nfor name, df in [(\"train\", df_train), (\"val\", df_val), (\"test\", df_test)]:\n    bad = df[~df[\"label_id\"].isin([0,1,2,3])]\n    assert len(bad) == 0, f\"{name}: found label_id outside [0,1,2,3]\"\n\nprint(\"\\nTrain label distribution:\\n\", df_train[\"label_name\"].value_counts())\nprint(\"\\nVal label distribution:\\n\", df_val[\"label_name\"].value_counts())\nprint(\"\\nTest label distribution:\\n\", df_test[\"label_name\"].value_counts())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data pipeline (datasets, transforms, loaders)\n\nWe use CSV-backed datasets so training is fully driven by the saved split artifacts.","metadata":{}},{"cell_type":"code","source":"# CELL: 03B_VERIFY_LABEL_MAPPING - ensure label_id <-> label_name is 1:1 and consistent\n\nimport pandas as pd\n\ndef verify(df, name):\n    assert \"label_id\" in df.columns, f\"{name} missing label_id\"\n    assert \"label_name\" in df.columns, f\"{name} missing label_name\"\n\n    pairs = df[[\"label_id\", \"label_name\"]].drop_duplicates().sort_values([\"label_id\", \"label_name\"])\n    print(f\"\\n== {name} label pairs ==\")\n    display(pairs)\n\n    # 1-to-1 checks\n    id_to_names = df.groupby(\"label_id\")[\"label_name\"].nunique()\n    name_to_ids = df.groupby(\"label_name\")[\"label_id\"].nunique()\n    assert (id_to_names <= 1).all(), f\"{name}: a label_id maps to multiple label_names: {id_to_names[id_to_names>1].to_dict()}\"\n    assert (name_to_ids <= 1).all(), f\"{name}: a label_name maps to multiple label_ids: {name_to_ids[name_to_ids>1].to_dict()}\"\n\n    # counts\n    print(f\"{name} label_id dist:\\n\", df[\"label_id\"].value_counts().sort_index())\n    print(f\"{name} label_name dist:\\n\", df[\"label_name\"].value_counts())\n\nverify(df_train, \"train\")\nverify(df_val,   \"val\")\nverify(df_test,  \"test\")\n\nprint(\"\\n✅ label_id/label_name mapping is consistent across splits\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL: 03C_VISUALIZE_LABELS - show examples per label_id so we stop guessing\n\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom PIL import Image\n\nK = 4\nN_PER_CLASS = 4\n\npairs = df_train[[\"label_id\",\"label_name\"]].drop_duplicates().sort_values(\"label_id\")\nprint(\"label map:\\n\", pairs)\n\nfor lid in sorted(df_train[\"label_id\"].unique()):\n    name = df_train.loc[df_train[\"label_id\"]==lid, \"label_name\"].iloc[0]\n    samp = df_train[df_train[\"label_id\"]==lid].sample(min(N_PER_CLASS, (df_train[\"label_id\"]==lid).sum()), random_state=0)\n\n    fig, axes = plt.subplots(1, len(samp), figsize=(3*len(samp), 3))\n    if len(samp) == 1:\n        axes = [axes]\n\n    for ax, (_, row) in zip(axes, samp.iterrows()):\n        img = Image.open(Path(row[\"path\"])).convert(\"RGB\")\n        ax.imshow(img)\n        ax.axis(\"off\")\n    fig.suptitle(f\"label_id={lid}  label_name={name}\", fontsize=14)\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL: 03C_LOCK_LABEL_MAP - hard assert canonical mapping\n\nEXPECTED = {\n    0: \"glioma\",\n    1: \"meningioma\",\n    2: \"pituitary\",\n    3: \"notumor\",\n}\n\nfor name, df in [(\"train\", df_train), (\"val\", df_val), (\"test\", df_test)]:\n    pairs = dict(df[[\"label_id\",\"label_name\"]].drop_duplicates().values.tolist())\n    assert pairs == EXPECTED, f\"{name} mapping changed!\\nGot: {pairs}\\nExpected: {EXPECTED}\"\n\nprint(\"✅ canonical label map locked:\", EXPECTED)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL: 04_TRAIN_CONFIG - Central training hyperparameters (CPU/GPU aware)\n\nfrom dataclasses import dataclass\n\nNUM_CLASSES = 4\nLABEL_MAP = {0:\"glioma\", 1:\"meningioma\", 2:\"pituitary\", 3:\"notumor\"}\n\n@dataclass(frozen=True)\nclass TrainConfig:\n    img_size: int = 224\n    batch_size_cpu: int = 32\n    batch_size_gpu: int = 128\n    num_workers_cpu: int = 2\n    num_workers_gpu: int = 4\n    lr: float = 3e-4\n    weight_decay: float = 1e-4\n    epochs_smoke: int = 1\n    epochs_full: int = 10\n\nCFG = TrainConfig()\n\nBATCH_SIZE = CFG.batch_size_gpu if USE_CUDA else CFG.batch_size_cpu\nNUM_WORKERS = CFG.num_workers_gpu if USE_CUDA else CFG.num_workers_cpu\n\nprint(\"CFG:\", CFG)\nprint(\"BATCH_SIZE:\", BATCH_SIZE, \"| NUM_WORKERS:\", NUM_WORKERS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T15:52:24.026980Z","iopub.execute_input":"2026-02-14T15:52:24.027686Z","iopub.status.idle":"2026-02-14T15:52:24.037541Z","shell.execute_reply.started":"2026-02-14T15:52:24.027645Z","shell.execute_reply":"2026-02-14T15:52:24.036285Z"}},"outputs":[{"name":"stdout","text":"CFG: TrainConfig(img_size=224, batch_size_cpu=32, batch_size_gpu=128, num_workers_cpu=2, num_workers_gpu=4, lr=0.0003, weight_decay=0.0001, epochs_smoke=1, epochs_full=10)\nBATCH_SIZE: 32 | NUM_WORKERS: 2\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# CELL: 05_DATASET_AND_TRANSFORMS - CSV dataset + torchvision transforms\n\nfrom pathlib import Path\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nimport torchvision.transforms as T\n\ntrain_tfms = T.Compose([\n    T.Resize((CFG.img_size, CFG.img_size)),\n    T.RandomHorizontalFlip(p=0.5),\n    T.ToTensor(),\n    # Baseline: ImageNet stats. We'll revisit for MRI-specific normalization later.\n    T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n])\n\neval_tfms = T.Compose([\n    T.Resize((CFG.img_size, CFG.img_size)),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n])\n\nclass CSVDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, transforms):\n        self.df = df.reset_index(drop=True)\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx: int):\n        row = self.df.iloc[idx]\n        img_path = Path(row[\"path\"])\n        y = int(row[\"label_id\"])\n        img = Image.open(img_path).convert(\"RGB\")\n        x = self.transforms(img)\n        return x, y\n\ntrain_ds = CSVDataset(df_train, train_tfms)\nval_ds   = CSVDataset(df_val, eval_tfms)\ntest_ds  = CSVDataset(df_test, eval_tfms)\n\nprint(\"✅ datasets ready:\", len(train_ds), len(val_ds), len(test_ds))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T15:52:28.823601Z","iopub.execute_input":"2026-02-14T15:52:28.824364Z","iopub.status.idle":"2026-02-14T15:52:28.836275Z","shell.execute_reply.started":"2026-02-14T15:52:28.824313Z","shell.execute_reply":"2026-02-14T15:52:28.835275Z"}},"outputs":[{"name":"stdout","text":"✅ datasets ready: 16646 1850 2055\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# CELL: 06_DATALOADERS - Build PyTorch DataLoaders\n\nfrom torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(\n    train_ds, batch_size=BATCH_SIZE, shuffle=True,\n    num_workers=NUM_WORKERS, pin_memory=USE_CUDA\n)\nval_loader = DataLoader(\n    val_ds, batch_size=BATCH_SIZE, shuffle=False,\n    num_workers=NUM_WORKERS, pin_memory=USE_CUDA\n)\ntest_loader = DataLoader(\n    test_ds, batch_size=BATCH_SIZE, shuffle=False,\n    num_workers=NUM_WORKERS, pin_memory=USE_CUDA\n)\n\nxb, yb = next(iter(train_loader))\nprint(\"batch x:\", xb.shape, xb.dtype, \"batch y:\", yb.shape, yb.dtype)\nprint(\"labels in batch:\", sorted(set(yb.tolist())))\nprint(\"✅ loaders ready\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T15:52:34.023644Z","iopub.execute_input":"2026-02-14T15:52:34.024015Z","iopub.status.idle":"2026-02-14T15:52:35.164856Z","shell.execute_reply.started":"2026-02-14T15:52:34.023986Z","shell.execute_reply":"2026-02-14T15:52:35.163946Z"}},"outputs":[{"name":"stdout","text":"batch x: torch.Size([32, 3, 224, 224]) torch.float32 batch y: torch.Size([32]) torch.int64\nlabels in batch: [0, 1, 2, 3]\n✅ loaders ready\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"!find /kaggle/input/datasets/masoudnickparvar/brain-tumor-mri-dataset -name \"Tr-no_0747.jpg\" | head\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T09:21:50.627012Z","iopub.execute_input":"2026-02-14T09:21:50.627407Z","iopub.status.idle":"2026-02-14T09:21:50.791662Z","shell.execute_reply.started":"2026-02-14T09:21:50.627372Z","shell.execute_reply":"2026-02-14T09:21:50.790432Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"## Model\n\nBaseline is **ResNet‑18** (ImageNet pretrained) with a 4‑way classification head.\n","metadata":{}},{"cell_type":"code","source":"# CELL: 07_MODEL_OPT - ResNet18, loss, optimizer (pretrained)\n\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\n\nassert \"CFG\" in globals(), \"CFG not defined — run Cell 01_CONFIG / 04_TRAIN_CONFIG first.\"\nassert \"DEVICE\" in globals(), \"DEVICE not defined — run Cell 01_CONFIG first.\"\n\n# Single source of truth\nNUM_CLASSES = int(getattr(CFG, \"num_classes\", 4))  # default 4 if not present\n\n# Build model\nweights = models.ResNet18_Weights.DEFAULT\nmodel = models.resnet18(weights=weights)\nprint(\"✅ loaded ImageNet pretrained weights\")\n\n# Replace head\nin_features = model.fc.in_features\nmodel.fc = nn.Linear(in_features, NUM_CLASSES)\n\nmodel = model.to(DEVICE)\n\n# Loss + optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=float(getattr(CFG, \"lr\", 3e-4)), weight_decay=float(getattr(CFG, \"weight_decay\", 1e-4)))\n\nprint(f\"✅ model ready | num_classes={NUM_CLASSES} | device={DEVICE} | lr={optimizer.param_groups[0]['lr']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T15:52:41.218221Z","iopub.execute_input":"2026-02-14T15:52:41.218612Z","iopub.status.idle":"2026-02-14T15:52:41.871149Z","shell.execute_reply.started":"2026-02-14T15:52:41.218574Z","shell.execute_reply":"2026-02-14T15:52:41.870276Z"}},"outputs":[{"name":"stdout","text":"Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 44.7M/44.7M [00:00<00:00, 145MB/s]","output_type":"stream"},{"name":"stdout","text":"✅ loaded ImageNet pretrained weights\n✅ model ready | num_classes=4 | device=cpu | lr=0.0003\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom pathlib import Path\n\nOUT = Path(\"/kaggle/working/train_artifacts\")\n\nckpt = torch.load(OUT/\"best_model.pth\", map_location=DEVICE)\nmodel.load_state_dict(ckpt[\"model_state_dict\"])\nmodel.eval()\n\nfeat_extractor = nn.Sequential(*list(model.children())[:-1]).to(DEVICE)\nfeat_extractor.eval()\n\nprint(\"✅ Loaded checkpoint and built feat_extractor\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T15:55:24.890812Z","iopub.execute_input":"2026-02-14T15:55:24.891139Z","iopub.status.idle":"2026-02-14T15:55:24.960305Z","shell.execute_reply.started":"2026-02-14T15:55:24.891110Z","shell.execute_reply":"2026-02-14T15:55:24.959405Z"}},"outputs":[{"name":"stdout","text":"✅ Loaded checkpoint and built feat_extractor\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"## Metrics\n\nWe track:\n- loss\n- confusion matrix\n- macro‑F1 (class-balanced)\n","metadata":{}},{"cell_type":"code","source":"# CELL: 08_METRICS - Confusion matrix + macro-F1 helpers\n\nimport torch\n\n@torch.no_grad()\ndef confusion_matrix_from_logits(logits: torch.Tensor, y_true: torch.Tensor, num_classes: int) -> torch.Tensor:\n    preds = torch.argmax(logits, dim=1)\n    cm = torch.zeros((num_classes, num_classes), dtype=torch.int64)\n    for t, p in zip(y_true.view(-1).cpu(), preds.view(-1).cpu()):\n        cm[int(t), int(p)] += 1\n    return cm\n\ndef macro_f1_from_cm(cm: torch.Tensor) -> float:\n    # cm[t, p]\n    cm = cm.to(torch.float32)\n    f1s = []\n    for k in range(cm.shape[0]):\n        tp = cm[k, k]\n        fp = cm[:, k].sum() - tp\n        fn = cm[k, :].sum() - tp\n        denom = (2*tp + fp + fn)\n        f1 = (2*tp / denom) if denom > 0 else torch.tensor(0.0)\n        f1s.append(f1)\n    return float(torch.stack(f1s).mean().item())\n\nprint(\"✅ metrics helpers loaded\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training\n\nRun a 1‑epoch smoke test first, then the full baseline training.\n\n**Tip:** turn GPU on in Kaggle Settings before running this section.","metadata":{}},{"cell_type":"code","source":"# CELL: 09_TRAIN - Train loop with AMP on GPU, save best checkpoint by val macro-F1\n\nimport time\nimport json\nimport torch\n\nuse_cuda = USE_CUDA\nscaler = torch.cuda.amp.GradScaler(enabled=use_cuda)\n\nbest_f1 = -1.0\nbest_path = OUT / \"best_model.pth\"\nhistory = []\n\ndef run_epoch(train: bool):\n    if train:\n        model.train()\n    else:\n        model.eval()\n\n    total_loss, n = 0.0, 0\n    cm = torch.zeros((NUM_CLASSES, NUM_CLASSES), dtype=torch.int64)\n\n    loader = train_loader if train else val_loader\n\n    for xb, yb in loader:\n        xb = xb.to(DEVICE, non_blocking=True)\n        yb = yb.to(DEVICE, non_blocking=True)\n\n        with torch.set_grad_enabled(train):\n            with torch.cuda.amp.autocast(enabled=use_cuda):\n                logits = model(xb)\n                loss = criterion(logits, yb)\n\n            if train:\n                optimizer.zero_grad(set_to_none=True)\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n\n        total_loss += float(loss.item()) * xb.size(0)\n        n += xb.size(0)\n\n        if not train:\n            cm += confusion_matrix_from_logits(logits.detach(), yb.detach(), NUM_CLASSES)\n\n    avg_loss = total_loss / max(n, 1)\n    return avg_loss, cm\n\ndef train_for(epochs: int, tag: str):\n    global best_f1\n    for epoch in range(1, epochs + 1):\n        t0 = time.time()\n        tr_loss, _ = run_epoch(train=True)\n        va_loss, cm = run_epoch(train=False)\n        va_f1 = macro_f1_from_cm(cm)\n\n        rec = {\n            \"phase\": tag,\n            \"epoch\": epoch,\n            \"train_loss\": tr_loss,\n            \"val_loss\": va_loss,\n            \"val_macro_f1\": va_f1,\n            \"elapsed_s\": round(time.time() - t0, 2),\n        }\n        history.append(rec)\n\n        improved = va_f1 > best_f1\n        if improved:\n            best_f1 = va_f1\n            torch.save(\n                {\n                    \"model_state_dict\": model.state_dict(),\n                    \"label_map\": LABEL_MAP,\n                    \"num_classes\": NUM_CLASSES,\n                    \"config\": CFG.__dict__,\n                    \"best_val_macro_f1\": best_f1,\n                },\n                best_path,\n            )\n\n        print(f\"[{tag}] epoch {epoch:02d} | train_loss={tr_loss:.4f} val_loss={va_loss:.4f} val_f1={va_f1:.4f} {'✅ best' if improved else ''}\")\n\n# 1 epoch smoke test\ntrain_for(CFG.epochs_smoke, tag=\"smoke\")\n\n# full training\ntrain_for(CFG.epochs_full, tag=\"full\")\n\nwith open(OUT/\"train_history.json\", \"w\") as f:\n    json.dump(history, f, indent=2)\n\nprint(\"\\n✅ Training done. best_val_macro_f1:\", best_f1)\nprint(\"best checkpoint:\", best_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CHECK: best_model.pth exists and is the right size\n\nfrom pathlib import Path\np = OUT / \"best_model.pth\"\nprint(\"Looking for:\", p)\nassert p.exists(), \"best_model.pth not found after training\"\nprint(\"✅ size:\", p.stat().st_size/1e6, \"MB\")\nassert p.stat().st_size > 20, \"Checkpoint too small — invalid\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T15:55:14.478050Z","iopub.execute_input":"2026-02-14T15:55:14.479190Z","iopub.status.idle":"2026-02-14T15:55:14.484685Z","shell.execute_reply.started":"2026-02-14T15:55:14.479157Z","shell.execute_reply":"2026-02-14T15:55:14.483869Z"}},"outputs":[{"name":"stdout","text":"Looking for: /kaggle/working/train_artifacts/best_model.pth\n✅ size: 44.794315 MB\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"## Evaluation\n\nEvaluate the best checkpoint on the held-out **image test split**.\n","metadata":{}},{"cell_type":"code","source":"# CELL: 10_EVAL_TEST - Evaluate best checkpoint on test split\n\nimport torch\n\nckpt = torch.load(OUT/\"best_model.pth\", map_location=DEVICE)\nmodel.load_state_dict(ckpt[\"model_state_dict\"])\nmodel.eval()\n\ntest_cm = torch.zeros((NUM_CLASSES, NUM_CLASSES), dtype=torch.int64)\ntest_loss_total, n = 0.0, 0\n\nwith torch.no_grad():\n    for xb, yb in test_loader:\n        xb = xb.to(DEVICE, non_blocking=True)\n        yb = yb.to(DEVICE, non_blocking=True)\n        logits = model(xb)\n        loss = criterion(logits, yb)\n        test_loss_total += float(loss.item()) * xb.size(0)\n        n += xb.size(0)\n        test_cm += confusion_matrix_from_logits(logits, yb, NUM_CLASSES)\n\ntest_loss = test_loss_total / max(n, 1)\ntest_f1 = macro_f1_from_cm(test_cm)\n\nprint(\"test_loss:\", round(test_loss, 5))\nprint(\"test_macro_f1:\", round(test_f1, 5))\nprint(\"confusion matrix (rows=true, cols=pred):\\n\", test_cm)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T11:14:43.997728Z","iopub.execute_input":"2026-02-14T11:14:43.998152Z","iopub.status.idle":"2026-02-14T11:14:46.158124Z","shell.execute_reply.started":"2026-02-14T11:14:43.998122Z","shell.execute_reply":"2026-02-14T11:14:46.157011Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/2943769614.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtest_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mtest_cm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mconfusion_matrix_from_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_loss_total\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'confusion_matrix_from_logits' is not defined"],"ename":"NameError","evalue":"name 'confusion_matrix_from_logits' is not defined","output_type":"error"}],"execution_count":20},{"cell_type":"code","source":"# CELL: 10A_COLLECT_LOGITS_VAL_TEST - Collect logits/labels for calibration (val + test)\n\nimport torch\nimport torch.nn.functional as F\n\n@torch.no_grad()\ndef collect_logits(loader, model):\n    model.eval()\n    all_logits, all_y = [], []\n    for xb, yb in loader:\n        xb = xb.to(DEVICE, non_blocking=True)\n        yb = yb.to(DEVICE, non_blocking=True)\n        logits = model(xb)\n        all_logits.append(logits.detach().cpu())\n        all_y.append(yb.detach().cpu())\n\n    logits = torch.cat(all_logits, dim=0)\n    y = torch.cat(all_y, dim=0)\n\n    assert logits.ndim == 2 and logits.shape[1] == NUM_CLASSES, logits.shape\n    assert y.ndim == 1 and y.shape[0] == logits.shape[0], (y.shape, logits.shape)\n    assert torch.isfinite(logits).all(), \"Found NaN/Inf in logits\"\n\n    return logits, y\n\nval_logits, val_y = collect_logits(val_loader, model)\ntest_logits, test_y = collect_logits(test_loader, model)\n\ntorch.save({\"logits\": val_logits, \"y\": val_y}, OUT / \"calib_val_logits.pt\")\ntorch.save({\"logits\": test_logits, \"y\": test_y}, OUT / \"calib_test_logits.pt\")\n\nprint(\"✅ Collected logits:\")\nprint(\"  val :\", val_logits.shape, val_y.shape)\nprint(\"  test:\", test_logits.shape, test_y.shape)\nprint(\"  saved ->\", OUT)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T15:56:45.983645Z","iopub.execute_input":"2026-02-14T15:56:45.984018Z","iopub.status.idle":"2026-02-14T15:59:51.584970Z","shell.execute_reply.started":"2026-02-14T15:56:45.983987Z","shell.execute_reply":"2026-02-14T15:59:51.583966Z"}},"outputs":[{"name":"stdout","text":"✅ Collected logits:\n  val : torch.Size([1850, 4]) torch.Size([1850])\n  test: torch.Size([2055, 4]) torch.Size([2055])\n  saved -> /kaggle/working/train_artifacts\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# CELL: 10B_CALIBRATION_METRICS_BEFORE - ECE + reliability bins + plots (before scaling)\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn.functional as F\n\ndef _onehot(y, k):\n    return F.one_hot(y.to(torch.int64), num_classes=k).to(torch.float32)\n\ndef compute_reliability_bins(probs: torch.Tensor, y_true: torch.Tensor, n_bins: int = 15):\n    \"\"\"\n    probs: (N, C) softmax probs\n    y_true: (N,)\n    Returns (ece, bins_df)\n    \"\"\"\n    assert probs.ndim == 2 and y_true.ndim == 1\n    conf, preds = probs.max(dim=1)\n    correct = (preds == y_true).to(torch.float32)\n\n    bin_edges = torch.linspace(0.0, 1.0, n_bins + 1)\n    rows = []\n    ece = 0.0\n    N = probs.shape[0]\n\n    for i in range(n_bins):\n        lo = float(bin_edges[i].item())\n        hi = float(bin_edges[i + 1].item())\n\n        # (lo, hi] except first bin includes 0\n        if i == 0:\n            in_bin = (conf >= lo) & (conf <= hi)\n        else:\n            in_bin = (conf > lo) & (conf <= hi)\n\n        idx = torch.where(in_bin)[0]\n        cnt = int(idx.numel())\n        if cnt == 0:\n            rows.append({\"bin_lo\": lo, \"bin_hi\": hi, \"count\": 0, \"avg_conf\": None, \"avg_acc\": None})\n            continue\n\n        avg_conf = float(conf[idx].mean().item())\n        avg_acc = float(correct[idx].mean().item())\n\n        frac = cnt / N\n        ece += frac * abs(avg_acc - avg_conf)\n\n        rows.append({\"bin_lo\": lo, \"bin_hi\": hi, \"count\": cnt, \"avg_conf\": avg_conf, \"avg_acc\": avg_acc})\n\n    return float(ece), pd.DataFrame(rows)\n\ndef plot_reliability(bins_df: pd.DataFrame, title: str, out_path):\n    df = bins_df.dropna(subset=[\"avg_conf\", \"avg_acc\"]).copy()\n    plt.figure(figsize=(6, 6))\n    plt.plot([0, 1], [0, 1])\n    plt.plot(df[\"avg_conf\"], df[\"avg_acc\"], marker=\"o\")\n    plt.xlabel(\"Confidence (avg per bin)\")\n    plt.ylabel(\"Accuracy (avg per bin)\")\n    plt.title(title)\n    plt.grid(True, alpha=0.3)\n    plt.savefig(out_path, bbox_inches=\"tight\")\n    plt.show()\n\ndef summarize_calibration(logits: torch.Tensor, y_true: torch.Tensor, split_name: str, tag: str, n_bins: int = 15):\n    probs = torch.softmax(logits, dim=1)\n    preds = probs.argmax(dim=1)\n\n    acc = float((preds == y_true).to(torch.float32).mean().item())\n    nll = float(F.cross_entropy(logits, y_true).item())\n\n    oh = _onehot(y_true, NUM_CLASSES)\n    brier = float(((probs - oh) ** 2).sum(dim=1).mean().item())\n\n    cm = confusion_matrix_from_logits(logits, y_true, NUM_CLASSES)\n    macro_f1 = macro_f1_from_cm(cm)\n\n    ece, bins_df = compute_reliability_bins(probs, y_true, n_bins=n_bins)\n\n    bins_csv = OUT / f\"reliability_bins_{split_name}_{tag}.csv\"\n    bins_df.to_csv(bins_csv, index=False)\n\n    plot_path = OUT / f\"reliability_{split_name}_{tag}.png\"\n    plot_reliability(bins_df, f\"{split_name.upper()} reliability ({tag}) | ECE={ece:.4f}\", plot_path)\n\n    return {\n        \"split\": split_name,\n        \"tag\": tag,\n        \"n\": int(y_true.shape[0]),\n        \"acc\": acc,\n        \"macro_f1\": float(macro_f1),\n        \"nll\": nll,\n        \"brier\": brier,\n        \"ece\": ece,\n        \"bins_csv\": str(bins_csv),\n        \"plot_png\": str(plot_path),\n    }\n\nN_BINS = 15\n\nval_before = summarize_calibration(val_logits, val_y, \"val\", \"before\", n_bins=N_BINS)\ntest_before = summarize_calibration(test_logits, test_y, \"test\", \"before\", n_bins=N_BINS)\n\nprint(\"✅ Calibration BEFORE:\")\nprint(val_before)\nprint(test_before)\n\npd.DataFrame([val_before, test_before]).to_csv(OUT / \"calibration_summary_before.csv\", index=False)\nprint(\"saved ->\", OUT / \"calibration_summary_before.csv\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL: 10C_TEMPERATURE_SCALING_FIT - Fit temperature on VAL by minimizing NLL (no test leakage)\n\nimport json\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass TemperatureScaler(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.log_T = nn.Parameter(torch.zeros(1))  # T=exp(0)=1\n\n    def temperature(self):\n        # Clamp to avoid degenerate values\n        return torch.exp(self.log_T).clamp(0.05, 100.0)\n\n    def forward(self, logits):\n        return logits / self.temperature()\n\nts = TemperatureScaler().to(DEVICE)\n\nval_logits_d = val_logits.to(DEVICE)\nval_y_d = val_y.to(DEVICE)\n\nopt = torch.optim.LBFGS([ts.log_T], lr=0.5, max_iter=50, line_search_fn=\"strong_wolfe\")\n\ndef closure():\n    opt.zero_grad()\n    loss = F.cross_entropy(ts(val_logits_d), val_y_d)\n    loss.backward()\n    return loss\n\nopt.step(closure)\n\nT = float(ts.temperature().detach().cpu().item())\nval_nll_after_fit = float(F.cross_entropy((val_logits / T), val_y).item())\n\nprint(f\"✅ Fitted temperature T = {T:.6f}\")\nprint(f\"VAL NLL (after applying T): {val_nll_after_fit:.6f}\")\n\nwith open(OUT / \"temperature_scaling.json\", \"w\") as f:\n    json.dump({\"temperature\": T, \"val_nll_after_fit\": val_nll_after_fit}, f, indent=2)\n\nprint(\"saved ->\", OUT / \"temperature_scaling.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T16:00:58.526975Z","iopub.execute_input":"2026-02-14T16:00:58.527354Z","iopub.status.idle":"2026-02-14T16:00:58.584037Z","shell.execute_reply.started":"2026-02-14T16:00:58.527316Z","shell.execute_reply":"2026-02-14T16:00:58.583219Z"}},"outputs":[{"name":"stdout","text":"✅ Fitted temperature T = 0.925916\nVAL NLL (after applying T): 0.012948\nsaved -> /kaggle/working/train_artifacts/temperature_scaling.json\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# CELL: 10D_CALIBRATION_METRICS_AFTER - Recompute metrics after temperature scaling (val + test)\n\nimport pandas as pd\nimport json\n\nval_logits_cal = val_logits / T\ntest_logits_cal = test_logits / T\n\nval_after = summarize_calibration(val_logits_cal, val_y, \"val\", \"after\", n_bins=N_BINS)\ntest_after = summarize_calibration(test_logits_cal, test_y, \"test\", \"after\", n_bins=N_BINS)\n\nprint(\"✅ Calibration AFTER:\")\nprint(val_after)\nprint(test_after)\n\ndf_all = pd.DataFrame([val_before, test_before, val_after, test_after])\ndf_all.to_csv(OUT / \"calibration_summary_before_after.csv\", index=False)\n\nwith open(OUT / \"calibration_metrics.json\", \"w\") as f:\n    json.dump(\n        {\n            \"n_bins\": N_BINS,\n            \"temperature\": T,\n            \"val\": {\"before\": val_before, \"after\": val_after},\n            \"test\": {\"before\": test_before, \"after\": test_after},\n        },\n        f,\n        indent=2,\n    )\n\nprint(\"saved ->\", OUT / \"calibration_summary_before_after.csv\")\nprint(\"saved ->\", OUT / \"calibration_metrics.json\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL: 10E_PICK_CONFIDENCE_THRESHOLD_FROM_VAL - Pick τ_conf from VAL (calibrated)\n\nimport torch\n\nval_probs_cal = torch.softmax(val_logits / T, dim=1)\nval_maxprob_cal = val_probs_cal.max(dim=1).values\n\n# Choose coverage target: keep 95% of in-domain samples\nKEEP_RATE = 0.95\ntau_conf = float(torch.quantile(val_maxprob_cal, 1.0 - KEEP_RATE).item())\n\nprint(\"✅ tau_conf chosen from VAL (calibrated)\")\nprint(\"KEEP_RATE:\", KEEP_RATE)\nprint(\"tau_conf:\", tau_conf)\n\n# Sanity: what fraction would abstain on val/test under this threshold?\ntest_probs_cal = torch.softmax(test_logits / T, dim=1)\ntest_maxprob_cal = test_probs_cal.max(dim=1).values\n\nval_abstain_rate = float((val_maxprob_cal < tau_conf).to(torch.float32).mean().item())\ntest_abstain_rate = float((test_maxprob_cal < tau_conf).to(torch.float32).mean().item())\n\nprint(\"VAL abstain rate:\", val_abstain_rate)\nprint(\"TEST abstain rate:\", test_abstain_rate)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL: 11_EVAL_EXTERNAL_NPZ - evaluate MUAZ external NPZ test (domain shift)\n\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport torch\nimport torch.nn.functional as F\n\n# ---- Locate split CSV ----\n# Prefer variables from earlier cells if they exist; otherwise auto-find.\nARTIFACTS_DIR = Path(split_train).parent if \"split_train\" in globals() else None\nif ARTIFACTS_DIR is None:\n    # fallback: search common location if you didn't keep split_train variable\n    ARTIFACTS_DIR = Path(\"/kaggle/input\").rglob(\"split_external_test_npz.csv\").__iter__().__next__().parent\n\nsplit_ext_path = ARTIFACTS_DIR / \"split_external_test_npz.csv\"\nassert split_ext_path.exists(), f\"Can't find split_external_test_npz.csv at: {split_ext_path}\"\n\ndf_ext = pd.read_csv(split_ext_path)\nprint(\"split_external_test_npz.csv rows:\", len(df_ext))\nprint(\"columns:\", df_ext.columns.tolist())\ndisplay(df_ext.head(3))\n\n# ---- Load checkpoint (best) into model ----\nbest_path = Path(\"/kaggle/working/train_artifacts/best_model.pth\")\nassert best_path.exists(), f\"Missing checkpoint: {best_path}\"\n\nckpt = torch.load(best_path, map_location=\"cpu\")\nmodel.load_state_dict(ckpt[\"model_state_dict\"])\nmodel.eval()\nmodel.to(DEVICE)\n\n# ---- Helpers: tensor normalization consistent with eval_tfms ----\nMEAN = torch.tensor([0.485,0.456,0.406]).view(1,3,1,1)\nSTD  = torch.tensor([0.229,0.224,0.225]).view(1,3,1,1)\n\ndef prep_npz_batch(x_np: np.ndarray) -> torch.Tensor:\n    \"\"\"\n    x_np: (B,H,W,3) float32. Values might be 0..1 or 0..255.\n    Returns: (B,3,224,224) float32 normalized.\n    \"\"\"\n    x = torch.from_numpy(x_np).permute(0,3,1,2).float()  # B,C,H,W\n    # scale if looks like 0..255\n    if x.max().item() > 1.5:\n        x = x / 255.0\n    x = F.interpolate(x, size=(224,224), mode=\"bilinear\", align_corners=False)\n    x = (x - MEAN) / STD\n    return x\n\n@torch.no_grad()\ndef eval_npz_arrays(x: np.ndarray, y_onehot: np.ndarray, batch_size: int = 256):\n    y = np.argmax(y_onehot, axis=1).astype(np.int64)\n    cm = torch.zeros((NUM_CLASSES, NUM_CLASSES), dtype=torch.int64)\n    total = 0\n\n    for i in range(0, len(x), batch_size):\n        xb = prep_npz_batch(x[i:i+batch_size]).to(DEVICE)\n        yb = torch.from_numpy(y[i:i+batch_size]).to(DEVICE)\n        logits = model(xb)\n        cm += confusion_matrix_from_logits(logits.detach().cpu(), yb.detach().cpu(), NUM_CLASSES)\n        total += len(xb)\n\n    f1 = macro_f1_from_cm(cm)\n    return f1, cm, total\n\n# ---- Decide how to evaluate based on CSV structure ----\n# Case A: CSV is per-sample with npz_path + idx\ncols = set(df_ext.columns)\nper_sample = ((\"npz_path\" in cols or \"npz_file\" in cols) and (\"idx\" in cols or \"index\" in cols or \"i\" in cols))\n\nif per_sample:\n    npz_col = \"npz_path\" if \"npz_path\" in cols else \"npz_file\"\n    idx_col = \"idx\" if \"idx\" in cols else (\"index\" if \"index\" in cols else \"i\")\n\n    # Load each npz file once, then gather samples\n    # (works even if there are multiple npz files)\n    grouped = df_ext.groupby(npz_col)\n\n    all_logits = []\n    all_y = []\n    for npz_path, g in grouped:\n        npz_path = Path(npz_path)\n        assert npz_path.exists(), f\"Missing npz: {npz_path}\"\n        data = np.load(npz_path)\n        x = data[\"x\"]\n        y = data[\"y\"]\n        idxs = g[idx_col].astype(int).to_numpy()\n\n        x_sel = x[idxs]\n        y_sel = y[idxs]\n        f1, cm, n = eval_npz_arrays(x_sel, y_sel)\n        print(f\"NPZ file {npz_path.name} n={n} macro_f1={f1:.4f}\")\n        print(\"cm:\\n\", cm)\n\nelse:\n    # Case B: CSV is just metadata; evaluate directly from MUAZ test.npz\n    # (This matches what you previously printed: files = training/validation/test.npz)\n    MUAZ_DIR = Path(\"/kaggle/input/datasets/muazalzoubi/brain-tumor-gliomameningiomapituitary-not-tumors\")\n    if not MUAZ_DIR.exists():\n        # fallback: locate by searching\n        MUAZ_DIR = next(Path(\"/kaggle/input/datasets\").rglob(\"brain-tumor-gliomameningiomapituitary-not-tumors\"))\n    test_npz = MUAZ_DIR / \"test.npz\"\n    assert test_npz.exists(), f\"Missing expected file: {test_npz}\"\n\n    data = np.load(test_npz)\n    x = data[\"x\"]\n    y = data[\"y\"]\n    f1, cm, n = eval_npz_arrays(x, y)\n    print(f\"✅ MUAZ external test.npz n={n} macro_f1={f1:.4f}\")\n    print(\"confusion matrix (rows=true, cols=pred):\\n\", cm)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL: 11B_MUAZ_LABEL_VISUAL_CHECK - confirm what MUAZ label_id means\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\nMUAZ_DIR = Path(\"/kaggle/input/datasets/muazalzoubi/brain-tumor-gliomameningiomapituitary-not-tumors\")\ntest_npz = MUAZ_DIR / \"test.npz\"\ndata = np.load(test_npz)\nx = data[\"x\"]          # (N,150,150,3)\ny = data[\"y\"]          # (N,4) one-hot\nlabel_id = y.argmax(axis=1)\n\ndef show_grid(lbl, n=12):\n    idx = np.where(label_id == lbl)[0][:n]\n    fig, axes = plt.subplots(3, 4, figsize=(10, 7))\n    fig.suptitle(f\"MUAZ label_id = {lbl}\", fontsize=14)\n    for ax, i in zip(axes.ravel(), idx):\n        img = x[i]\n        if img.max() <= 1.5:  # likely 0..1\n            img = (img * 255).clip(0,255).astype(\"uint8\")\n        ax.imshow(img)\n        ax.axis(\"off\")\n    plt.tight_layout()\n    plt.show()\n\nfor lbl in [0,1,2,3]:\n    show_grid(lbl)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL: 11C_MUAZ_FIND_LABEL_MAPPING - find best MUAZ->OURS label_id mapping (no eyeballing)\n\nimport numpy as np\nimport itertools\nimport torch\nimport torch.nn.functional as F\nfrom pathlib import Path\n\nassert \"model\" in globals(), \"model not defined yet — run model load cell first\"\nassert \"DEVICE\" in globals(), \"DEVICE not defined yet — run device setup cell first\"\n\n# --- Load MUAZ test ---\nMUAZ_DIR = Path(\"/kaggle/input/datasets/muazalzoubi/brain-tumor-gliomameningiomapituitary-not-tumors\")\ntest_npz = MUAZ_DIR / \"test.npz\"\ndata = np.load(test_npz)\nx = data[\"x\"]\ny_onehot = data[\"y\"]\ny_muaz = y_onehot.argmax(axis=1).astype(np.int64)\nMUAZ_TO_OURS = np.array([0, 1, 3, 2], dtype=np.int64)\ny_true = MUAZ_TO_OURS[y_muaz]\n\nprint(\"counts y_muaz:\", np.bincount(y_muaz, minlength=4))\nprint(\"counts y_true:\", np.bincount(y_true, minlength=4))\n\n\n# --- Prepare like eval ---\nMEAN = torch.tensor([0.485,0.456,0.406]).view(1,3,1,1)\nSTD  = torch.tensor([0.229,0.224,0.225]).view(1,3,1,1)\n\ndef prep_npz_batch(x_np):\n    x_t = torch.from_numpy(x_np).permute(0,3,1,2).float()  # B,C,H,W\n    if x_t.max().item() > 1.5:\n        x_t = x_t / 255.0\n    x_t = F.interpolate(x_t, size=(224,224), mode=\"bilinear\", align_corners=False)\n    x_t = (x_t - MEAN) / STD\n    return x_t\n\n@torch.no_grad()\ndef predict_all(batch_size=256):\n    preds = []\n    for i in range(0, len(x), batch_size):\n        xb = prep_npz_batch(x[i:i+batch_size]).to(DEVICE)\n        logits = model(xb)\n        pred = logits.argmax(dim=1).detach().cpu().numpy()\n        preds.append(pred)\n    return np.concatenate(preds)\n\npred_ours = predict_all(batch_size=256 if DEVICE==\"cuda\" else 64)\n\ndef cm_from_ints(y_true, y_pred, k=4):\n    cm = np.zeros((k,k), dtype=np.int64)\n    for t,p in zip(y_true, y_pred):\n        cm[t,p] += 1\n    return cm\n\ndef macro_f1_from_cm(cm):\n    f1s = []\n    for c in range(cm.shape[0]):\n        tp = cm[c,c]\n        fp = cm[:,c].sum() - tp\n        fn = cm[c,:].sum() - tp\n        denom = (2*tp + fp + fn)\n        f1 = (2*tp / denom) if denom > 0 else 0.0\n        f1s.append(f1)\n    return float(np.mean(f1s))\n\n# Try all MUAZ->OURS mappings: mapped_true = perm[muaz_id]\nbest = None\nfor perm in itertools.permutations(range(4)):\n    mapped_true = np.array([perm[t] for t in y_muaz], dtype=np.int64)\n    cm = cm_from_ints(mapped_true, pred_ours, k=4)\n    f1 = macro_f1_from_cm(cm)\n    if (best is None) or (f1 > best[\"f1\"]):\n        best = {\"perm\": perm, \"f1\": f1, \"cm\": cm}\n\nprint(\"✅ Best MUAZ->OURS mapping (muaz_id -> ours_id):\", best[\"perm\"])\nprint(\"✅ macro_f1 after remap:\", round(best[\"f1\"], 4))\nprint(\"confusion matrix after remap (rows=true, cols=pred):\\n\", best[\"cm\"])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL: 11D_MUAZ_EXTERNAL_EVAL_MAPPED - Evaluate MUAZ test.npz with fixed label remap (muaz->ours)\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom pathlib import Path\n\n# --- Guards: model + device must exist ---\nassert \"model\" in globals(), \"model not defined — run the model init/load cell first.\"\nDEVICE = globals().get(\"DEVICE\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(DEVICE).eval()\n\nIMG_SIZE = getattr(globals().get(\"CFG\", object()), \"img_size\", 224)\n\n# --- MUAZ paths ---\nMUAZ_DIR = Path(\"/kaggle/input/datasets/muazalzoubi/brain-tumor-gliomameningiomapituitary-not-tumors\")\ntest_npz = MUAZ_DIR / \"test.npz\"\nassert test_npz.exists(), f\"Missing MUAZ test file: {test_npz}\"\n\n# --- Load MUAZ test ---\ndata = np.load(test_npz)\nx = data[\"x\"]                     # (N, H, W, 3)\ny_onehot = data[\"y\"]              # (N, 4) one-hot\ny_muaz = y_onehot.argmax(axis=1).astype(np.int64)\n\n# --- Fixed mapping discovered earlier: (0, 1, 3, 2) ---\n# Meaning: ours_id = MUAZ_TO_OURS[muaz_id]\nMUAZ_TO_OURS = np.array([0, 1, 3, 2], dtype=np.int64)\ny_true = MUAZ_TO_OURS[y_muaz]\n\nprint(\"counts y_muaz:\", np.bincount(y_muaz, minlength=4).tolist())\nprint(\"counts y_true:\", np.bincount(y_true, minlength=4).tolist())\nprint(\"✅ Using MUAZ_TO_OURS:\", MUAZ_TO_OURS.tolist(), \"(muaz_id -> ours_id)\")\n\n# --- Preprocess: match training (ImageNet mean/std + resize) ---\nMEAN = torch.tensor([0.485, 0.456, 0.406], device=DEVICE).view(1, 3, 1, 1)\nSTD  = torch.tensor([0.229, 0.224, 0.225], device=DEVICE).view(1, 3, 1, 1)\n\ndef prep_npz_batch(x_np: np.ndarray) -> torch.Tensor:\n    x_t = torch.from_numpy(x_np).permute(0, 3, 1, 2).float().to(DEVICE)  # B,C,H,W\n    # If MUAZ pixels are 0..255, scale to 0..1\n    if x_t.max().item() > 1.5:\n        x_t = x_t / 255.0\n    x_t = F.interpolate(x_t, size=(IMG_SIZE, IMG_SIZE), mode=\"bilinear\", align_corners=False)\n    x_t = (x_t - MEAN) / STD\n    return x_t\n\n@torch.no_grad()\ndef predict_all(batch_size: int = 256) -> np.ndarray:\n    preds = []\n    for i in range(0, len(x), batch_size):\n        xb = prep_npz_batch(x[i:i+batch_size])\n        # autocast speeds up on GPU; safe no-op on CPU\n        if DEVICE == \"cuda\":\n            with torch.amp.autocast(device_type=\"cuda\"):\n                logits = model(xb)\n        else:\n            logits = model(xb)\n        pred = logits.argmax(dim=1).detach().cpu().numpy()\n        preds.append(pred)\n    return np.concatenate(preds)\n\npred = predict_all(batch_size=256 if DEVICE == \"cuda\" else 64)\n\n# --- Metrics (no sklearn dependency) ---\ndef cm_from_ints(y_t: np.ndarray, y_p: np.ndarray, k: int = 4) -> np.ndarray:\n    cm = np.zeros((k, k), dtype=np.int64)\n    for t, p in zip(y_t, y_p):\n        cm[int(t), int(p)] += 1\n    return cm\n\ndef macro_f1_from_cm(cm: np.ndarray) -> float:\n    f1s = []\n    for c in range(cm.shape[0]):\n        tp = cm[c, c]\n        fp = cm[:, c].sum() - tp\n        fn = cm[c, :].sum() - tp\n        denom = (2 * tp + fp + fn)\n        f1 = (2 * tp / denom) if denom > 0 else 0.0\n        f1s.append(f1)\n    return float(np.mean(f1s))\n\ncm = cm_from_ints(y_true, pred, k=4)\nmacro_f1 = macro_f1_from_cm(cm)\n\nprint(f\"✅ MUAZ external test.npz n={len(x)} macro_f1={macro_f1:.4f}\")\nprint(\"confusion matrix (rows=true, cols=pred):\\n\", cm)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL: 11E_EXTERNAL_MUAZ_POLICY_EVAL - Build MUAZ test loader from NPZ + eval baseline vs policy\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom pathlib import Path\nfrom PIL import Image\n\n# ----------------------------\n# Preconditions\n# ----------------------------\nneed = [\"DEVICE\", \"OUT\", \"NUM_CLASSES\", \"model\", \"feat_extractor\", \"eval_tfms\", \"T\", \"tau_conf\", \"clf\"]\nmissing = [k for k in need if k not in globals()]\nassert not missing, f\"Missing prereqs: {missing}. Run: 05_DATASET_AND_TRANSFORMS, 06_DATALOADERS, calibration (10A-10D), domain guard (12E).\"\n\n# Use your locked tau_domain (from sweep recommendation)\nTAU_DOMAIN = float(globals().get(\"TAU_DOMAIN\", 0.580288))\n\nprint(\"Using policy thresholds:\")\nprint(\"  tau_conf  :\", float(tau_conf))\nprint(\"  tau_domain:\", TAU_DOMAIN)\nprint(\"  T         :\", float(T))\n\n# ----------------------------\n# Locate MUAZ NPZ file\n# ----------------------------\n# NOTE: this requires the MUAZ dataset to be attached as Kaggle input\n# Dataset: muazalzoubi/brain-tumor-gliomameningiomapituitary-not-tumors\n\nMUAZ_ROOT_CANDIDATES = [\n    Path(\"/kaggle/input/datasets/muazalzoubi/brain-tumor-gliomameningiomapituitary-not-tumors\"),\n    Path(\"/kaggle/input/brain-tumor-gliomameningiomapituitary-not-tumors\"),\n]\n\nmuaz_root = None\nfor p in MUAZ_ROOT_CANDIDATES:\n    if p.exists():\n        muaz_root = p\n        break\n\nassert muaz_root is not None, (\n    \"MUAZ dataset not found in /kaggle/input. Attach Kaggle dataset:\\n\"\n    \"muazalzoubi/brain-tumor-gliomameningiomapituitary-not-tumors\"\n)\n\nnpz_files = sorted(muaz_root.rglob(\"*.npz\"))\nassert len(npz_files) > 0, f\"No .npz files found under {muaz_root}\"\n\n# Prefer a file with 'test' in the name\ndef score_npz(p: Path) -> int:\n    name = p.name.lower()\n    if \"test\" in name:\n        return 0\n    if \"valid\" in name or \"val\" in name:\n        return 1\n    if \"train\" in name:\n        return 2\n    return 3\n\nnpz_files = sorted(npz_files, key=lambda p: (score_npz(p), len(p.name), p.name.lower()))\nnpz_path = npz_files[0]\n\nprint(\"\\n✅ Using MUAZ NPZ:\", npz_path)\n\n# ----------------------------\n# Load NPZ robustly (handle different key names)\n# ----------------------------\nnpz = np.load(npz_path, allow_pickle=True)\nkeys = list(npz.keys())\nprint(\"NPZ keys:\", keys)\n\ndef pick_first(keys, options):\n    for k in options:\n        if k in keys:\n            return k\n    return None\n\nx_key = pick_first(keys, [\"images\", \"image\", \"x\", \"X\", \"data\", \"arr_0\"])\ny_key = pick_first(keys, [\"labels\", \"label\", \"y\", \"Y\", \"targets\", \"target\", \"arr_1\"])\n\nassert x_key is not None, f\"Could not find image key in NPZ. Keys: {keys}\"\nassert y_key is not None, f\"Could not find label key in NPZ. Keys: {keys}\"\n\nX = npz[x_key]\ny_muaz = npz[y_key]\n\n# Ensure arrays are numpy arrays\nX = np.asarray(X)\ny_muaz = np.asarray(y_muaz).astype(np.int64)\n\nprint(\"y raw shape:\", y_muaz.shape, \"unique values:\", np.unique(y_muaz))\n\n# Convert one-hot labels -> class ids if needed\nif y_muaz.ndim == 2:\n    # Expect shape (N,4)\n    assert y_muaz.shape[1] == 4, f\"Expected one-hot with 4 columns, got {y_muaz.shape}\"\n    y_muaz = y_muaz.argmax(axis=1).astype(np.int64)\nelif y_muaz.ndim == 1:\n    y_muaz = y_muaz.astype(np.int64)\nelse:\n    raise AssertionError(f\"Unexpected label array shape: {y_muaz.shape}\")\n\nprint(\"y class shape:\", y_muaz.shape, \"classes:\", np.unique(y_muaz))\n\nassert X.shape[0] == y_muaz.shape[0], \"Mismatch between X and y length\"\n\n\n# ----------------------------\n# MUAZ label-id -> OUR label-id mapping (from your permutation discovery)\n# muaz_id -> ours_id\n# ----------------------------\nMUAZ_TO_OURS = {0: 0, 1: 1, 2: 3, 3: 2}\n\ndef map_labels_muaz_to_ours(y):\n    y2 = np.vectorize(MUAZ_TO_OURS.get)(y)\n    return y2.astype(np.int64)\n\ny_ours = map_labels_muaz_to_ours(y_muaz)\n\n# ----------------------------\n# Dataset: convert each slice to PIL RGB then apply eval_tfms\n# ----------------------------\ndef to_uint8_img(arr):\n    arr = np.asarray(arr)\n    # common shapes: (H,W), (H,W,1), (H,W,3)\n    if arr.ndim == 3 and arr.shape[-1] == 1:\n        arr = arr[..., 0]\n    if arr.ndim == 3 and arr.shape[-1] == 3:\n        # already 3-channel\n        pass\n\n    # scale to uint8\n    if arr.dtype != np.uint8:\n        arr = arr.astype(np.float32)\n        mn, mx = float(arr.min()), float(arr.max())\n        if mx > mn:\n            arr = (arr - mn) / (mx - mn)\n        else:\n            arr = np.zeros_like(arr)\n        arr = (arr * 255.0).clip(0, 255).astype(np.uint8)\n\n    # make PIL\n    if arr.ndim == 2:\n        img = Image.fromarray(arr).convert(\"RGB\")\n    else:\n        img = Image.fromarray(arr).convert(\"RGB\")\n    return img\n\nclass MUAZNPZDataset(Dataset):\n    def __init__(self, X, y, tfms):\n        self.X = X\n        self.y = y\n        self.tfms = tfms\n\n    def __len__(self):\n        return self.y.shape[0]\n\n    def __getitem__(self, idx):\n        img = to_uint8_img(self.X[idx])\n        x = self.tfms(img)\n        return x, int(self.y[idx]), int(idx), int(y_muaz[idx])  # x, y_ours, row_idx, y_muaz\n\nmuaz_ds = MUAZNPZDataset(X, y_ours, eval_tfms)\nmuaz_loader = DataLoader(muaz_ds, batch_size=128, shuffle=False, num_workers=2, pin_memory=True)\n\nprint(\"✅ MUAZ loader ready. rows:\", len(muaz_ds))\n\n# ----------------------------\n# Metrics helpers\n# ----------------------------\ndef confusion_matrix(preds, y_true, k):\n    cm = torch.zeros((k, k), dtype=torch.int64)\n    for t, p in zip(y_true.tolist(), preds.tolist()):\n        cm[t, p] += 1\n    return cm\n\ndef macro_f1_from_cm(cm):\n    k = cm.shape[0]\n    f1s = []\n    for c in range(k):\n        tp = cm[c, c].item()\n        fp = cm[:, c].sum().item() - tp\n        fn = cm[c, :].sum().item() - tp\n        denom = (2*tp + fp + fn)\n        f1 = 0.0 if denom == 0 else (2*tp / denom)\n        f1s.append(f1)\n    return float(sum(f1s) / k)\n\n# ----------------------------\n# Inference: baseline vs policy\n# ----------------------------\nmodel.eval()\nfeat_extractor.eval()\n\nrows = []\ny_true_all = []\npred_all = []\nabstain_all = []\n\nwith torch.no_grad():\n    for xb, yb, ridx, yb_muaz in muaz_loader:\n        xb = xb.to(DEVICE, non_blocking=True)\n        yb = yb.to(torch.int64)\n\n        logits = model(xb)\n        probs_cal = torch.softmax(logits / T, dim=1)\n        max_prob, pred_id = probs_cal.max(dim=1)\n\n        # domain score from embeddings\n        z = feat_extractor(xb).view(xb.shape[0], -1).detach().cpu().numpy()  # (B,512)\n        p_in = clf.predict_proba(z)[:, 1]\n\n        # abstain rule\n        max_prob_np = max_prob.detach().cpu().numpy()\n        abstain = (p_in < TAU_DOMAIN) | (max_prob_np < float(tau_conf))\n\n        # collect\n        y_true_all.append(yb)\n        pred_all.append(pred_id.detach().cpu())\n        abstain_all.append(torch.tensor(abstain, dtype=torch.bool))\n\n        # row-level outputs\n        ridx = ridx.detach().cpu().numpy()\n        yb_muaz = yb_muaz.detach().cpu().numpy()\n        yb_np = yb.detach().cpu().numpy()\n        pred_np = pred_id.detach().cpu().numpy()\n\n        for j in range(len(ridx)):\n            rows.append({\n                \"row_idx\": int(ridx[j]),\n                \"y_muaz\": int(yb_muaz[j]),\n                \"y_ours\": int(yb_np[j]),\n                \"pred_id\": int(pred_np[j]),\n                \"max_prob_cal\": float(max_prob_np[j]),\n                \"p_in_domain\": float(p_in[j]),\n                \"abstain\": bool(abstain[j]),\n            })\n\ny_true = torch.cat(y_true_all, dim=0)\npred = torch.cat(pred_all, dim=0)\nabstain_mask = torch.cat(abstain_all, dim=0)\n\n# Baseline metrics (no abstain)\nacc = float((pred == y_true).to(torch.float32).mean().item())\ncm = confusion_matrix(pred, y_true, NUM_CLASSES)\nmacro_f1 = macro_f1_from_cm(cm)\n\n# Policy metrics (accepted only)\naccepted = ~abstain_mask\ncoverage = float(accepted.to(torch.float32).mean().item())\nabstain_rate = 1.0 - coverage\n\nif accepted.any():\n    y_acc = y_true[accepted]\n    p_acc = pred[accepted]\n    acc_acc = float((p_acc == y_acc).to(torch.float32).mean().item())\n    cm_acc = confusion_matrix(p_acc, y_acc, NUM_CLASSES)\n    macro_f1_acc = macro_f1_from_cm(cm_acc)\nelse:\n    acc_acc, macro_f1_acc = None, None\n\nprint(\"\\n✅ MUAZ BASELINE (mapped labels)\")\nprint(\"rows:\", int(y_true.shape[0]))\nprint(\"acc:\", acc)\nprint(\"macro_f1:\", macro_f1)\n\nprint(\"\\n✅ MUAZ WITH POLICY (tau_conf + tau_domain)\")\nprint(\"coverage:\", coverage)\nprint(\"abstain_rate:\", abstain_rate)\nprint(\"acc_on_accepted:\", acc_acc)\nprint(\"macro_f1_on_accepted:\", macro_f1_acc)\n\n# Save artifacts\ndf_rows = pd.DataFrame(rows)\ndf_rows.to_csv(OUT / \"muaz_policy_outputs.csv\", index=False)\n\npd.DataFrame([{\n    \"npz_path\": str(npz_path),\n    \"rows\": int(y_true.shape[0]),\n    \"baseline_acc\": acc,\n    \"baseline_macro_f1\": macro_f1,\n    \"policy_coverage\": coverage,\n    \"policy_abstain_rate\": abstain_rate,\n    \"policy_acc_on_accepted\": acc_acc,\n    \"policy_macro_f1_on_accepted\": macro_f1_acc,\n    \"tau_conf\": float(tau_conf),\n    \"tau_domain\": float(TAU_DOMAIN),\n    \"T\": float(T),\n}]).to_csv(OUT / \"muaz_policy_report.csv\", index=False)\n\nprint(\"\\nsaved ->\", OUT / \"muaz_policy_outputs.csv\")\nprint(\"saved ->\", OUT / \"muaz_policy_report.csv\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL: 12_AUDIT_CHALLENGE_CONFIDENCE - confidence + prediction audit on challenge domains (handles JPG/PNG + DICOM)\n\nfrom __future__ import annotations\n\nfrom pathlib import Path\nfrom typing import List, Tuple, Dict, Any\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom PIL import Image\nimport pydicom\n\n# ---- display prefs (once) ----\npd.set_option(\"display.max_columns\", 100)\npd.set_option(\"display.width\", 160)\npd.set_option(\"display.max_colwidth\", 80)\n\n# ---- guards ----\nassert \"model\" in globals(), \"model not defined — run model init/load cells first.\"\nassert \"DEVICE\" in globals(), \"DEVICE not defined — run device cell first.\"\nassert \"eval_tfms\" in globals(), \"eval_tfms not defined — run transforms cell first.\"\n\n# ---- locate challenge split ----\nARTIFACTS_DIR = Path(split_train).parent if \"split_train\" in globals() else globals().get(\"ARTIFACTS_DIR\")\nassert ARTIFACTS_DIR is not None, \"ARTIFACTS_DIR not found — run Cell 00 that locates artifacts first.\"\n\nsplit_chal_path = Path(ARTIFACTS_DIR) / \"split_challenge_sampled.csv\"\nassert split_chal_path.exists(), f\"Missing: {split_chal_path}\"\n\ndf_chal = pd.read_csv(split_chal_path)\nprint(\"challenge rows:\", len(df_chal))\nprint(\"domains:\", df_chal[\"domain\"].value_counts().to_dict())\ndisplay(df_chal.head(3))\n\n# ---- load checkpoint (best) ----\nbest_path = Path(\"/kaggle/working/train_artifacts/best_model.pth\")\nassert best_path.exists(), f\"Missing checkpoint: {best_path}\"\n\nckpt = torch.load(best_path, map_location=\"cpu\")\nmodel.load_state_dict(ckpt[\"model_state_dict\"])\nmodel.to(DEVICE).eval()\n\n# ---- label map (OURS) ----\nID2NAME = {0: \"glioma\", 1: \"meningioma\", 2: \"pituitary\", 3: \"notumor\"}\n\ndef load_any_image(path: Path) -> Image.Image:\n    \"\"\"Load JPG/PNG or DICOM and return RGB PIL.\"\"\"\n    suffix = path.suffix.lower()\n    if suffix == \".dcm\":\n        ds = pydicom.dcmread(str(path), force=True)\n        arr = ds.pixel_array.astype(np.float32)\n\n        # min-max normalize per slice for stable conversion\n        arr = arr - np.min(arr)\n        denom = np.max(arr) + 1e-6\n        arr = (arr / denom * 255.0).clip(0, 255).astype(np.uint8)\n\n        return Image.fromarray(arr).convert(\"RGB\")\n\n    return Image.open(path).convert(\"RGB\")\n\n@torch.no_grad()\ndef infer_rows(\n    rows: pd.DataFrame,\n    batch_size: int = 128,\n) -> Tuple[pd.DataFrame, Dict[str, int]]:\n    \"\"\"\n    Runs inference over df rows (expects `path` col).\n    Returns (df_scored, stats) where df_scored contains only successfully read rows.\n    \"\"\"\n    ok_rows = []\n    failed = 0\n\n    # We'll build batches of tensors and keep their corresponding row dicts.\n    for i in range(0, len(rows), batch_size):\n        chunk = rows.iloc[i:i+batch_size]\n        xs = []\n        meta = []\n\n        for _, r in chunk.iterrows():\n            p = Path(r[\"path\"])\n            try:\n                img = load_any_image(p)\n                x = eval_tfms(img)\n                xs.append(x)\n                meta.append(r.to_dict())\n            except Exception:\n                failed += 1\n\n        if not xs:\n            continue\n\n        xb = torch.stack(xs).to(DEVICE)\n        logits = model(xb)\n        prob = torch.softmax(logits, dim=1)\n        max_prob, pred_id = torch.max(prob, dim=1)\n\n        pred_id = pred_id.detach().cpu().numpy().astype(np.int64)\n        max_prob = max_prob.detach().cpu().numpy().astype(np.float32)\n\n        for m, pid, conf in zip(meta, pred_id, max_prob):\n            m[\"pred_id\"] = int(pid)\n            m[\"max_prob\"] = float(conf)\n            m[\"pred_name\"] = ID2NAME[int(pid)]\n            ok_rows.append(m)\n\n    df_scored = pd.DataFrame(ok_rows)\n    stats = {\"failed_reads\": failed, \"scored_rows\": len(df_scored)}\n    return df_scored, stats\n\nbatch = 256 if DEVICE == \"cuda\" else 32\ndf_out, stats = infer_rows(df_chal, batch_size=batch)\nprint(f\"✅ scored_rows={stats['scored_rows']}  failed_reads={stats['failed_reads']}  batch={batch}\")\n\nassert len(df_out) > 0, \"No rows scored — check paths and loaders.\"\n\n# ---- confidence summary per domain ----\ndef q(x: np.ndarray, quant: float) -> float:\n    return float(np.quantile(x, quant))\n\nsummary = []\nfor dom, g in df_out.groupby(\"domain\"):\n    conf = g[\"max_prob\"].to_numpy()\n    top_pred = g[\"pred_name\"].value_counts().idxmax()\n    top_pred_frac = float((g[\"pred_name\"] == top_pred).mean())\n\n    summary.append({\n        \"domain\": dom,\n        \"n\": int(len(g)),\n        \"mean_conf\": float(conf.mean()),\n        \"p50\": q(conf, 0.50),\n        \"p90\": q(conf, 0.90),\n        \"p95\": q(conf, 0.95),\n        \"p99\": q(conf, 0.99),\n        \"frac_ge_0.90\": float((conf >= 0.90).mean()),\n        \"frac_ge_0.95\": float((conf >= 0.95).mean()),\n        \"top_pred\": top_pred,\n        \"top_pred_frac\": top_pred_frac,\n    })\n\ndf_summary = pd.DataFrame(summary).sort_values(\"domain\")\ndisplay(df_summary)\n\n# ---- NEW: prediction distribution per domain (counts + %) ----\ndf_pred_dist = (\n    df_out\n    .groupby([\"domain\", \"pred_name\"])\n    .size()\n    .reset_index(name=\"count\")\n)\ndf_pred_dist[\"pct\"] = df_pred_dist.groupby(\"domain\")[\"count\"].transform(lambda s: s / s.sum())\ndf_pred_dist = df_pred_dist.sort_values([\"domain\", \"count\"], ascending=[True, False])\ndisplay(df_pred_dist.head(20))\n\n# ---- NEW: mean confidence by (domain, pred_name) ----\ndf_conf_by_pred = (\n    df_out\n    .groupby([\"domain\", \"pred_name\"])[\"max_prob\"]\n    .agg(n=\"count\", mean_conf=\"mean\", p50=lambda s: float(np.quantile(s, 0.50)), p95=lambda s: float(np.quantile(s, 0.95)))\n    .reset_index()\n    .sort_values([\"domain\", \"n\"], ascending=[True, False])\n)\ndisplay(df_conf_by_pred.head(20))\n\n# ---- save artifacts ----\nOUT = Path(\"/kaggle/working/train_artifacts\")\nOUT.mkdir(parents=True, exist_ok=True)\n\ndf_out.to_csv(OUT / \"challenge_predictions.csv\", index=False)\ndf_summary.to_csv(OUT / \"challenge_confidence_summary.csv\", index=False)\ndf_pred_dist.to_csv(OUT / \"challenge_pred_distribution.csv\", index=False)\ndf_conf_by_pred.to_csv(OUT / \"challenge_conf_by_pred.csv\", index=False)\n\nprint(\"✅ wrote:\", OUT / \"challenge_predictions.csv\")\nprint(\"✅ wrote:\", OUT / \"challenge_confidence_summary.csv\")\nprint(\"✅ wrote:\", OUT / \"challenge_pred_distribution.csv\")\nprint(\"✅ wrote:\", OUT / \"challenge_conf_by_pred.csv\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL: 12B_OOD_GUARD_POLICY - OOD guard + abstain policy on challenge (diag-Mahalanobis + calibrated confidence)\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport pydicom\nfrom pathlib import Path\n\n# ----------------------------\n# Preconditions / guardrails\n# ----------------------------\nneed = [\"DEVICE\", \"OUT\", \"NUM_CLASSES\", \"model\", \"val_loader\", \"eval_tfms\", \"df_chal\", \"T\"]\nmissing = [k for k in need if k not in globals()]\nassert not missing, f\"Missing prereqs: {missing}. Run: 01_CONFIG, 05_DATASET_AND_TRANSFORMS, 06_DATALOADERS, 09/10, 10A-10D, and 12.\"\n\n# ----------------------------\n# 1) Pick tau_conf from VAL (calibrated)\n# ----------------------------\nKEEP_RATE = 0.95  # keep 95% of in-domain samples; abstain ~5%\nassert \"val_logits\" in globals() and \"val_y\" in globals(), \"Run calibration Cell 10A first (val_logits/val_y missing).\"\n\nval_probs_cal = torch.softmax(val_logits / T, dim=1)\nval_maxprob_cal = val_probs_cal.max(dim=1).values\ntau_conf = float(torch.quantile(val_maxprob_cal, 1.0 - KEEP_RATE).item())\n\nprint(\"✅ tau_conf chosen from VAL (calibrated)\")\nprint(\"KEEP_RATE:\", KEEP_RATE)\nprint(\"tau_conf:\", tau_conf)\n\n# ----------------------------\n# 2) Robust loader for JPG/PNG + DICOM\n# ----------------------------\ndef load_any_image(path: str) -> Image.Image:\n    p = Path(path)\n    suf = p.suffix.lower()\n\n    if suf == \".dcm\":\n        dcm = pydicom.dcmread(str(p), force=True)\n        arr = dcm.pixel_array.astype(np.float32)\n\n        # demo-safe min-max per slice (consistent with your existing challenge audit)\n        mn, mx = float(arr.min()), float(arr.max())\n        if mx > mn:\n            arr = (arr - mn) / (mx - mn)\n        else:\n            arr = np.zeros_like(arr)\n\n        arr = (arr * 255.0).clip(0, 255).astype(np.uint8)\n        return Image.fromarray(arr).convert(\"RGB\")\n\n    return Image.open(str(p)).convert(\"RGB\")\n\nclass ChallengeDataset(Dataset):\n    def __init__(self, df, tfms):\n        self.df = df.reset_index(drop=True)\n        self.tfms = tfms\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img = load_any_image(row[\"path\"])\n        x = self.tfms(img)\n        return x, idx\n\n# ----------------------------\n# 3) Build feature extractor + in-domain embedding stats (VAL)\n#    Using ResNet18: penultimate features = model without final FC\n# ----------------------------\nfeat_extractor = torch.nn.Sequential(*list(model.children())[:-1]).to(DEVICE)\nfeat_extractor.eval()\nmodel.eval()\n\n@torch.no_grad()\ndef collect_embeddings(loader):\n    embs = []\n    for xb, _ in loader:\n        xb = xb.to(DEVICE, non_blocking=True)\n        z = feat_extractor(xb)          # (B,512,1,1)\n        z = z.view(z.shape[0], -1)      # (B,512)\n        embs.append(z.detach().cpu())\n    return torch.cat(embs, dim=0)       # (N,512)\n\n# Collect in-domain embeddings from VAL\n# NOTE: this iterates val_loader once; acceptable for demo-grade, reproducible\nval_embs = collect_embeddings(val_loader)  # (N,512)\n\nmu = val_embs.mean(dim=0)\nvar = val_embs.var(dim=0, unbiased=False) + 1e-6  # diagonal covariance (stability)\n\n# diag Mahalanobis distance for VAL, then set tau_ood at 99th percentile\nval_dist = (((val_embs - mu) ** 2) / var).sum(dim=1)\nOOD_Q = 0.99\ntau_ood = float(torch.quantile(val_dist, OOD_Q).item())\n\nprint(\"\\n✅ OOD threshold tau_ood (diag-Mahalanobis)\")\nprint(\"OOD_Q:\", OOD_Q)\nprint(\"tau_ood:\", tau_ood)\n\n# ----------------------------\n# 4) Run challenge inference + policy\n# ----------------------------\nchal_ds = ChallengeDataset(df_chal, eval_tfms)\n\n# num_workers=0 is the safest for pydicom; avoids worker crashes\nchal_loader = DataLoader(chal_ds, batch_size=64, shuffle=False, num_workers=0, pin_memory=True)\n\nrows = []\n\n@torch.no_grad()\ndef run_policy(loader):\n    for xb, idxs in loader:\n        xb = xb.to(DEVICE, non_blocking=True)\n\n        # logits -> calibrated probs for confidence\n        logits = model(xb)\n        probs_cal = torch.softmax(logits / T, dim=1)\n        max_prob, pred_id = probs_cal.max(dim=1)\n\n        # embeddings for OOD score\n        z = feat_extractor(xb).view(xb.shape[0], -1).detach().cpu()\n        dist = (((z - mu) ** 2) / var).sum(dim=1)\n\n        max_prob = max_prob.detach().cpu()\n        pred_id = pred_id.detach().cpu()\n\n        for j in range(xb.shape[0]):\n            ridx = int(idxs[j].item())\n            # domain column name differs sometimes; try common ones\n            dom = None\n            for k in [\"domain\", \"source\", \"dataset\", \"origin\"]:\n                if k in df_chal.columns:\n                    dom = df_chal.iloc[ridx][k]\n                    break\n            if dom is None:\n                dom = \"unknown\"\n\n            rows.append({\n                \"row_idx\": ridx,\n                \"path\": df_chal.iloc[ridx][\"path\"],\n                \"domain\": dom,\n                \"pred_id\": int(pred_id[j].item()),\n                \"max_prob_cal\": float(max_prob[j].item()),\n                \"ood_dist\": float(dist[j].item()),\n            })\n\nrun_policy(chal_loader)\ndf_out = pd.DataFrame(rows)\n\n# pred_id -> pred_name mapping\nif \"ID2NAME\" in globals():\n    id2name = ID2NAME\nelse:\n    id2name = {0: \"glioma\", 1: \"meningioma\", 2: \"pituitary\", 3: \"notumor\"}\n\ndf_out[\"pred_name\"] = df_out[\"pred_id\"].map(id2name)\n\n# Policy:\n# 1) if OOD -> ABSTAIN\n# 2) else if low confidence -> ABSTAIN\n# 3) else -> predicted class\ndf_out[\"is_ood\"] = df_out[\"ood_dist\"] > tau_ood\ndf_out[\"abstain_lowconf\"] = df_out[\"max_prob_cal\"] < tau_conf\ndf_out[\"final_action\"] = np.where(df_out[\"is_ood\"] | df_out[\"abstain_lowconf\"], \"ABSTAIN\", df_out[\"pred_name\"])\n\ntumor_set = {\"glioma\", \"meningioma\", \"pituitary\"}\ndf_out[\"is_tumor_pred\"] = df_out[\"pred_name\"].isin(tumor_set)\n\npre_tumor = int(df_out[\"is_tumor_pred\"].sum())\npost_tumor = int(df_out[\"final_action\"].isin(tumor_set).sum())\nabstain_total = int((df_out[\"final_action\"] == \"ABSTAIN\").sum())\n\nprint(\"\\n✅ Challenge policy summary:\")\nprint(\"Total rows:\", len(df_out))\nprint(\"Tumor preds BEFORE policy:\", pre_tumor)\nprint(\"Tumor preds AFTER policy:\", post_tumor)\nprint(\"Total ABSTAINS:\", abstain_total)\n\n# Save artifacts\ndf_out.to_csv(OUT / \"challenge_policy_outputs.csv\", index=False)\n\nsummary = pd.DataFrame([{\n    \"keep_rate_in_domain\": KEEP_RATE,\n    \"tau_conf\": tau_conf,\n    \"ood_quantile\": OOD_Q,\n    \"tau_ood\": tau_ood,\n    \"challenge_rows\": len(df_out),\n    \"tumor_preds_before\": pre_tumor,\n    \"tumor_preds_after\": post_tumor,\n    \"abstain_total\": abstain_total,\n}])\nsummary.to_csv(OUT / \"policy_summary.csv\", index=False)\n\nprint(\"saved ->\", OUT / \"challenge_policy_outputs.csv\")\nprint(\"saved ->\", OUT / \"policy_summary.csv\")\n\n# Show worst offenders (highest-confidence tumor preds) and whether policy catches them\ntry:\n    from IPython.display import display\n    df_tumor = df_out[df_out[\"is_tumor_pred\"]].sort_values(\"max_prob_cal\", ascending=False)\n    print(\"\\nTop 10 tumor preds by confidence (with policy flags):\")\n    display(df_tumor[[\"domain\",\"pred_name\",\"max_prob_cal\",\"is_ood\",\"abstain_lowconf\",\"final_action\",\"path\"]].head(10))\nexcept Exception as e:\n    print(\"Display skipped:\", e)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T16:01:14.158599Z","iopub.execute_input":"2026-02-14T16:01:14.159040Z","iopub.status.idle":"2026-02-14T16:11:12.926345Z","shell.execute_reply.started":"2026-02-14T16:01:14.159008Z","shell.execute_reply":"2026-02-14T16:11:12.925100Z"}},"outputs":[{"name":"stdout","text":"✅ tau_conf chosen from VAL (calibrated)\nKEEP_RATE: 0.95\ntau_conf: 0.9921115636825562\n\n✅ OOD threshold tau_ood (diag-Mahalanobis)\nOOD_Q: 0.99\ntau_ood: 1089.089599609375\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n  warnings.warn(warn_msg)\n","output_type":"stream"},{"name":"stdout","text":"\n✅ Challenge policy summary:\nTotal rows: 4275\nTumor preds BEFORE policy: 563\nTumor preds AFTER policy: 62\nTotal ABSTAINS: 1282\nsaved -> /kaggle/working/train_artifacts/challenge_policy_outputs.csv\nsaved -> /kaggle/working/train_artifacts/policy_summary.csv\n\nTop 10 tumor preds by confidence (with policy flags):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"            domain   pred_name  max_prob_cal  is_ood  abstain_lowconf  \\\n4008        stroke  meningioma      0.999789   False            False   \n2808        stroke  meningioma      0.999732   False            False   \n115   normal_dicom  meningioma      0.999600   False            False   \n2371        stroke  meningioma      0.999562   False            False   \n4254        stroke  meningioma      0.999487   False            False   \n2633        stroke  meningioma      0.999435   False            False   \n3454        stroke  meningioma      0.999348   False            False   \n206   normal_dicom  meningioma      0.999341   False            False   \n3999        stroke  meningioma      0.999204   False            False   \n165   normal_dicom  meningioma      0.998870   False            False   \n\n     final_action                                               path  \n4008   meningioma  /kaggle/input/datasets/mitangshu11/brain-strok...  \n2808   meningioma  /kaggle/input/datasets/mitangshu11/brain-strok...  \n115    meningioma  /kaggle/input/datasets/trainingdatapro/dicom-b...  \n2371   meningioma  /kaggle/input/datasets/mitangshu11/brain-strok...  \n4254   meningioma  /kaggle/input/datasets/mitangshu11/brain-strok...  \n2633   meningioma  /kaggle/input/datasets/mitangshu11/brain-strok...  \n3454   meningioma  /kaggle/input/datasets/mitangshu11/brain-strok...  \n206    meningioma  /kaggle/input/datasets/trainingdatapro/dicom-b...  \n3999   meningioma  /kaggle/input/datasets/mitangshu11/brain-strok...  \n165    meningioma  /kaggle/input/datasets/trainingdatapro/dicom-b...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>domain</th>\n      <th>pred_name</th>\n      <th>max_prob_cal</th>\n      <th>is_ood</th>\n      <th>abstain_lowconf</th>\n      <th>final_action</th>\n      <th>path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4008</th>\n      <td>stroke</td>\n      <td>meningioma</td>\n      <td>0.999789</td>\n      <td>False</td>\n      <td>False</td>\n      <td>meningioma</td>\n      <td>/kaggle/input/datasets/mitangshu11/brain-strok...</td>\n    </tr>\n    <tr>\n      <th>2808</th>\n      <td>stroke</td>\n      <td>meningioma</td>\n      <td>0.999732</td>\n      <td>False</td>\n      <td>False</td>\n      <td>meningioma</td>\n      <td>/kaggle/input/datasets/mitangshu11/brain-strok...</td>\n    </tr>\n    <tr>\n      <th>115</th>\n      <td>normal_dicom</td>\n      <td>meningioma</td>\n      <td>0.999600</td>\n      <td>False</td>\n      <td>False</td>\n      <td>meningioma</td>\n      <td>/kaggle/input/datasets/trainingdatapro/dicom-b...</td>\n    </tr>\n    <tr>\n      <th>2371</th>\n      <td>stroke</td>\n      <td>meningioma</td>\n      <td>0.999562</td>\n      <td>False</td>\n      <td>False</td>\n      <td>meningioma</td>\n      <td>/kaggle/input/datasets/mitangshu11/brain-strok...</td>\n    </tr>\n    <tr>\n      <th>4254</th>\n      <td>stroke</td>\n      <td>meningioma</td>\n      <td>0.999487</td>\n      <td>False</td>\n      <td>False</td>\n      <td>meningioma</td>\n      <td>/kaggle/input/datasets/mitangshu11/brain-strok...</td>\n    </tr>\n    <tr>\n      <th>2633</th>\n      <td>stroke</td>\n      <td>meningioma</td>\n      <td>0.999435</td>\n      <td>False</td>\n      <td>False</td>\n      <td>meningioma</td>\n      <td>/kaggle/input/datasets/mitangshu11/brain-strok...</td>\n    </tr>\n    <tr>\n      <th>3454</th>\n      <td>stroke</td>\n      <td>meningioma</td>\n      <td>0.999348</td>\n      <td>False</td>\n      <td>False</td>\n      <td>meningioma</td>\n      <td>/kaggle/input/datasets/mitangshu11/brain-strok...</td>\n    </tr>\n    <tr>\n      <th>206</th>\n      <td>normal_dicom</td>\n      <td>meningioma</td>\n      <td>0.999341</td>\n      <td>False</td>\n      <td>False</td>\n      <td>meningioma</td>\n      <td>/kaggle/input/datasets/trainingdatapro/dicom-b...</td>\n    </tr>\n    <tr>\n      <th>3999</th>\n      <td>stroke</td>\n      <td>meningioma</td>\n      <td>0.999204</td>\n      <td>False</td>\n      <td>False</td>\n      <td>meningioma</td>\n      <td>/kaggle/input/datasets/mitangshu11/brain-strok...</td>\n    </tr>\n    <tr>\n      <th>165</th>\n      <td>normal_dicom</td>\n      <td>meningioma</td>\n      <td>0.998870</td>\n      <td>False</td>\n      <td>False</td>\n      <td>meningioma</td>\n      <td>/kaggle/input/datasets/trainingdatapro/dicom-b...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"# CELL: 12C_CLASS_CONDITIONAL_INLIER_GUARD - Catch high-confidence OOD tumor preds via class-conditional distance\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader\n\n# Preconditions: 12B must have run\nneed = [\"DEVICE\", \"OUT\", \"NUM_CLASSES\", \"model\", \"val_loader\", \"feat_extractor\", \"tau_conf\", \"tau_ood\", \"T\", \"df_out\"]\nmissing = [k for k in need if k not in globals()]\nassert not missing, f\"Missing prereqs: {missing}. Run 12B first.\"\n\nmodel.eval()\nfeat_extractor.eval()\n\n@torch.no_grad()\ndef collect_embeddings_and_labels(loader):\n    Zs, Ys = [], []\n    for xb, yb in loader:\n        xb = xb.to(DEVICE, non_blocking=True)\n        z = feat_extractor(xb).view(xb.shape[0], -1).detach().cpu()\n        Zs.append(z)\n        Ys.append(yb.detach().cpu())\n    return torch.cat(Zs, dim=0), torch.cat(Ys, dim=0)\n\n# 1) Build class-conditional stats from in-domain VAL embeddings\nZ_val, y_val = collect_embeddings_and_labels(val_loader)  # (N, D), (N,)\nD = Z_val.shape[1]\n\nmu_c = torch.zeros((NUM_CLASSES, D))\nvar_c = torch.zeros((NUM_CLASSES, D))\ntau_c = torch.zeros((NUM_CLASSES,))\n\nQ_CLASS = 0.99  # class-conditional inlier threshold\n\nfor c in range(NUM_CLASSES):\n    idx = torch.where(y_val == c)[0]\n    assert idx.numel() > 10, f\"Too few val examples for class {c}: {idx.numel()}\"\n    Zc = Z_val[idx]\n    mu = Zc.mean(dim=0)\n    var = Zc.var(dim=0, unbiased=False) + 1e-6\n\n    # class-conditional diag Mahalanobis dist for in-domain examples of class c\n    dist_c = (((Zc - mu) ** 2) / var).sum(dim=1)\n    tau = float(torch.quantile(dist_c, Q_CLASS).item())\n\n    mu_c[c] = mu\n    var_c[c] = var\n    tau_c[c] = tau\n\nprint(\"✅ Built class-conditional thresholds (tau_c) at quantile:\", Q_CLASS)\nprint(\"tau_c:\", [float(x) for x in tau_c])\n\n# 2) Compute embedding distances for each challenge row to its predicted class cluster\n# We need embeddings for challenge again (df_out contains row_idx in original df_chal order)\n# We'll reuse the cached chal embeddings approach by reloading from paths.\n\nfrom PIL import Image\nimport pydicom\nfrom pathlib import Path\nfrom torch.utils.data import Dataset\n\ndef load_any_image(path: str) -> Image.Image:\n    p = Path(path)\n    suf = p.suffix.lower()\n    if suf == \".dcm\":\n        dcm = pydicom.dcmread(str(p), force=True)\n        arr = dcm.pixel_array.astype(np.float32)\n        mn, mx = float(arr.min()), float(arr.max())\n        if mx > mn:\n            arr = (arr - mn) / (mx - mn)\n        else:\n            arr = np.zeros_like(arr)\n        arr = (arr * 255.0).clip(0,255).astype(np.uint8)\n        return Image.fromarray(arr).convert(\"RGB\")\n    return Image.open(str(p)).convert(\"RGB\")\n\nclass ChallengeEmbedDataset(Dataset):\n    def __init__(self, df_out, tfms):\n        self.df = df_out.reset_index(drop=True)\n        self.tfms = tfms\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        path = self.df.iloc[idx][\"path\"]\n        img = load_any_image(path)\n        x = self.tfms(img)\n        return x, idx\n\nchal_ds2 = ChallengeEmbedDataset(df_out, eval_tfms)\nchal_loader2 = DataLoader(chal_ds2, batch_size=64, shuffle=False, num_workers=0, pin_memory=True)\n\nood_dist_predclass = np.zeros(len(df_out), dtype=np.float32)\n\n@torch.no_grad()\ndef compute_predclass_dist(loader):\n    for xb, idxs in loader:\n        xb = xb.to(DEVICE, non_blocking=True)\n        z = feat_extractor(xb).view(xb.shape[0], -1).detach().cpu()  # (B,D)\n\n        for j in range(z.shape[0]):\n            i = int(idxs[j].item())\n            pred = int(df_out.iloc[i][\"pred_id\"])  # predicted class from 12B\n            mu = mu_c[pred]\n            var = var_c[pred]\n            d = (((z[j] - mu) ** 2) / var).sum().item()\n            ood_dist_predclass[i] = d\n\ncompute_predclass_dist(chal_loader2)\ndf_out[\"predclass_dist\"] = ood_dist_predclass\n\n# 3) New guard: if far from predicted class cluster => abstain\ndf_out[\"is_outlier_predclass\"] = df_out[\"predclass_dist\"] > df_out[\"pred_id\"].map(lambda c: float(tau_c[c]))\n\ntumor_set = {\"glioma\",\"meningioma\",\"pituitary\"}\ndf_out[\"is_tumor_pred\"] = df_out[\"pred_name\"].isin(tumor_set)\n\n# Policy v2:\n# ABSTAIN if (global OOD) OR (low confidence) OR (outlier for predicted class)\ndf_out[\"final_action_v2\"] = np.where(\n    df_out[\"is_ood\"] | df_out[\"abstain_lowconf\"] | df_out[\"is_outlier_predclass\"],\n    \"ABSTAIN\",\n    df_out[\"pred_name\"]\n)\n\npre_tumor = int(df_out[\"is_tumor_pred\"].sum())\npost_tumor_v1 = int(df_out[\"final_action\"].isin(tumor_set).sum())\npost_tumor_v2 = int(df_out[\"final_action_v2\"].isin(tumor_set).sum())\nabstain_v1 = int((df_out[\"final_action\"] == \"ABSTAIN\").sum())\nabstain_v2 = int((df_out[\"final_action_v2\"] == \"ABSTAIN\").sum())\n\nprint(\"\\n✅ Policy comparison on challenge:\")\nprint(\"Tumor preds BEFORE policy:\", pre_tumor)\nprint(\"Tumor preds AFTER policy v1:\", post_tumor_v1)\nprint(\"Tumor preds AFTER policy v2:\", post_tumor_v2)\nprint(\"ABSTAINS v1:\", abstain_v1)\nprint(\"ABSTAINS v2:\", abstain_v2)\n\n# Save artifacts\ndf_out.to_csv(OUT / \"challenge_policy_outputs_v2.csv\", index=False)\n\nsummary2 = pd.DataFrame([{\n    \"tau_conf\": float(tau_conf),\n    \"tau_ood\": float(tau_ood),\n    \"q_class\": float(Q_CLASS),\n    \"tau_c\": [float(x) for x in tau_c],\n    \"challenge_rows\": int(len(df_out)),\n    \"tumor_before\": pre_tumor,\n    \"tumor_after_v1\": post_tumor_v1,\n    \"tumor_after_v2\": post_tumor_v2,\n    \"abstain_v1\": abstain_v1,\n    \"abstain_v2\": abstain_v2,\n}])\nsummary2.to_csv(OUT / \"policy_summary_v2.csv\", index=False)\n\nprint(\"saved ->\", OUT / \"challenge_policy_outputs_v2.csv\")\nprint(\"saved ->\", OUT / \"policy_summary_v2.csv\")\n\n# Show top remaining tumor preds after v2 (if any)\ntry:\n    from IPython.display import display\n    df_remain = df_out[df_out[\"final_action_v2\"].isin(tumor_set)].sort_values(\"max_prob_cal\", ascending=False)\n    print(\"\\nTop 10 remaining tumor preds AFTER v2:\")\n    display(df_remain[[\"domain\",\"pred_name\",\"max_prob_cal\",\"is_ood\",\"abstain_lowconf\",\"is_outlier_predclass\",\"final_action_v2\",\"path\"]].head(10))\nexcept Exception as e:\n    print(\"Display skipped:\", e)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL: 12D_DOMAIN_CLASSIFIER_OOD_GUARD - Train a domain (in-domain vs OOD) guard on embeddings + apply abstain policy\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom pathlib import Path\nfrom PIL import Image\nimport pydicom\n\n# Preconditions\nneed = [\"DEVICE\",\"OUT\",\"model\",\"feat_extractor\",\"eval_tfms\",\"df_chal\",\"val_loader\",\"T\",\"tau_conf\"]\nmissing = [k for k in need if k not in globals()]\nassert not missing, f\"Missing prereqs: {missing}. Run 12B first (and calibration).\"\n\n# --- robust image loader ---\ndef load_any_image(path: str) -> Image.Image:\n    p = Path(path)\n    suf = p.suffix.lower()\n    if suf == \".dcm\":\n        dcm = pydicom.dcmread(str(p), force=True)\n        arr = dcm.pixel_array.astype(np.float32)\n        mn, mx = float(arr.min()), float(arr.max())\n        if mx > mn:\n            arr = (arr - mn) / (mx - mn)\n        else:\n            arr = np.zeros_like(arr)\n        arr = (arr * 255.0).clip(0,255).astype(np.uint8)\n        return Image.fromarray(arr).convert(\"RGB\")\n    return Image.open(str(p)).convert(\"RGB\")\n\nclass PathDataset(Dataset):\n    def __init__(self, paths, tfms):\n        self.paths = list(paths)\n        self.tfms = tfms\n    def __len__(self):\n        return len(self.paths)\n    def __getitem__(self, idx):\n        img = load_any_image(self.paths[idx])\n        x = self.tfms(img)\n        return x\n\n@torch.no_grad()\ndef embed_loader(loader):\n    feat_extractor.eval()\n    Z = []\n    for xb in loader:\n        if isinstance(xb, (tuple, list)):\n            xb = xb[0]\n        xb = xb.to(DEVICE, non_blocking=True)\n        z = feat_extractor(xb).view(xb.shape[0], -1).detach().cpu()\n        Z.append(z)\n    return torch.cat(Z, dim=0).numpy()  # (N,D)\n\n# -------------------------\n# 1) Get in-domain embeddings (VAL) and OOD embeddings (challenge)\n#    Keep it tight + reproducible (VAL only) to start\n# -------------------------\n@torch.no_grad()\ndef embed_from_val_loader(val_loader):\n    Z = []\n    for xb, _ in val_loader:\n        xb = xb.to(DEVICE, non_blocking=True)\n        z = feat_extractor(xb).view(xb.shape[0], -1).detach().cpu()\n        Z.append(z)\n    return torch.cat(Z, dim=0).numpy()\n\nZ_in = embed_from_val_loader(val_loader)   # in-domain\npaths_ood = df_chal[\"path\"].tolist()\nood_ds = PathDataset(paths_ood, eval_tfms)\nood_loader = DataLoader(ood_ds, batch_size=64, shuffle=False, num_workers=0, pin_memory=True)\nZ_ood = embed_loader(ood_loader)\n\ny_in = np.ones((Z_in.shape[0],), dtype=np.int64)\ny_ood = np.zeros((Z_ood.shape[0],), dtype=np.int64)\n\nX = np.vstack([Z_in, Z_ood])\ny = np.concatenate([y_in, y_ood])\n\nprint(\"Embeddings shapes:\")\nprint(\"  in-domain (val):\", Z_in.shape)\nprint(\"  ood (challenge):\", Z_ood.shape)\nprint(\"  X:\", X.shape, \"y:\", y.shape)\n\n# -------------------------\n# 2) Train a simple domain classifier on embeddings\n# -------------------------\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\n\nclf = LogisticRegression(max_iter=2000, class_weight=\"balanced\", n_jobs=-1)\nclf.fit(X, y)\n\np_in_train = clf.predict_proba(X)[:, 1]\nauc = roc_auc_score(y, p_in_train)\nprint(\"✅ Domain guard AUC (val vs challenge):\", auc)\n\n# -------------------------\n# 3) Choose tau_domain to keep 99% of in-domain val (low false abstain)\n# -------------------------\nKEEP_IN_DOMAIN = 0.99\np_in_val = clf.predict_proba(Z_in)[:, 1]\ntau_domain = float(np.quantile(p_in_val, 1.0 - KEEP_IN_DOMAIN))\n\nprint(\"✅ tau_domain chosen from VAL\")\nprint(\"KEEP_IN_DOMAIN:\", KEEP_IN_DOMAIN)\nprint(\"tau_domain:\", tau_domain)\n\n# -------------------------\n# 4) Apply domain guard to challenge + compare to your v1 results\n# -------------------------\n# We reuse df_out if it exists (from 12B/12C). Otherwise, we rebuild a minimal df_out using your existing challenge predictions.\nassert \"df_out\" in globals(), \"df_out not found. Run 12B first (it creates df_out).\"\n\n# Compute p_in_domain for each challenge row using embeddings we already computed (Z_ood aligns with df_chal order)\n# df_out uses df_chal row order; we rebuild embeddings aligned to df_out rows:\n# safest: re-embed df_out paths (small enough, 4275)\npaths_out = df_out[\"path\"].tolist()\nout_ds = PathDataset(paths_out, eval_tfms)\nout_loader = DataLoader(out_ds, batch_size=64, shuffle=False, num_workers=0, pin_memory=True)\nZ_out = embed_loader(out_loader)\n\np_in_out = clf.predict_proba(Z_out)[:, 1]\ndf_out[\"p_in_domain\"] = p_in_out\n\ndf_out[\"abstain_domain\"] = df_out[\"p_in_domain\"] < tau_domain\n\ntumor_set = {\"glioma\",\"meningioma\",\"pituitary\"}\ndf_out[\"final_action_v3\"] = np.where(\n    df_out[\"abstain_domain\"] | df_out[\"abstain_lowconf\"],\n    \"ABSTAIN\",\n    df_out[\"pred_name\"]\n)\n\npre_tumor = int(df_out[\"pred_name\"].isin(tumor_set).sum())\npost_tumor_v1 = int(df_out[\"final_action\"].isin(tumor_set).sum())\npost_tumor_v3 = int(df_out[\"final_action_v3\"].isin(tumor_set).sum())\nabstain_v1 = int((df_out[\"final_action\"] == \"ABSTAIN\").sum())\nabstain_v3 = int((df_out[\"final_action_v3\"] == \"ABSTAIN\").sum())\n\nprint(\"\\n✅ Policy comparison (challenge):\")\nprint(\"Tumor preds BEFORE:\", pre_tumor)\nprint(\"Tumor preds AFTER v1:\", post_tumor_v1)\nprint(\"Tumor preds AFTER v3 (domain guard):\", post_tumor_v3)\nprint(\"ABSTAINS v1:\", abstain_v1)\nprint(\"ABSTAINS v3:\", abstain_v3)\n\n# Save artifacts\ndf_out.to_csv(OUT / \"challenge_policy_outputs_v3.csv\", index=False)\npd.DataFrame([{\n    \"tau_conf\": float(tau_conf),\n    \"keep_in_domain\": float(KEEP_IN_DOMAIN),\n    \"tau_domain\": float(tau_domain),\n    \"auc_val_vs_challenge\": float(auc),\n    \"tumor_before\": pre_tumor,\n    \"tumor_after_v1\": post_tumor_v1,\n    \"tumor_after_v3\": post_tumor_v3,\n    \"abstain_v1\": abstain_v1,\n    \"abstain_v3\": abstain_v3,\n}]).to_csv(OUT / \"policy_summary_v3.csv\", index=False)\n\nprint(\"saved ->\", OUT / \"challenge_policy_outputs_v3.csv\")\nprint(\"saved ->\", OUT / \"policy_summary_v3.csv\")\n\n# Show top remaining tumor preds after v3\ntry:\n    from IPython.display import display\n    remain = df_out[df_out[\"final_action_v3\"].isin(tumor_set)].sort_values(\"max_prob_cal\", ascending=False)\n    print(\"\\nTop 10 remaining tumor preds AFTER v3:\")\n    display(remain[[\"domain\",\"pred_name\",\"max_prob_cal\",\"p_in_domain\",\"abstain_domain\",\"final_action_v3\",\"path\"]].head(10))\nexcept Exception as e:\n    print(\"Display skipped:\", e)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T16:11:12.928384Z","iopub.execute_input":"2026-02-14T16:11:12.928715Z","iopub.status.idle":"2026-02-14T16:20:59.557696Z","shell.execute_reply.started":"2026-02-14T16:11:12.928684Z","shell.execute_reply":"2026-02-14T16:20:59.556744Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n  warnings.warn(warn_msg)\n","output_type":"stream"},{"name":"stdout","text":"Embeddings shapes:\n  in-domain (val): (1850, 512)\n  ood (challenge): (4275, 512)\n  X: (6125, 512) y: (6125,)\n✅ Domain guard AUC (val vs challenge): 1.0\n✅ tau_domain chosen from VAL\nKEEP_IN_DOMAIN: 0.99\ntau_domain: 0.9011013984593388\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n  warnings.warn(warn_msg)\n","output_type":"stream"},{"name":"stdout","text":"\n✅ Policy comparison (challenge):\nTumor preds BEFORE: 563\nTumor preds AFTER v1: 62\nTumor preds AFTER v3 (domain guard): 0\nABSTAINS v1: 1282\nABSTAINS v3: 4275\nsaved -> /kaggle/working/train_artifacts/challenge_policy_outputs_v3.csv\nsaved -> /kaggle/working/train_artifacts/policy_summary_v3.csv\n\nTop 10 remaining tumor preds AFTER v3:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Empty DataFrame\nColumns: [domain, pred_name, max_prob_cal, p_in_domain, abstain_domain, final_action_v3, path]\nIndex: []","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>domain</th>\n      <th>pred_name</th>\n      <th>max_prob_cal</th>\n      <th>p_in_domain</th>\n      <th>abstain_domain</th>\n      <th>final_action_v3</th>\n      <th>path</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"import numpy as np\nfrom pathlib import Path\nfrom PIL import Image\nimport pydicom\nfrom torch.utils.data import Dataset\n\ndef load_any_image(path: str) -> Image.Image:\n    p = Path(path)\n    suf = p.suffix.lower()\n    if suf == \".dcm\":\n        dcm = pydicom.dcmread(str(p), force=True)\n        arr = dcm.pixel_array.astype(np.float32)\n        mn, mx = float(arr.min()), float(arr.max())\n        if mx > mn:\n            arr = (arr - mn) / (mx - mn)\n        else:\n            arr = np.zeros_like(arr)\n        arr = (arr * 255.0).clip(0,255).astype(np.uint8)\n        return Image.fromarray(arr).convert(\"RGB\")\n    return Image.open(str(p)).convert(\"RGB\")\n\nclass PathDataset(Dataset):\n    def __init__(self, paths, tfms):\n        self.paths = list(paths)\n        self.tfms = tfms\n    def __len__(self):\n        return len(self.paths)\n    def __getitem__(self, idx):\n        img = load_any_image(self.paths[idx])\n        return self.tfms(img)\n\nprint(\"✅ PathDataset + load_any_image ready\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL: 12E_DOMAIN_GUARD_HELDOUT_AND_SWEEP - held-out AUC + tau_domain sweep (val/test/challenge)\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\n\n# Preconditions\nneed = [\"DEVICE\",\"feat_extractor\",\"eval_tfms\",\"val_loader\",\"test_loader\",\"df_chal\"]\nmissing = [k for k in need if k not in globals()]\nassert not missing, f\"Missing prereqs: {missing}. Run 12D prerequisites first.\"\n\nfeat_extractor.eval()\n\n@torch.no_grad()\ndef embed_from_labeled_loader(loader):\n    Z = []\n    for xb, _ in loader:\n        xb = xb.to(DEVICE, non_blocking=True)\n        z = feat_extractor(xb).view(xb.shape[0], -1).detach().cpu()\n        Z.append(z)\n    return torch.cat(Z, dim=0).numpy()\n\n# reuse the PathDataset + load_any_image from 12D if present\nassert \"PathDataset\" in globals() and \"load_any_image\" in globals(), \"Run 12D first (expects PathDataset/load_any_image).\"\nfrom torch.utils.data import DataLoader\n\n@torch.no_grad()\ndef embed_from_paths(paths, batch_size=64):\n    ds = PathDataset(paths, eval_tfms)\n    dl = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n    Z = []\n    for xb in dl:\n        xb = xb.to(DEVICE, non_blocking=True)\n        z = feat_extractor(xb).view(xb.shape[0], -1).detach().cpu()\n        Z.append(z)\n    return torch.cat(Z, dim=0).numpy()\n\n# 1) Embeddings\nZ_val = embed_from_labeled_loader(val_loader)   # in-domain\nZ_test = embed_from_labeled_loader(test_loader) # in-domain (holdout)\nZ_chal = embed_from_paths(df_chal[\"path\"].tolist())\n\ny_val = np.ones((Z_val.shape[0],), dtype=np.int64)\ny_chal = np.zeros((Z_chal.shape[0],), dtype=np.int64)\n\n# 2) Held-out evaluation (split within val+challenge)\nX = np.vstack([Z_val, Z_chal])\ny = np.concatenate([y_val, y_chal])\n\nX_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n\nclf = LogisticRegression(max_iter=2000, class_weight=\"balanced\", n_jobs=-1)\nclf.fit(X_tr, y_tr)\n\np_in_te = clf.predict_proba(X_te)[:, 1]\nauc_heldout = roc_auc_score(y_te, p_in_te)\n\nprint(\"✅ Held-out domain AUC (val vs challenge):\", auc_heldout)\n\n# 3) Sweep tau_domain based on \"keep rate\" on VAL, report abstain rates on TEST + CHALLENGE\np_in_val = clf.predict_proba(Z_val)[:, 1]\np_in_test = clf.predict_proba(Z_test)[:, 1]\np_in_chal = clf.predict_proba(Z_chal)[:, 1]\n\nkeep_rates = [0.999, 0.995, 0.99, 0.98, 0.95, 0.90]\n\nrows = []\nfor keep in keep_rates:\n    tau = float(np.quantile(p_in_val, 1.0 - keep))\n    abst_val = float((p_in_val < tau).mean())\n    abst_test = float((p_in_test < tau).mean())\n    abst_chal = float((p_in_chal < tau).mean())\n    rows.append({\n        \"keep_in_val_target\": keep,\n        \"tau_domain\": tau,\n        \"abstain_val\": abst_val,\n        \"abstain_test\": abst_test,\n        \"abstain_challenge\": abst_chal,\n    })\n\ndf_sweep = pd.DataFrame(rows).sort_values(\"keep_in_val_target\", ascending=False)\nprint(\"\\n✅ tau_domain sweep:\")\ndisplay(df_sweep)\n\ndf_sweep.to_csv(OUT / \"domain_guard_tau_sweep.csv\", index=False)\nprint(\"saved ->\", OUT / \"domain_guard_tau_sweep.csv\")\n\n# --- export domain guard weights for MVP (no sklearn dependency) ---\nimport numpy as np\nnp.savez(\n    OUT / \"domain_guard_lr.npz\",\n    coef=clf.coef_.astype(np.float32),\n    intercept=clf.intercept_.astype(np.float32),\n)\nprint(\"saved ->\", OUT / \"domain_guard_lr.npz\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T16:24:08.347108Z","iopub.execute_input":"2026-02-14T16:24:08.347470Z","iopub.status.idle":"2026-02-14T16:31:36.100266Z","shell.execute_reply.started":"2026-02-14T16:24:08.347441Z","shell.execute_reply":"2026-02-14T16:31:36.098787Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n  warnings.warn(warn_msg)\n","output_type":"stream"},{"name":"stdout","text":"✅ Held-out domain AUC (val vs challenge): 0.9999157380295338\n\n✅ tau_domain sweep:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   keep_in_val_target  tau_domain  abstain_val  abstain_test  \\\n0               0.999    0.428545     0.001081      0.007786   \n1               0.995    0.800385     0.004865      0.023844   \n2               0.990    0.848166     0.010270      0.026764   \n3               0.980    0.913868     0.020000      0.039416   \n4               0.950    0.964167     0.050270      0.061800   \n5               0.900    0.988675     0.100000      0.112895   \n\n   abstain_challenge  \n0           0.998363  \n1           0.999766  \n2           0.999766  \n3           1.000000  \n4           1.000000  \n5           1.000000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>keep_in_val_target</th>\n      <th>tau_domain</th>\n      <th>abstain_val</th>\n      <th>abstain_test</th>\n      <th>abstain_challenge</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.999</td>\n      <td>0.428545</td>\n      <td>0.001081</td>\n      <td>0.007786</td>\n      <td>0.998363</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.995</td>\n      <td>0.800385</td>\n      <td>0.004865</td>\n      <td>0.023844</td>\n      <td>0.999766</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.990</td>\n      <td>0.848166</td>\n      <td>0.010270</td>\n      <td>0.026764</td>\n      <td>0.999766</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.980</td>\n      <td>0.913868</td>\n      <td>0.020000</td>\n      <td>0.039416</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.950</td>\n      <td>0.964167</td>\n      <td>0.050270</td>\n      <td>0.061800</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.900</td>\n      <td>0.988675</td>\n      <td>0.100000</td>\n      <td>0.112895</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"saved -> /kaggle/working/train_artifacts/domain_guard_tau_sweep.csv\nsaved -> /kaggle/working/train_artifacts/domain_guard_lr.npz\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"from pathlib import Path\np = Path(\"/kaggle/working/train_artifacts/domain_guard_lr.npz\")\nprint(\"domain_guard_lr.npz exists:\", p.exists(), \"size_mb:\", (p.stat().st_size/1e6 if p.exists() else None))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T16:32:43.165839Z","iopub.execute_input":"2026-02-14T16:32:43.166194Z","iopub.status.idle":"2026-02-14T16:32:43.172529Z","shell.execute_reply.started":"2026-02-14T16:32:43.166161Z","shell.execute_reply":"2026-02-14T16:32:43.171667Z"}},"outputs":[{"name":"stdout","text":"domain_guard_lr.npz exists: True size_mb: 0.002564\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# CELL: 12F_FINAL_POLICY_REPORT - Lock tau_domain + report final demo metrics (test + challenge)\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn.functional as F\nimport json\n\n# ---- set your chosen tau_domain from the sweep ----\nTAU_DOMAIN = 0.580288  # keep_in_val_target=0.999 row\n\n# prereqs\nneed = [\"OUT\",\"NUM_CLASSES\",\"T\",\"tau_conf\",\"test_logits\",\"test_y\",\"df_out\"]\nmissing = [k for k in need if k not in globals()]\nassert not missing, f\"Missing prereqs: {missing}. Run calibration + 12B + domain guard cells first.\"\n\n# Helper: macro-F1 on subset\ndef confusion_matrix(preds, y_true, k):\n    cm = torch.zeros((k, k), dtype=torch.int64)\n    for t, p in zip(y_true.tolist(), preds.tolist()):\n        cm[t, p] += 1\n    return cm\n\ndef macro_f1_from_cm(cm):\n    k = cm.shape[0]\n    f1s = []\n    for c in range(k):\n        tp = cm[c, c].item()\n        fp = cm[:, c].sum().item() - tp\n        fn = cm[c, :].sum().item() - tp\n        denom = (2*tp + fp + fn)\n        f1 = 0.0 if denom == 0 else (2*tp / denom)\n        f1s.append(f1)\n    return float(sum(f1s) / k)\n\n# -----------------------\n# 1) IN-DOMAIN (TEST): apply final policy\n# -----------------------\ntest_probs_cal = torch.softmax(test_logits / T, dim=1)\ntest_maxprob = test_probs_cal.max(dim=1).values\ntest_pred = test_probs_cal.argmax(dim=1)\n\n# Domain scores on TEST were computed in the sweep cell as p_in_test;\n# if not in globals, we recompute quickly using clf + Z_test if present.\nassert \"p_in_test\" in globals() or (\"clf\" in globals() and \"Z_test\" in globals()), \\\n    \"Need p_in_test from sweep cell. Re-run the sweep cell first.\"\n\nif \"p_in_test\" not in globals():\n    p_in_test = clf.predict_proba(Z_test)[:, 1]\n\np_in_test = np.asarray(p_in_test)\nassert p_in_test.shape[0] == test_y.shape[0], (p_in_test.shape, test_y.shape)\n\nabstain_domain_test = p_in_test < TAU_DOMAIN\nabstain_lowconf_test = (test_maxprob.numpy() < float(tau_conf))\n\nabstain_test = abstain_domain_test | abstain_lowconf_test\ncoverage_test = float((~abstain_test).mean())\nabstain_rate_test = float(abstain_test.mean())\n\n# Metrics on accepted only\naccepted_idx = np.where(~abstain_test)[0]\ntest_acc_all = float((test_pred == test_y).to(torch.float32).mean().item())\n\nif accepted_idx.size > 0:\n    y_acc = test_y[accepted_idx]\n    p_acc = test_pred[accepted_idx]\n    acc_acc = float((p_acc == y_acc).to(torch.float32).mean().item())\n    cm = confusion_matrix(p_acc, y_acc, NUM_CLASSES)\n    f1_acc = macro_f1_from_cm(cm)\nelse:\n    acc_acc, f1_acc = None, None\n\nprint(\"✅ FINAL POLICY (TEST)\")\nprint(\"tau_conf:\", float(tau_conf))\nprint(\"tau_domain:\", TAU_DOMAIN)\nprint(\"test_acc_all (no abstain):\", test_acc_all)\nprint(\"test_abstain_rate:\", abstain_rate_test)\nprint(\"test_coverage:\", coverage_test)\nprint(\"test_acc_on_accepted:\", acc_acc)\nprint(\"test_macro_f1_on_accepted:\", f1_acc)\n\n# -----------------------\n# 2) OOD (CHALLENGE): apply final policy using p_in_domain from domain guard cell\n# -----------------------\nassert \"p_in_domain\" in df_out.columns, \"df_out missing p_in_domain. Run domain guard cell (12D) first.\"\n\ndf_final = df_out.copy()\ndf_final[\"abstain_domain\"] = df_final[\"p_in_domain\"] < TAU_DOMAIN\ndf_final[\"final_action_demo\"] = np.where(\n    df_final[\"abstain_domain\"] | df_final[\"abstain_lowconf\"],\n    \"ABSTAIN\",\n    df_final[\"pred_name\"]\n)\n\ntumor_set = {\"glioma\",\"meningioma\",\"pituitary\"}\ntumor_before = int(df_final[\"pred_name\"].isin(tumor_set).sum())\ntumor_after = int(df_final[\"final_action_demo\"].isin(tumor_set).sum())\nabstain_chal = int((df_final[\"final_action_demo\"] == \"ABSTAIN\").sum())\ncoverage_chal = 1.0 - abstain_chal / len(df_final)\n\nprint(\"\\n✅ FINAL POLICY (CHALLENGE)\")\nprint(\"challenge_rows:\", len(df_final))\nprint(\"tumor_preds_before:\", tumor_before)\nprint(\"tumor_preds_after:\", tumor_after)\nprint(\"abstains:\", abstain_chal)\nprint(\"coverage:\", coverage_chal)\n\n# Save artifacts + config\ndf_final.to_csv(OUT / \"challenge_policy_outputs_demo.csv\", index=False)\n\ncfg = {\n    \"temperature_T\": float(T),\n    \"tau_conf\": float(tau_conf),\n    \"tau_domain\": float(TAU_DOMAIN),\n    \"notes\": \"Final demo policy: ABSTAIN if p_in_domain < tau_domain OR max_prob_cal < tau_conf.\"\n}\nwith open(OUT / \"final_policy_config.json\", \"w\") as f:\n    json.dump(cfg, f, indent=2)\n\npd.DataFrame([{\n    \"test_acc_all\": test_acc_all,\n    \"test_abstain_rate\": abstain_rate_test,\n    \"test_coverage\": coverage_test,\n    \"test_acc_on_accepted\": acc_acc,\n    \"test_macro_f1_on_accepted\": f1_acc,\n    \"challenge_rows\": len(df_final),\n    \"challenge_tumor_before\": tumor_before,\n    \"challenge_tumor_after\": tumor_after,\n    \"challenge_abstains\": abstain_chal,\n    \"challenge_coverage\": coverage_chal,\n    \"tau_conf\": float(tau_conf),\n    \"tau_domain\": float(TAU_DOMAIN),\n    \"T\": float(T),\n}]).to_csv(OUT / \"final_policy_report.csv\", index=False)\n\nprint(\"\\nsaved ->\", OUT / \"challenge_policy_outputs_demo.csv\")\nprint(\"saved ->\", OUT / \"final_policy_config.json\")\nprint(\"saved ->\", OUT / \"final_policy_report.csv\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL: 13_CHALLENGE_TOP_TUMOR_FALSE_POSITIVES - inspect highest-confidence tumor preds on non-tumor domains\n\nimport pandas as pd\nfrom pathlib import Path\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nOUT = Path(\"/kaggle/working/train_artifacts\")\npred_path = OUT / \"challenge_predictions.csv\"\nassert pred_path.exists(), f\"Missing: {pred_path} (run the challenge audit cell first)\"\n\ndfp = pd.read_csv(pred_path)\n\n# Tumor classes only (everything except notumor)\ntumor = dfp[dfp[\"pred_name\"].isin([\"glioma\", \"meningioma\", \"pituitary\"])].copy()\ntumor = tumor.sort_values(\"max_prob\", ascending=False)\n\nprint(\"tumor-pred rows:\", len(tumor))\ndisplay(tumor[[\"domain\",\"pred_name\",\"max_prob\",\"path\"]].head(20))\n\n# Visual grid (top N)\nN = 24\nshow = tumor.head(N).reset_index(drop=True)\n\ncols = 6\nrows = (N + cols - 1) // cols\nplt.figure(figsize=(3*cols, 3*rows))\n\nfor i in range(len(show)):\n    p = Path(show.loc[i, \"path\"])\n    title = f'{show.loc[i,\"domain\"]}\\n{show.loc[i,\"pred_name\"]} {show.loc[i,\"max_prob\"]:.2f}'\n    try:\n        img = Image.open(p).convert(\"RGB\")\n    except Exception:\n        # If any DICOM slipped in, skip rendering here (your audit loader handles DICOM).\n        continue\n    ax = plt.subplot(rows, cols, i+1)\n    ax.imshow(img)\n    ax.set_title(title, fontsize=9)\n    ax.axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL98: SAVE_AND_ZIP_ARTIFACTS - Verify artifacts + bundle into one zip for download\n\nfrom pathlib import Path\nimport os, zipfile, json, time\n\nOUT = Path(OUT)  # ensure Path\n\nmust_have = [\n    \"best_model.pth\",\n    \"train_history.json\",\n    \"env_snapshot.json\",\n    \"temperature_scaling.json\",\n    \"calibration_summary_before_after.csv\",\n    \"calibration_metrics.json\",\n    \"final_policy_config.json\",\n    \"domain_guard_lr.npz\",\n    \"final_policy_report.csv\",\n    \"muaz_policy_report.csv\",\n    \"muaz_policy_outputs.csv\",\n    \"challenge_policy_outputs_demo.csv\",\n]\n\nprint(\"OUT =\", OUT)\nprint(\"\\nChecking required files...\\n\")\n\nmissing = []\npresent = []\nfor fn in must_have:\n    p = OUT / fn\n    if p.exists():\n        present.append((fn, p.stat().st_size))\n    else:\n        missing.append(fn)\n\nfor fn, sz in sorted(present, key=lambda x: -x[1]):\n    print(f\"✅ {fn:35s} {sz/1e6:8.2f} MB\")\n\nif missing:\n    print(\"\\n❌ Missing:\")\n    for fn in missing:\n        print(\" -\", fn)\nelse:\n    print(\"\\n✅ All required files present.\")\n\n# Always include \"everything in OUT\" in the zip (even if some names differ)\nzip_path = OUT.parent / \"mri_demo_artifacts_bundle.zip\"\nif zip_path.exists():\n    zip_path.unlink()\n\nprint(\"\\nZipping all files under:\", OUT)\nwith zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n    for p in OUT.rglob(\"*\"):\n        if p.is_file():\n            z.write(p, arcname=str(p.relative_to(OUT.parent)))\n\nprint(\"\\n✅ Zip created:\", zip_path, f\"size={zip_path.stat().st_size/1e6:.2f} MB\")\n\n# Small manifest for human sanity\nmanifest = {\n    \"created_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n    \"out_dir\": str(OUT),\n    \"zip_path\": str(zip_path),\n    \"files\": [{\"name\": f, \"bytes\": int(sz)} for f, sz in present],\n    \"missing\": missing,\n}\nmanifest_path = OUT / \"BUNDLE_MANIFEST.json\"\nwith open(manifest_path, \"w\") as f:\n    json.dump(manifest, f, indent=2)\n\nprint(\"✅ Wrote:\", manifest_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T16:35:34.839641Z","iopub.execute_input":"2026-02-14T16:35:34.839995Z","iopub.status.idle":"2026-02-14T16:35:41.683679Z","shell.execute_reply.started":"2026-02-14T16:35:34.839966Z","shell.execute_reply":"2026-02-14T16:35:41.682660Z"}},"outputs":[{"name":"stdout","text":"OUT = /kaggle/working/train_artifacts\n\nChecking required files...\n\n✅ best_model.pth                         44.79 MB\n✅ challenge_policy_outputs_demo.csv       1.13 MB\n✅ muaz_policy_outputs.csv                 0.07 MB\n✅ domain_guard_lr.npz                     0.00 MB\n✅ train_history.json                      0.00 MB\n✅ calibration_metrics.json                0.00 MB\n✅ calibration_summary_before_after.csv     0.00 MB\n✅ muaz_policy_report.csv                  0.00 MB\n✅ final_policy_report.csv                 0.00 MB\n✅ final_policy_config.json                0.00 MB\n✅ env_snapshot.json                       0.00 MB\n✅ temperature_scaling.json                0.00 MB\n\n✅ All required files present.\n\nZipping all files under: /kaggle/working/train_artifacts\n\n✅ Zip created: /kaggle/working/mri_demo_artifacts_bundle.zip size=84.05 MB\n✅ Wrote: /kaggle/working/train_artifacts/BUNDLE_MANIFEST.json\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"## Appendix: Debug helpers (optional)\n\nThese cells are optional and safe. They help confirm inputs and GPU availability.\n","metadata":{"execution":{"iopub.status.busy":"2026-02-12T19:03:56.403644Z","iopub.status.idle":"2026-02-12T19:03:56.404035Z","shell.execute_reply.started":"2026-02-12T19:03:56.403876Z","shell.execute_reply":"2026-02-12T19:03:56.403901Z"}}},{"cell_type":"code","source":"# CELL: 99_DEBUG_INPUTS - List mounted inputs (optional)\n\n!ls -1 /kaggle/input | sed -n '1,200p'\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL: 99_DEBUG_GPU - Check GPU (optional)\n\n!nvidia-smi || true\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%bash\nls -lah /kaggle/working | sed -n '1,120p'\nls -lah /kaggle/working/train_artifacts 2>/dev/null | sed -n '1,200p' || true\n\necho \"---- searching for best_model.pth ----\"\nfind /kaggle -maxdepth 6 -name \"best_model.pth\" -print 2>/dev/null | sed -n '1,50p'\n\necho \"---- searching for any .pth ----\"\nfind /kaggle -maxdepth 6 -name \"*.pth\" -print 2>/dev/null | sed -n '1,50p'\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL: 13_PERSIST_PACKAGE - bundle model + metadata for publishing (safe to run multiple times)\n\nfrom pathlib import Path\nimport json, hashlib, shutil, time\n\nSRC = Path(\"/kaggle/working/train_artifacts\")\nassert SRC.exists(), f\"Missing {SRC}. Did you run training in this session?\"\n\n# Required checkpoint\nCKPT = SRC / \"best_model.pth\"\nassert CKPT.exists(), f\"Missing checkpoint: {CKPT}\\nTip: check SRC contents with: !ls -lah /kaggle/working/train_artifacts\"\n\n# Create a clean publish folder\nPUBLISH = Path(\"/kaggle/working/publish_mri_model_v1\")\nif PUBLISH.exists():\n    shutil.rmtree(PUBLISH)\nPUBLISH.mkdir(parents=True, exist_ok=True)\n\n# Copy everything in train_artifacts (keeps your eval CSVs too)\nfor p in SRC.iterdir():\n    if p.is_file():\n        shutil.copy2(p, PUBLISH / p.name)\n\n# Compute sha256 of checkpoint (so you can verify integrity later)\nh = hashlib.sha256()\nwith open(CKPT, \"rb\") as f:\n    for chunk in iter(lambda: f.read(1024 * 1024), b\"\"):\n        h.update(chunk)\nckpt_sha256 = h.hexdigest()\n\n# Write lightweight metadata\nmeta = {\n    \"created_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n    \"checkpoint_file\": CKPT.name,\n    \"checkpoint_sha256\": ckpt_sha256,\n    \"artifacts_included\": sorted([p.name for p in PUBLISH.iterdir() if p.is_file()]),\n    \"notes\": \"Baseline ResNet18 classifier. See train_history.json + challenge_* CSVs for metrics/audits.\",\n}\n(PUBLISH / \"MODEL_PACKAGE_METADATA.json\").write_text(json.dumps(meta, indent=2))\n\n# Optional: create a zip for easy download (dataset can store folder OR zip)\nzip_path = shutil.make_archive(str(PUBLISH), \"zip\", root_dir=str(PUBLISH))\n\nprint(\"✅ publish folder:\", PUBLISH)\nprint(\"✅ zip:\", zip_path)\nprint(\"✅ checkpoint sha256:\", ckpt_sha256)\nprint(\"Files in publish folder:\")\nprint(\"\\n\".join([\" - \" + p.name for p in sorted(PUBLISH.iterdir()) if p.is_file()]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T16:58:36.988422Z","iopub.execute_input":"2026-02-14T16:58:36.988800Z","iopub.status.idle":"2026-02-14T16:58:44.037526Z","shell.execute_reply.started":"2026-02-14T16:58:36.988769Z","shell.execute_reply":"2026-02-14T16:58:44.036304Z"}},"outputs":[{"name":"stdout","text":"✅ publish folder: /kaggle/working/publish_mri_model_v1\n✅ zip: /kaggle/working/publish_mri_model_v1.zip\n✅ checkpoint sha256: caa46dbe12301cb25a75225d82496952b14841f2166db75041a9eccc7b70c861\nFiles in publish folder:\n - BUNDLE_MANIFEST.json\n - MODEL_PACKAGE_METADATA.json\n - best_model.pth\n - calib_test_logits.pt\n - calib_val_logits.pt\n - calibration_metrics.json\n - calibration_summary_before.csv\n - calibration_summary_before_after.csv\n - challenge_conf_by_pred.csv\n - challenge_confidence_summary.csv\n - challenge_policy_outputs.csv\n - challenge_policy_outputs_demo.csv\n - challenge_policy_outputs_v2.csv\n - challenge_policy_outputs_v3.csv\n - challenge_pred_distribution.csv\n - challenge_predictions.csv\n - domain_guard_lr.npz\n - domain_guard_tau_sweep.csv\n - env_snapshot.json\n - final_policy_config.json\n - final_policy_report.csv\n - mri_resnet18_baseline_best.pth\n - muaz_policy_outputs.csv\n - muaz_policy_report.csv\n - policy_summary.csv\n - policy_summary_v2.csv\n - policy_summary_v3.csv\n - reliability_bins_test_after.csv\n - reliability_bins_test_before.csv\n - reliability_bins_val_after.csv\n - reliability_bins_val_before.csv\n - reliability_test_after.png\n - reliability_test_before.png\n - reliability_val_after.png\n - reliability_val_before.png\n - temperature_scaling.json\n - train_history.json\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# CHECKPOINT INVENTORY (run this now)\n!echo \"=== /kaggle/working ===\"\n!ls -lah /kaggle/working | sed -n '1,200p' || true\n\n!echo \"\\n=== /kaggle/working/train_artifacts ===\"\n!ls -lah /kaggle/working/train_artifacts | sed -n '1,200p' || true\n\n!echo \"\\n=== find checkpoints (*.pth, *.pt) ===\"\n!find /kaggle/working -maxdepth 4 -type f \\( -name \"*.pth\" -o -name \"*.pt\" \\) -print 2>/dev/null | sed -n '1,200p' || true\n\n!echo \"\\n=== sizes ===\"\n!du -sh /kaggle/working/train_artifacts 2>/dev/null || true\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pathlib import Path\nimport shutil\n\nsrc = Path(\"/kaggle/working/train_artifacts/best_model.pth\")\ndst = Path(\"/kaggle/working/train_artifacts/mri_resnet18_baseline_best.pth\")\n\nassert src.exists(), f\"Missing: {src}\"\nshutil.copy2(src, dst)\nprint(\"✅ copied to:\", dst)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# VERIFY: What datasets/domains are in each split (by parsing the path prefix)\n\nimport pandas as pd\nimport re\n\ndef dataset_key_from_path(p: str) -> str:\n    # common Kaggle pattern you’ve seen: /kaggle/input/datasets/<owner>/<slug>/...\n    m = re.search(r\"/kaggle/input/datasets/([^/]+/[^/]+)/\", str(p))\n    if m:\n        return m.group(1)  # owner/slug\n    # fallback: /kaggle/input/<slug>/...\n    m2 = re.search(r\"/kaggle/input/([^/]+)/\", str(p))\n    return m2.group(1) if m2 else \"unknown\"\n\ndef summarize_split(df, name):\n    df = df.copy()\n    df[\"dataset_key\"] = df[\"path\"].map(dataset_key_from_path)\n    print(f\"\\n=== {name} ===\")\n    print(\"rows:\", len(df))\n    if \"label_name\" in df.columns:\n        print(\"\\nlabel distribution:\")\n        display(df[\"label_name\"].value_counts(dropna=False))\n    print(\"\\ndataset_key distribution:\")\n    display(df[\"dataset_key\"].value_counts(dropna=False).head(20))\n    print(\"\\nexample paths:\")\n    display(df[[\"dataset_key\",\"path\"]].head(5))\n\n# If df_train/df_val/df_test already exist, use them; otherwise read from split paths\nif \"df_train\" in globals() and \"df_val\" in globals() and \"df_test\" in globals():\n    summarize_split(df_train, \"TRAIN\")\n    summarize_split(df_val,   \"VAL\")\n    summarize_split(df_test,  \"TEST\")\nelse:\n    # uses your earlier split_train/split_val/split_test paths\n    df_train2 = pd.read_csv(split_train)\n    df_val2   = pd.read_csv(split_val)\n    df_test2  = pd.read_csv(split_test)\n    summarize_split(df_train2, \"TRAIN\")\n    summarize_split(df_val2,   \"VAL\")\n    summarize_split(df_test2,  \"TEST\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# DIAG: list all DataLoaders currently defined (name + dataset size + batch size)\n\nimport torch\n\nloaders = []\nfor name, obj in list(globals().items()):\n    if isinstance(obj, torch.utils.data.DataLoader):\n        ds_len = None\n        try:\n            ds_len = len(obj.dataset)\n        except Exception:\n            pass\n        loaders.append((name, ds_len, getattr(obj, \"batch_size\", None)))\n\nloaders = sorted(loaders, key=lambda x: (x[1] is None, x[1] if x[1] is not None else 10**9))\nprint(\"Found DataLoaders:\")\nfor name, ds_len, bs in loaders:\n    print(f\" - {name:25s} dataset_len={ds_len} batch_size={bs}\")\n\n# Heuristic: pick external MUAZ loader by size ~1311\ncands = [x for x in loaders if x[1] == 1311]\nprint(\"\\nCandidates with dataset_len == 1311:\", cands)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nckpt = torch.load(OUT/\"best_model.pth\", map_location=DEVICE)\nmodel.load_state_dict(ckpt[\"model_state_dict\"])\nmodel.eval()\n\n# penultimate layer features for embeddings\nfeat_extractor = nn.Sequential(*list(model.children())[:-1]).to(DEVICE)\nfeat_extractor.eval()\n\nprint(\"✅ loaded checkpoint + built feat_extractor\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}