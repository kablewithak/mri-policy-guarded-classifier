{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14772061,"sourceType":"datasetVersion","datasetId":9442378},{"sourceId":14841445,"sourceType":"datasetVersion","datasetId":9492413}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# MRI Tumor Classifier (Multi-Slice MVP) — Build Notebook\n\n**Purpose:** Reproducible Kaggle notebook that writes a small Python package (`mri-mvp`), installs it, runs smoke tests, and launches a **public Gradio demo**.\n\n**Run policy:** Run cells top-to-bottom. Only re-run the launch after running the reset cell.\n","metadata":{}},{"cell_type":"markdown","source":"## 1. Project scaffold (filesystem + package folders)\n","metadata":{}},{"cell_type":"code","source":"# CELL: 00_MODEL_BUNDLE_PATHS - Locate model/policy artifacts in Kaggle inputs\n\nfrom pathlib import Path\n\nINPUT_ROOT = Path(\"/kaggle/input\")\nprint(\"Mounted inputs:\")\nfor p in sorted(INPUT_ROOT.iterdir()):\n    if p.is_dir():\n        print(\" -\", p.name)\n\nNEEDED = [\n    \"best_model.pth\",\n    \"temperature_scaling.json\",\n    \"final_policy_config.json\",\n    \"domain_guard_lr.npz\",\n]\n\ndef find(name: str) -> Path | None:\n    hits = list(INPUT_ROOT.rglob(name))\n    if not hits:\n        return None\n    hits = sorted(hits, key=lambda p: (len(p.parts), str(p)))\n    return hits[0]\n\npaths = {n: find(n) for n in NEEDED}\nprint(\"\\nResolved paths:\")\nfor k,v in paths.items():\n    print(f\"{k}: {v}\")\n\nmissing = [k for k,v in paths.items() if v is None]\nassert not missing, f\"Missing artifacts: {missing}. Attach/upload the model bundle.\"\n\nMODEL_BUNDLE_DIR = paths[\"best_model.pth\"].parent\nprint(\"\\nMODEL_BUNDLE_DIR:\", MODEL_BUNDLE_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:34:57.553931Z","iopub.execute_input":"2026-02-14T18:34:57.554503Z","iopub.status.idle":"2026-02-14T18:34:57.600843Z","shell.execute_reply.started":"2026-02-14T18:34:57.554469Z","shell.execute_reply":"2026-02-14T18:34:57.599796Z"}},"outputs":[{"name":"stdout","text":"Mounted inputs:\n - datasets\n\nResolved paths:\nbest_model.pth: /kaggle/input/datasets/kabomolefe/mri-model-bundle-v1/best_model.pth\ntemperature_scaling.json: /kaggle/input/datasets/kabomolefe/mri-model-bundle-v1/temperature_scaling.json\nfinal_policy_config.json: /kaggle/input/datasets/kabomolefe/mri-model-bundle-v1/final_policy_config.json\ndomain_guard_lr.npz: /kaggle/input/datasets/kabomolefe/mri-model-bundle-v1/domain_guard_lr.npz\n\nMODEL_BUNDLE_DIR: /kaggle/input/datasets/kabomolefe/mri-model-bundle-v1\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# CELL: 01_SCAFFOLD\n!pwd\n!mkdir -p /kaggle/working/mri-mvp/src/mri/{io,preprocess,qc,infer,serving,application} /kaggle/working/mri-mvp/tests /kaggle/working/mri-mvp/scripts\n!touch /kaggle/working/mri-mvp/src/mri/__init__.py\n!touch /kaggle/working/mri-mvp/src/mri/io/__init__.py\n!touch /kaggle/working/mri-mvp/src/mri/preprocess/__init__.py\n!touch /kaggle/working/mri-mvp/src/mri/qc/__init__.py\n!touch /kaggle/working/mri-mvp/src/mri/infer/__init__.py\n!touch /kaggle/working/mri-mvp/src/mri/serving/__init__.py\n!touch /kaggle/working/mri-mvp/src/mri/application/__init__.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:34:57.603112Z","iopub.execute_input":"2026-02-14T18:34:57.603458Z","iopub.status.idle":"2026-02-14T18:34:58.642729Z","shell.execute_reply.started":"2026-02-14T18:34:57.603429Z","shell.execute_reply":"2026-02-14T18:34:58.640594Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## 2. Dependencies (pinned for reproducibility)\n\n> After this cell, Kaggle may prompt you to restart the kernel. Do it once, then continue.\n","metadata":{}},{"cell_type":"code","source":"# CELL: 02_INSTALL_DEPS (PINNED)\n%pip install -q \\\n  \"gradio==5.49.1\" \\\n  \"httpx==0.28.1\" \\\n  \"httpcore==1.0.9\" \\\n  \"fastapi\" \\\n  \"uvicorn\" \\\n  \"pydicom\" \\\n  \"pandas\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:34:58.644574Z","iopub.execute_input":"2026-02-14T18:34:58.644878Z","iopub.status.idle":"2026-02-14T18:35:08.046580Z","shell.execute_reply.started":"2026-02-14T18:34:58.644845Z","shell.execute_reply":"2026-02-14T18:35:08.045210Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\nlangchain-core 0.3.79 requires packaging<26.0.0,>=23.2.0, but you have packaging 26.0rc2 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## 3. Source modules (written into `/kaggle/working/mri-mvp/src/mri/...`)\n","metadata":{}},{"cell_type":"markdown","source":"### 3.1 IO — upload ingestion (images + DICOM ZIP + nested ZIP)\n","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/mri-mvp/src/mri/io/load_case.py\n# CELL: 03_LOAD_CASE (IO loader)\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Any, Tuple\nimport os\nimport zipfile\nimport tempfile\n\nimport numpy as np\nfrom PIL import Image\nimport pydicom\n\n\n@dataclass\nclass SliceRecord:\n    image: Image.Image\n    meta: Dict[str, Any]\n\n\ndef _looks_like_dicom(path: str) -> bool:\n    try:\n        ds = pydicom.dcmread(path, stop_before_pixels=True, force=True)\n        return hasattr(ds, \"SeriesInstanceUID\") or hasattr(ds, \"StudyInstanceUID\") or hasattr(ds, \"SOPClassUID\")\n    except Exception:\n        return False\n\n\ndef _to_uint8(arr: np.ndarray) -> np.ndarray:\n    a = arr.astype(np.float32)\n    lo, hi = np.percentile(a, (1, 99))\n    if not np.isfinite(lo) or not np.isfinite(hi) or hi <= lo:\n        lo = float(a.min())\n        hi = float(a.max() if a.max() > a.min() else a.min() + 1.0)\n    a = np.clip((a - lo) / (hi - lo), 0.0, 1.0)\n    return (a * 255.0).astype(np.uint8)\n\n\ndef _load_dicom_series(paths: List[str]) -> Tuple[List[SliceRecord], List[str]]:\n    warnings: List[str] = []\n    items: List[SliceRecord] = []\n\n    def _inst(p: str):\n        try:\n            ds = pydicom.dcmread(p, stop_before_pixels=True, force=True)\n            v = getattr(ds, \"InstanceNumber\", None)\n            return int(v) if v is not None else 10**9\n        except Exception:\n            return 10**9\n\n    paths = sorted(paths, key=_inst)\n\n    for p in paths:\n        try:\n            ds = pydicom.dcmread(p, force=True)\n            if not hasattr(ds, \"pixel_array\"):\n                continue\n            arr = ds.pixel_array\n\n            photo = str(getattr(ds, \"PhotometricInterpretation\", \"\")).upper()\n            if photo == \"MONOCHROME1\":\n                arr = arr.max() - arr\n\n            arr8 = _to_uint8(arr)\n            img = Image.fromarray(arr8).convert(\"RGB\")\n            items.append(SliceRecord(image=img, meta={\"path\": p}))\n        except Exception as e:\n            warnings.append(f\"Skipped slice (decode failed): {os.path.basename(p)} | {type(e).__name__}\")\n\n    if not items:\n        warnings.append(\"No decodable DICOM pixel data found (might be compressed).\")\n\n    return items, warnings\n\n\ndef _load_images(paths: List[str]) -> Tuple[List[SliceRecord], List[str]]:\n    warnings: List[str] = []\n    out: List[SliceRecord] = []\n    for p in paths:\n        try:\n            img = Image.open(p).convert(\"RGB\")\n            out.append(SliceRecord(image=img, meta={\"path\": p}))\n        except Exception:\n            warnings.append(f\"Could not open as image: {os.path.basename(p)}\")\n    return out, warnings\n\n\ndef load_case(filepaths: List[str]) -> Tuple[List[SliceRecord], Dict[str, Any]]:\n    warnings: List[str] = []\n    filepaths = [str(p) for p in (filepaths or [])]\n\n    if not filepaths:\n        return [], {\"warnings\": [\"No files provided.\"]}\n\n    # ZIP case (supports nested zips)\n    if len(filepaths) == 1 and filepaths[0].lower().endswith(\".zip\"):\n        zip_path = filepaths[0]\n\n        with tempfile.TemporaryDirectory() as tmp:\n            # extract outer zip\n            with zipfile.ZipFile(zip_path, \"r\") as z:\n                z.extractall(tmp)\n\n            # find nested zips (depth=1) and extract them too\n            nested = []\n            for root, _, files in os.walk(tmp):\n                for fn in files:\n                    if fn.lower().endswith(\".zip\"):\n                        nested.append(os.path.join(root, fn))\n\n            if nested:\n                warnings.append(f\"Found nested zip(s): {[os.path.basename(n) for n in nested[:5]]}\")\n                for nz in nested[:5]:\n                    sub = os.path.join(tmp, \"_nested_\" + os.path.basename(nz))\n                    os.makedirs(sub, exist_ok=True)\n                    try:\n                        with zipfile.ZipFile(nz, \"r\") as z2:\n                            z2.extractall(sub)\n                    except Exception as e:\n                        warnings.append(f\"Could not extract nested zip {os.path.basename(nz)}: {type(e).__name__}\")\n\n            # collect all non-zip files (including extracted nested contents)\n            all_files = []\n            for root, _, files in os.walk(tmp):\n                for fn in files:\n                    if fn.lower().endswith(\".zip\"):\n                        continue\n                    p = os.path.join(root, fn)\n                    try:\n                        if os.path.getsize(p) < 128:\n                            continue\n                    except Exception:\n                        continue\n                    all_files.append(p)\n\n            dicom_paths = [p for p in all_files if _looks_like_dicom(p)]\n\n            if not dicom_paths:\n                warnings.append(\"ZIP contained no DICOM files (detected by header).\")\n                warnings.append(f\"Sample files: {[os.path.relpath(p, tmp) for p in all_files[:25]]}\")\n                return [], {\"warnings\": warnings}\n\n            # IMPORTANT: load happens INSIDE tempdir context (prevents FileNotFoundError)\n            slices, w2 = _load_dicom_series(dicom_paths)\n            warnings.extend(w2)\n            return slices, {\"warnings\": warnings}\n\n    # Otherwise treat as regular images\n    slices, w = _load_images(filepaths)\n    warnings.extend(w)\n    return slices, {\"warnings\": warnings}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:35:08.049354Z","iopub.execute_input":"2026-02-14T18:35:08.049710Z","iopub.status.idle":"2026-02-14T18:35:08.060102Z","shell.execute_reply.started":"2026-02-14T18:35:08.049678Z","shell.execute_reply":"2026-02-14T18:35:08.059022Z"}},"outputs":[{"name":"stdout","text":"Writing /kaggle/working/mri-mvp/src/mri/io/load_case.py\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"### 3.2 Preprocess — crop/pad + normalization helpers\n","metadata":{}},{"cell_type":"code","source":"\n%%writefile /kaggle/working/mri-mvp/src/mri/preprocess/core.py\n# CELL: 04_PREPROCESS\nfrom __future__ import annotations\nfrom typing import Dict, Any, Tuple\nimport numpy as np\nfrom PIL import Image\n\n\ndef crop_pad_square(img: Image.Image, margin: int = 8) -> Tuple[Image.Image, Dict[str, Any]]:\n    \"\"\"\n    Robust ROI crop (foreground bbox) + square pad.\n    Returns processed image + metadata.\n\n    Why: Prevents \"tiny brain\" (too much black border) and prevents distortion.\n    \"\"\"\n    meta: Dict[str, Any] = {\"ok\": True, \"reason\": None, \"bbox\": None}\n\n    try:\n        rgb = img.convert(\"RGB\")\n        gray = rgb.convert(\"L\")\n        arr = np.array(gray)\n\n        # Dynamic threshold: more robust across varying contrasts than a hard-coded number\n        thr = np.percentile(arr, 20)  # background-ish percentile\n        thr = max(10, thr)            # avoid thresholds that are too low\n        mask = arr > thr\n\n        coords = np.argwhere(mask)\n        if coords.size == 0:\n            meta[\"ok\"] = False\n            meta[\"reason\"] = \"no_foreground_detected\"\n            return rgb, meta\n\n        y0, x0 = coords.min(axis=0)\n        y1, x1 = coords.max(axis=0) + 1\n\n        # Add margin (clamped)\n        y0 = max(0, y0 - margin)\n        x0 = max(0, x0 - margin)\n        y1 = min(arr.shape[0], y1 + margin)\n        x1 = min(arr.shape[1], x1 + margin)\n\n        meta[\"bbox\"] = (int(x0), int(y0), int(x1), int(y1))\n\n        cropped = rgb.crop((x0, y0, x1, y1))\n\n        # Pad to square\n        w, h = cropped.size\n        s = max(w, h)\n        canvas = Image.new(\"RGB\", (s, s), (0, 0, 0))\n        canvas.paste(cropped, ((s - w) // 2, (s - h) // 2))\n\n        return canvas, meta\n\n    except Exception:\n        meta[\"ok\"] = False\n        meta[\"reason\"] = \"preprocess_exception\"\n        return img.convert(\"RGB\"), meta\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:35:08.061418Z","iopub.execute_input":"2026-02-14T18:35:08.062029Z","iopub.status.idle":"2026-02-14T18:35:08.087975Z","shell.execute_reply.started":"2026-02-14T18:35:08.062000Z","shell.execute_reply":"2026-02-14T18:35:08.087076Z"}},"outputs":[{"name":"stdout","text":"Writing /kaggle/working/mri-mvp/src/mri/preprocess/core.py\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"### 3.3 QC — basic slice validity checks (incl. grayscale gate)\n","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/mri-mvp/src/mri/qc/basic.py\n# CELL: 05_QC\nfrom __future__ import annotations\nfrom typing import Dict, Any, Tuple\nimport numpy as np\nfrom PIL import Image\n\ndef qc_slice(img: Image.Image, min_foreground_ratio: float = 0.05, min_std: float = 5.0) -> Tuple[bool, Dict[str, Any]]:\n    \"\"\"\n    Basic QC to reject garbage slices before the model sees them.\n\n    Added:\n    - grayscale_score gate: rejects colorful screenshots/photos\n    \"\"\"\n    rgb = img.convert(\"RGB\")\n    gray = rgb.convert(\"L\")\n    arr = np.array(gray).astype(np.float32)\n\n    # Existing QC\n    fg_ratio = float((arr > 10).mean())\n    std = float(arr.std())\n    mean = float(arr.mean())\n\n    # NEW: \"Is this grayscale-like?\"\n    rgb_arr = np.array(rgb).astype(np.float32)  # HxWx3\n    # per-pixel channel spread, then average\n    grayscale_score = float(np.mean(np.std(rgb_arr, axis=2)))  # 0 for perfect grayscale\n\n    ok = True\n    reasons = []\n\n    if fg_ratio < min_foreground_ratio:\n        ok = False\n        reasons.append(\"too_much_background\")\n\n    if std < min_std:\n        ok = False\n        reasons.append(\"low_contrast\")\n\n    # Tune threshold: start strict to block screenshots\n    # Typical MRI grayscale_score is near ~0–2; screenshots much higher.\n    if grayscale_score > 3.0:\n        ok = False\n        reasons.append(\"not_grayscale_like\")\n\n    return ok, {\n        \"foreground_ratio\": fg_ratio,\n        \"mean\": mean,\n        \"std\": std,\n        \"grayscale_score\": grayscale_score,\n        \"reasons\": reasons,\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:35:08.089442Z","iopub.execute_input":"2026-02-14T18:35:08.090018Z","iopub.status.idle":"2026-02-14T18:35:08.115814Z","shell.execute_reply.started":"2026-02-14T18:35:08.089985Z","shell.execute_reply":"2026-02-14T18:35:08.114708Z"}},"outputs":[{"name":"stdout","text":"Writing /kaggle/working/mri-mvp/src/mri/qc/basic.py\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"### 3.4 Inference core — model load + per-slice + case aggregation\n","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/mri-mvp/src/mri/infer/predictor.py\n# CELL: 06_PREDICTOR\n\"\"\"\nPredictor + policy engine\n\nGoals:\n- Accept *either* a raw state_dict checkpoint (legacy MVP) OR the training notebook's dict checkpoint.\n- Drive label order, temperature scaling, and abstain/OOD policy from training artifacts when available.\n- Always return a structured result: ACCEPT / ABSTAIN_* with reason codes (never \"no output\").\n\"\"\"\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Any, Optional, Tuple\nimport os\nimport json\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import models, transforms\nfrom PIL import Image\n\nfrom mri.preprocess.core import crop_pad_square\nfrom mri.qc.basic import qc_slice\n\n\nIMG_SIZE = 224\n\n# --- safe legacy fallback ONLY (used when no label_map is present) ---\nLEGACY_LABELS = [\"Glioma\", \"Meningioma\", \"No Tumor\", \"Pituitary\"]\n\n\n@dataclass(frozen=True)\nclass DomainGuard:\n    \"\"\"Binary logistic regression p(in_domain) = sigmoid(w·z + b).\"\"\"\n    w: torch.Tensor  # (D,) on CPU\n    b: float         # scalar\n\n    def score(self, z: torch.Tensor) -> float:\n        # z: (D,) CPU tensor\n        x = float(torch.dot(self.w, z).item() + self.b)\n        p = 1.0 / (1.0 + float(torch.exp(torch.tensor(-x)).item()))\n        return float(p)\n\n\n@dataclass\nclass ModelBundle:\n    model: nn.Module\n    feat_extractor: nn.Module\n    labels: List[str]                  # index -> label string\n    label_map_raw: Optional[Dict[str, str]]\n    num_classes: int\n    temperature: float                 # T (>= 0.05)\n    tau_conf: Optional[float]\n    tau_domain: Optional[float]\n    domain_guard: Optional[DomainGuard]\n    source_dir: str\n\n\ndef _read_json(path: str) -> Optional[dict]:\n    try:\n        with open(path, \"r\") as f:\n            return json.load(f)\n    except Exception:\n        return None\n\n\ndef _clean_state_dict(sd: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n    # Support DataParallel checkpoints\n    return {k.replace(\"module.\", \"\"): v for k, v in sd.items()}\n\n\ndef _resolve_bundle_dir(model_ref: str) -> Tuple[str, str]:\n    \"\"\"\n    Returns (bundle_dir, ckpt_path).\n    - If model_ref is a directory, look for best_model.pth inside.\n    - If model_ref is a file, treat its parent as bundle_dir.\n    \"\"\"\n    if os.path.isdir(model_ref):\n        bundle_dir = model_ref\n        ckpt_path = os.path.join(bundle_dir, \"best_model.pth\")\n        if not os.path.exists(ckpt_path):\n            # allow alternate naming if user copied it\n            alt = os.path.join(bundle_dir, \"mri_resnet18_baseline_best.pth\")\n            if os.path.exists(alt):\n                ckpt_path = alt\n        return bundle_dir, ckpt_path\n\n    bundle_dir = os.path.dirname(model_ref) or \".\"\n    return bundle_dir, model_ref\n\n\ndef load_bundle(model_ref: str, device: torch.device) -> ModelBundle:\n    \"\"\"\n    Loads a model bundle.\n    model_ref can be:\n      - path to .pth (legacy MVP state_dict OR training dict checkpoint)\n      - directory containing best_model.pth + policy artifacts\n\n    Policy artifacts (optional but preferred):\n      - temperature_scaling.json   {\"temperature\": T, ...}\n      - final_policy_config.json   {\"tau_conf\": ..., \"tau_domain\": ..., \"temperature_T\": ...}\n      - domain_guard_lr.npz        {\"coef\": (1,D), \"intercept\": (1,)}  OR {\"w\": (D,), \"b\": ()}\n    \"\"\"\n    if not os.path.exists(model_ref):\n        raise RuntimeError(f\"Model reference not found: {model_ref}\")\n\n    bundle_dir, ckpt_path = _resolve_bundle_dir(model_ref)\n\n    if not os.path.exists(ckpt_path):\n        raise RuntimeError(f\"Checkpoint not found: {ckpt_path}\")\n\n    # Sanity: avoid tiny invalid files (e.g., pointer)\n    size_mb = os.path.getsize(ckpt_path) / (1024 * 1024)\n    if size_mb < 10:\n        raise RuntimeError(\n            f\"Checkpoint looks invalid (too small: {size_mb:.2f} MB): {ckpt_path}\"\n        )\n\n    raw = torch.load(ckpt_path, map_location=\"cpu\")\n\n    # Determine checkpoint shape\n    if isinstance(raw, dict) and \"model_state_dict\" in raw:\n        sd = raw[\"model_state_dict\"]\n        label_map = raw.get(\"label_map\")  # often {0:\"glioma\",...} but JSON may coerce keys to str elsewhere\n        num_classes = int(raw.get(\"num_classes\", 4))\n    elif isinstance(raw, dict):\n        # Legacy: assume it's a raw state_dict\n        sd = raw\n        label_map = None\n        num_classes = 4\n    else:\n        raise RuntimeError(f\"Unsupported checkpoint format: type={type(raw)}\")\n\n    sd = _clean_state_dict(sd)\n\n    # Build model\n    model = models.resnet18(weights=None)\n    model.fc = nn.Linear(model.fc.in_features, num_classes)\n    model.load_state_dict(sd, strict=True)\n    model.to(device).eval()\n\n    # Feature extractor = penultimate pooled features\n    feat_extractor = nn.Sequential(*list(model.children())[:-1]).to(device).eval()\n\n    # Labels from training artifact if possible\n    labels: List[str]\n    label_map_raw: Optional[Dict[str, str]] = None\n\n    if isinstance(label_map, dict) and len(label_map) == num_classes:\n        # normalize keys to int order\n        label_map_raw = {str(k): str(v) for k, v in label_map.items()}\n        labels = [label_map_raw[str(i)] for i in range(num_classes)]\n    else:\n        labels = LEGACY_LABELS[:num_classes]\n\n    # Load temperature\n    T = 1.0\n    ts = _read_json(os.path.join(bundle_dir, \"temperature_scaling.json\"))\n    if isinstance(ts, dict) and \"temperature\" in ts:\n        try:\n            T = float(ts[\"temperature\"])\n        except Exception:\n            T = 1.0\n    T = float(max(0.05, min(T, 100.0)))\n\n    # Load policy thresholds\n    tau_conf = None\n    tau_domain = None\n    pol = _read_json(os.path.join(bundle_dir, \"final_policy_config.json\"))\n    if isinstance(pol, dict):\n        if \"tau_conf\" in pol:\n            try:\n                tau_conf = float(pol[\"tau_conf\"])\n            except Exception:\n                tau_conf = None\n        if \"tau_domain\" in pol:\n            try:\n                tau_domain = float(pol[\"tau_domain\"])\n            except Exception:\n                tau_domain = None\n        # If policy config includes temperature_T, prefer it over temperature_scaling.json\n        if \"temperature_T\" in pol:\n            try:\n                T = float(pol[\"temperature_T\"])\n                T = float(max(0.05, min(T, 100.0)))\n            except Exception:\n                pass\n\n    # Load domain guard weights (no sklearn dependency)\n    domain_guard = None\n    npz_path = os.path.join(bundle_dir, \"domain_guard_lr.npz\")\n    if os.path.exists(npz_path):\n        try:\n            import numpy as np\n            d = np.load(npz_path)\n            if \"coef\" in d and \"intercept\" in d:\n                coef = d[\"coef\"].astype(\"float32\")\n                intercept = d[\"intercept\"].astype(\"float32\")\n                w = torch.tensor(coef.reshape(-1), dtype=torch.float32)  # (D,)\n                b = float(intercept.reshape(-1)[0])\n                domain_guard = DomainGuard(w=w, b=b)\n            elif \"w\" in d and \"b\" in d:\n                w = torch.tensor(d[\"w\"].astype(\"float32\").reshape(-1), dtype=torch.float32)\n                b = float(d[\"b\"].reshape(-1)[0])\n                domain_guard = DomainGuard(w=w, b=b)\n        except Exception:\n            domain_guard = None\n\n    return ModelBundle(\n        model=model,\n        feat_extractor=feat_extractor,\n        labels=labels,\n        label_map_raw=label_map_raw,\n        num_classes=num_classes,\n        temperature=T,\n        tau_conf=tau_conf,\n        tau_domain=tau_domain,\n        domain_guard=domain_guard,\n        source_dir=bundle_dir,\n    )\n\n\n_val_tf = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n])\n\n\ndef _probs_from_logits(logits_1d: torch.Tensor) -> List[float]:\n    p = torch.softmax(logits_1d, dim=-1).detach().cpu().tolist()\n    return [float(x) for x in p]\n\n\ndef _embed_1d(feat_extractor: nn.Module, x: torch.Tensor) -> torch.Tensor:\n    # x: (1,3,H,W) on device\n    with torch.no_grad():\n        z = feat_extractor(x)  # (1,512,1,1)\n    z = z.view(z.shape[0], -1)[0].detach().cpu()  # (512,) CPU\n    return z\n\n\ndef predict_case(\n    bundle: ModelBundle,\n    slices: List[Image.Image],\n    device: torch.device,\n    min_valid_slices: int = 3,\n    abstain_agree_threshold: float = 0.50,\n) -> Dict[str, Any]:\n    \"\"\"\n    Multi-slice inference with policy:\n      - QC-gate slices\n      - aggregate logits + embeddings across valid slices\n      - apply temperature scaling (if available)\n      - apply policy: ABSTAIN if (p_in_domain < tau_domain) OR (max_prob_cal < tau_conf)\n    \"\"\"\n    labels = bundle.labels\n    k = bundle.num_classes\n\n    processed_images: List[Image.Image] = []\n    per_slice: List[Dict[str, Any]] = []\n    logits_list: List[torch.Tensor] = []\n    emb_list: List[torch.Tensor] = []\n\n    for idx, img in enumerate(slices):\n        proc, pmeta = crop_pad_square(img)\n        processed_images.append(proc)\n\n        ok, qcmeta = qc_slice(proc)\n\n        record: Dict[str, Any] = {\n            \"slice_index\": idx,\n            \"preprocess_ok\": bool(pmeta.get(\"ok\", True)),\n            \"preprocess_reason\": pmeta.get(\"reason\"),\n            \"bbox\": pmeta.get(\"bbox\"),\n            \"qc_ok\": bool(ok),\n            \"qc\": qcmeta,\n            \"top_label\": None,\n            \"top_conf\": None,\n        }\n\n        if not ok:\n            per_slice.append(record)\n            continue\n\n        x = _val_tf(proc).unsqueeze(0).to(device)\n\n        with torch.no_grad():\n            logits = bundle.model(x)[0]  # (K,)\n\n        probs = _probs_from_logits(logits)\n        top_i = int(torch.argmax(torch.tensor(probs)).item())\n\n        record[\"probs\"] = {labels[i]: float(probs[i]) for i in range(k)}\n        record[\"top_label\"] = labels[top_i]\n        record[\"top_conf\"] = float(probs[top_i])\n\n        per_slice.append(record)\n        logits_list.append(logits)\n\n        # embedding (CPU)\n        try:\n            emb = _embed_1d(bundle.feat_extractor, x)\n            emb_list.append(emb)\n        except Exception:\n            # Do not fail inference if embedding path breaks\n            pass\n\n    result: Dict[str, Any] = {\n        \"status\": \"ABSTAIN\",\n        \"abstain_type\": \"QC\",\n        \"abstain_reason\": None,\n        \"case_prediction\": None,\n        \"case_probs\": None,\n        \"valid_slices\": len(logits_list),\n        \"agree_rate\": 0.0,\n        \"top_conf\": None,\n        \"p_in_domain\": None,\n        \"thresholds\": {\n            \"tau_conf\": bundle.tau_conf,\n            \"tau_domain\": bundle.tau_domain,\n            \"temperature\": bundle.temperature,\n        },\n        \"per_slice\": per_slice,\n        \"processed_images\": processed_images,\n    }\n\n    if len(logits_list) < min_valid_slices:\n        result[\"abstain_reason\"] = f\"too_few_valid_slices<{min_valid_slices}\"\n        return result\n\n    # Aggregate logits (stable) and embed\n    case_logits = torch.stack(logits_list, dim=0).mean(dim=0)  # (K,)\n    T = float(bundle.temperature) if bundle.temperature else 1.0\n    case_probs_cal = _probs_from_logits(case_logits / T)\n\n    top_i = int(torch.argmax(torch.tensor(case_probs_cal)).item())\n    top_label = labels[top_i]\n    top_conf = float(case_probs_cal[top_i])\n\n    result[\"case_prediction\"] = top_label\n    result[\"case_probs\"] = {labels[i]: float(case_probs_cal[i]) for i in range(k)}\n    result[\"top_conf\"] = top_conf\n\n    # Disagreement: how many valid slices vote for the same top label?\n    votes = [r.get(\"top_label\") for r in per_slice if r.get(\"qc_ok\") and r.get(\"top_label\") is not None]\n    agree = sum(1 for v in votes if v == top_label) / max(1, len(votes))\n    result[\"agree_rate\"] = float(agree)\n\n    # Domain score (case-level) if we have a guard + embeddings\n    if bundle.domain_guard is not None and len(emb_list) > 0:\n        z_case = torch.stack(emb_list, dim=0).mean(dim=0)  # (D,) CPU\n        p_in = bundle.domain_guard.score(z_case)\n        result[\"p_in_domain\"] = float(p_in)\n\n    # Policy evaluation\n    abstain_domain = False\n    abstain_lowconf = False\n    abstain_disagree = False\n\n    if bundle.tau_domain is not None and result[\"p_in_domain\"] is not None:\n        abstain_domain = result[\"p_in_domain\"] < float(bundle.tau_domain)\n\n    if bundle.tau_conf is not None:\n        abstain_lowconf = top_conf < float(bundle.tau_conf)\n\n    # Optional extra guard (kept from legacy MVP): disagreement can abstain\n    abstain_disagree = (agree < abstain_agree_threshold)\n\n    if abstain_domain:\n        result[\"status\"] = \"ABSTAIN\"\n        result[\"abstain_type\"] = \"OOD\"\n        result[\"abstain_reason\"] = f\"p_in_domain<{bundle.tau_domain:.3f}\"\n        result[\"case_prediction\"] = None\n        result[\"case_probs\"] = {\"ABSTAIN_OOD\": 1.0}\n        return result\n\n    if abstain_lowconf or abstain_disagree:\n        result[\"status\"] = \"ABSTAIN\"\n        result[\"abstain_type\"] = \"UNCERTAIN\"\n        reason_bits = []\n        if abstain_lowconf:\n            reason_bits.append(f\"max_prob_cal<{bundle.tau_conf:.3f}\")\n        if abstain_disagree:\n            reason_bits.append(f\"agree<{abstain_agree_threshold:.2f}\")\n        result[\"abstain_reason\"] = \",\".join(reason_bits) if reason_bits else \"uncertain\"\n        result[\"case_prediction\"] = None\n        result[\"case_probs\"] = {\"ABSTAIN_UNCERTAIN\": 1.0}\n        return result\n\n    # Accept\n    result[\"status\"] = \"ACCEPT\"\n    result[\"abstain_type\"] = None\n    result[\"abstain_reason\"] = None\n    return result\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:35:08.117293Z","iopub.execute_input":"2026-02-14T18:35:08.117599Z","iopub.status.idle":"2026-02-14T18:35:08.147654Z","shell.execute_reply.started":"2026-02-14T18:35:08.117573Z","shell.execute_reply":"2026-02-14T18:35:08.146680Z"}},"outputs":[{"name":"stdout","text":"Writing /kaggle/working/mri-mvp/src/mri/infer/predictor.py\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"### 3.5 Application layer — `InferenceService` (clean architecture seam)\n","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/mri-mvp/src/mri/application/__init__.py\n# Application layer package\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:35:08.149121Z","iopub.execute_input":"2026-02-14T18:35:08.149638Z","iopub.status.idle":"2026-02-14T18:35:08.174442Z","shell.execute_reply.started":"2026-02-14T18:35:08.149597Z","shell.execute_reply":"2026-02-14T18:35:08.173212Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/mri-mvp/src/mri/application/__init__.py\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"%%writefile /kaggle/working/mri-mvp/src/mri/application/inference_service.py\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import List, Dict\n\nimport pandas as pd\nimport torch\nfrom PIL import Image\n\nfrom mri.io.load_case import load_case\nfrom mri.infer.predictor import load_bundle, predict_case, ModelBundle\n\n\nclass ModelNotLoadedError(RuntimeError):\n    pass\n\n\n@dataclass(frozen=True)\nclass InferenceOutputs:\n    header: str\n    gallery: List[Image.Image]\n    case_label: Dict[str, float]\n    table: pd.DataFrame\n    warnings: List[str]\n\n\nclass InferenceService:\n    \"\"\"\n    Application-layer orchestration:\n    - loads/holds model bundle (weights + label map + policy artifacts)\n    - loads case from paths\n    - runs multi-slice inference + abstain/OOD policy\n    - formats outputs for UI adapters\n    \"\"\"\n\n    def __init__(self, model_ref: str, device: torch.device):\n        self.model_ref = model_ref\n        self.device = device\n        self._bundle: ModelBundle | None = None\n        self._model_error: str | None = None\n\n    def load(self) -> None:\n        if self._bundle is not None or self._model_error is not None:\n            return\n        try:\n            self._bundle = load_bundle(self.model_ref, self.device)\n        except Exception as e:\n            self._model_error = str(e)\n\n    def _require_bundle(self) -> ModelBundle:\n        self.load()\n        if self._bundle is None:\n            raise ModelNotLoadedError(\n                \"Model bundle not loaded.\\n\"\n                f\"Expected model_ref at: {self.model_ref}\\n\"\n                f\"Error: {self._model_error}\"\n            )\n        return self._bundle\n\n    def predict_from_paths(self, filepaths: List[str]) -> InferenceOutputs:\n        bundle = self._require_bundle()\n\n        slices, meta = load_case(filepaths)\n        if not slices:\n            raise ValueError(f\"No usable slices. Loader warnings: {meta.get('warnings', [])}\")\n\n        pil_slices = [s.image for s in slices]\n        out = predict_case(bundle, pil_slices, device=self.device)\n\n        # Per-slice table\n        rows = []\n        for r in out[\"per_slice\"]:\n            rows.append({\n                \"slice_index\": r[\"slice_index\"],\n                \"qc_ok\": r[\"qc_ok\"],\n                \"top_label\": r.get(\"top_label\"),\n                \"top_conf\": r.get(\"top_conf\"),\n                \"fg_ratio\": r[\"qc\"][\"foreground_ratio\"],\n                \"std\": r[\"qc\"][\"std\"],\n                \"qc_reasons\": \",\".join(r[\"qc\"][\"reasons\"]),\n            })\n        df = pd.DataFrame(rows)\n\n        # UI label block\n        case_label = out[\"case_probs\"] if out[\"case_probs\"] else {\"ABSTAIN\": 1.0}\n\n        # Header\n        status = out.get(\"status\", \"ABSTAIN\")\n        abstain_type = out.get(\"abstain_type\")\n        abstain_reason = out.get(\"abstain_reason\") or \"\"\n        p_in = out.get(\"p_in_domain\")\n        top_conf = out.get(\"top_conf\")\n\n        header = (\n            f\"{status}\"\n            f\"{'/' + str(abstain_type) if abstain_type else ''}\"\n            f\" | valid_slices={out['valid_slices']}\"\n            f\" | agree={out.get('agree_rate', 0):.2f}\"\n            f\" | top_conf_cal={float(top_conf or 0):.3f}\"\n            f\" | p_in_domain={float(p_in) if p_in is not None else 'na'}\"\n            f\" | {abstain_reason}\"\n        )\n\n        gallery = out[\"processed_images\"][:32]\n        warnings = meta.get(\"warnings\", [])\n\n        return InferenceOutputs(\n            header=header,\n            gallery=gallery,\n            case_label=case_label,\n            table=df,\n            warnings=warnings,\n        )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:35:08.177956Z","iopub.execute_input":"2026-02-14T18:35:08.178453Z","iopub.status.idle":"2026-02-14T18:35:08.199677Z","shell.execute_reply.started":"2026-02-14T18:35:08.178419Z","shell.execute_reply":"2026-02-14T18:35:08.198713Z"}},"outputs":[{"name":"stdout","text":"Writing /kaggle/working/mri-mvp/src/mri/application/inference_service.py\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"### 3.6 Serving — Gradio UI entrypoint (`mri.serving.app:demo`)\n","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/mri-mvp/src/mri/serving/app.py\n# CELL: 07_SERVING_APP\nfrom __future__ import annotations\n\nimport os\nimport gradio as gr\nfrom fastapi import FastAPI\nimport torch\nimport pandas as pd\n\nfrom mri.io.load_case import load_case\nfrom mri.infer.predictor import load_bundle, predict_case, ModelBundle\n\n# Feature flag (safe rollout)\nUSE_SERVICE = os.getenv(\"USE_SERVICE\", \"1\") == \"1\"\n_service = None\n\n# MODEL_REF can be:\n#   - directory with best_model.pth + policy artifacts (preferred)\n#   - path to .pth checkpoint (legacy MVP or training best_model.pth)\nDEFAULT_MODEL_REF = \"/kaggle/working/mri-mvp/best_metric_model.pth\"\nMODEL_REF = os.getenv(\"MODEL_REF\", os.getenv(\"MODEL_PATH\", DEFAULT_MODEL_REF))\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Eager load (gives clearer error message early)\n_bundle: ModelBundle | None = None\n_bundle_error: str | None = None\ntry:\n    _bundle = load_bundle(MODEL_REF, device)\nexcept Exception as e:\n    _bundle_error = str(e)\n\n\ndef run_inference(files):\n    # Normalize uploaded file objects to filepaths\n    files = files or []\n    filepaths = []\n    for f in files:\n        if hasattr(f, \"name\"):\n            filepaths.append(f.name)\n        elif isinstance(f, dict) and \"name\" in f:\n            filepaths.append(f[\"name\"])\n        else:\n            filepaths.append(str(f))\n\n    if _bundle is None:\n        raise gr.Error(\n            \"Model bundle not loaded.\\n\"\n            f\"MODEL_REF: {MODEL_REF}\\n\"\n            f\"Error: {_bundle_error}\"\n        )\n\n    slices, meta = load_case(filepaths)\n    if not slices:\n        raise gr.Error(f\"No usable slices. Loader warnings: {meta.get('warnings', [])}\")\n\n    pil_slices = [s.image for s in slices]\n    out = predict_case(_bundle, pil_slices, device=device)\n\n    rows = []\n    for r in out[\"per_slice\"]:\n        rows.append({\n            \"slice_index\": r[\"slice_index\"],\n            \"qc_ok\": r[\"qc_ok\"],\n            \"top_label\": r.get(\"top_label\"),\n            \"top_conf\": r.get(\"top_conf\"),\n            \"fg_ratio\": r[\"qc\"][\"foreground_ratio\"],\n            \"std\": r[\"qc\"][\"std\"],\n            \"qc_reasons\": \",\".join(r[\"qc\"][\"reasons\"]),\n        })\n    df = pd.DataFrame(rows)\n\n    case_label = out[\"case_probs\"] if out[\"case_probs\"] else {\"ABSTAIN\": 1.0}\n\n    status = out.get(\"status\", \"ABSTAIN\")\n    abstain_type = out.get(\"abstain_type\")\n    abstain_reason = out.get(\"abstain_reason\") or \"\"\n    top_conf = out.get(\"top_conf\") or 0.0\n    p_in = out.get(\"p_in_domain\")\n\n    header = (\n        f\"{status}\"\n        f\"{'/' + str(abstain_type) if abstain_type else ''}\"\n        f\" | valid_slices={out['valid_slices']}\"\n        f\" | agree={out.get('agree_rate', 0):.2f}\"\n        f\" | top_conf_cal={float(top_conf):.3f}\"\n        f\" | p_in_domain={float(p_in) if p_in is not None else 'na'}\"\n        f\" | {abstain_reason}\"\n    )\n\n    gallery = out[\"processed_images\"][:32]\n    return header, gallery, case_label, df, meta.get(\"warnings\", [])\n\n\ndemo = gr.Interface(\n    fn=run_inference,\n    inputs=gr.Files(label=\"Upload multiple images OR a ZIP containing a DICOM series\"),\n    outputs=[\n        gr.Textbox(label=\"Case Status\"),\n        gr.Gallery(label=\"What the model saw (processed slices)\", columns=4, rows=2),\n        gr.Label(num_top_classes=4, label=\"Case Prediction / Abstain\"),\n        gr.Dataframe(label=\"Per-slice QC + Predictions\"),\n        gr.JSON(label=\"Loader Warnings\"),\n    ],\n    title=\"MRI Tumor Classifier (Multi-Slice MVP)\",\n    description=(\n        \"Upload multiple image slices or a ZIP containing a DICOM series. \"\n        \"Returns case-level prediction OR ABSTAIN (OOD/uncertain) with debug signals.\"\n    )\n)\n\napp = FastAPI()\napp = gr.mount_gradio_app(app, demo, path=\"/\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:35:08.201422Z","iopub.execute_input":"2026-02-14T18:35:08.201808Z","iopub.status.idle":"2026-02-14T18:35:08.224383Z","shell.execute_reply.started":"2026-02-14T18:35:08.201769Z","shell.execute_reply":"2026-02-14T18:35:08.223344Z"}},"outputs":[{"name":"stdout","text":"Writing /kaggle/working/mri-mvp/src/mri/serving/app.py\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"%%writefile /kaggle/working/mri-mvp/tests/test_io_nested_zip.py\nfrom __future__ import annotations\n\nimport os\nimport zipfile\nfrom pathlib import Path\n\nimport numpy as np\nimport pytest\n\n# If pydicom isn't installed, these tests cannot run (and your loader won't work either).\npydicom = pytest.importorskip(\"pydicom\")\nfrom pydicom.dataset import FileDataset, FileMetaDataset\nfrom pydicom.uid import ExplicitVRLittleEndian, generate_uid\n\nfrom mri.io.load_case import load_case\n\n\ndef _write_synthetic_dicom(\n    path: Path,\n    *,\n    instance_number: int,\n    series_uid: str,\n    study_uid: str,\n    arr: np.ndarray,\n) -> None:\n    \"\"\"\n    Writes a minimal uncompressed MR-like DICOM slice with valid pixel data.\n    File may be extensionless; loader relies on header detection.\n    \"\"\"\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n    file_meta = FileMetaDataset()\n    file_meta.TransferSyntaxUID = ExplicitVRLittleEndian\n    file_meta.MediaStorageSOPClassUID = \"1.2.840.10008.5.1.4.1.1.7\"  # Secondary Capture\n    file_meta.MediaStorageSOPInstanceUID = generate_uid()\n    file_meta.ImplementationClassUID = generate_uid()\n\n    ds = FileDataset(\n        filename_or_obj=str(path),\n        dataset={},\n        file_meta=file_meta,\n        preamble=b\"\\0\" * 128,\n    )\n\n    # Minimal identity / modality fields (not strictly required by _looks_like_dicom, but good hygiene)\n    ds.SOPClassUID = file_meta.MediaStorageSOPClassUID\n    ds.SOPInstanceUID = file_meta.MediaStorageSOPInstanceUID\n    ds.StudyInstanceUID = study_uid\n    ds.SeriesInstanceUID = series_uid\n    ds.InstanceNumber = int(instance_number)\n    ds.Modality = \"MR\"\n    ds.PatientID = \"TEST_PATIENT\"\n    ds.PatientName = \"TEST^PATIENT\"\n\n    # Pixel data fields for a single-channel image\n    if arr.dtype != np.uint16:\n        arr = arr.astype(np.uint16)\n\n    rows, cols = arr.shape\n    ds.Rows = int(rows)\n    ds.Columns = int(cols)\n    ds.SamplesPerPixel = 1\n    ds.PhotometricInterpretation = \"MONOCHROME2\"\n    ds.BitsAllocated = 16\n    ds.BitsStored = 16\n    ds.HighBit = 15\n    ds.PixelRepresentation = 0  # unsigned\n    ds.PixelData = arr.tobytes()\n\n    # pydicom v4+ prefers explicit args at write time\n    ds.save_as(\n        str(path),\n        enforce_file_format=True,\n        little_endian=True,\n        implicit_vr=False,\n    )\n\n\n\ndef _zip_dir(zip_path: Path, root_dir: Path) -> None:\n    \"\"\"\n    Zips the entire directory tree under root_dir into zip_path.\n    \"\"\"\n    zip_path.parent.mkdir(parents=True, exist_ok=True)\n    with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n        for p in root_dir.rglob(\"*\"):\n            if p.is_file():\n                z.write(p, arcname=p.relative_to(root_dir))\n\n\ndef _make_dicom_series(dir_path: Path, n: int = 5) -> list[Path]:\n    \"\"\"\n    Creates a small synthetic DICOM series in a deep folder tree with extensionless files.\n    \"\"\"\n    series_uid = generate_uid()\n    study_uid = generate_uid()\n\n    out_paths: list[Path] = []\n    for i in range(1, n + 1):\n        # simple gradient-ish pattern per slice\n        arr = np.full((64, 64), i * 500, dtype=np.uint16)\n        # extensionless on purpose\n        p = dir_path / f\"IM_{i:04d}\"\n        _write_synthetic_dicom(\n            p,\n            instance_number=i,\n            series_uid=series_uid,\n            study_uid=study_uid,\n            arr=arr,\n        )\n        out_paths.append(p)\n    return out_paths\n\n\ndef test_load_case_zip_extensionless_dicom_deep_tree(tmp_path: Path):\n    \"\"\"\n    ZIP ingestion should:\n    - discover DICOMs by header (not extension)\n    - handle deep folder structures\n    - return decoded PIL images as SliceRecords\n    \"\"\"\n    deep_root = tmp_path / \"outer_payload\" / \"a\" / \"b\" / \"c\" / \"series\"\n    _make_dicom_series(deep_root, n=6)\n\n    zip_path = tmp_path / \"case.zip\"\n    _zip_dir(zip_path, tmp_path / \"outer_payload\")\n\n    slices, meta = load_case([str(zip_path)])\n\n    assert isinstance(meta, dict)\n    assert \"warnings\" in meta\n    assert isinstance(meta[\"warnings\"], list)\n\n    assert len(slices) == 6, f\"Expected 6 slices, got {len(slices)}. Warnings: {meta['warnings']}\"\n    # basic sanity on returned objects\n    for s in slices:\n        assert hasattr(s, \"image\")\n        assert s.image.size == (64, 64)\n\n\ndef test_load_case_nested_zip_extensionless_dicom(tmp_path: Path):\n    \"\"\"\n    Nested ZIP ingestion should:\n    - extract outer zip\n    - find inner zip(s)\n    - extract inner zip(s)\n    - discover DICOMs by header\n    \"\"\"\n    inner_root = tmp_path / \"inner_payload\" / \"deep\" / \"series\"\n    _make_dicom_series(inner_root, n=4)\n\n    inner_zip = tmp_path / \"inner.zip\"\n    _zip_dir(inner_zip, tmp_path / \"inner_payload\")\n\n    # Outer zip contains the inner zip in a nested folder\n    outer_payload = tmp_path / \"outer_payload\"\n    (outer_payload / \"x\" / \"y\").mkdir(parents=True, exist_ok=True)\n    (outer_payload / \"x\" / \"y\" / \"inner.zip\").write_bytes(inner_zip.read_bytes())\n\n    outer_zip = tmp_path / \"outer.zip\"\n    _zip_dir(outer_zip, outer_payload)\n\n    slices, meta = load_case([str(outer_zip)])\n\n    warnings = meta.get(\"warnings\", [])\n    assert any(\"Found nested zip\" in w for w in warnings), f\"Expected nested-zip warning. Got: {warnings}\"\n\n    assert len(slices) == 4, f\"Expected 4 slices, got {len(slices)}. Warnings: {warnings}\"\n    for s in slices:\n        assert s.image.size == (64, 64)\n\n\ndef test_load_case_zip_with_no_dicoms_returns_clean_warnings(tmp_path: Path):\n    \"\"\"\n    If a ZIP contains no DICOMs (by header), loader should return ([], warnings)\n    and not crash.\n    \"\"\"\n    payload = tmp_path / \"payload\"\n    payload.mkdir(parents=True, exist_ok=True)\n    # put a random small file\n    (payload / \"readme.txt\").write_text(\"hello\", encoding=\"utf-8\")\n\n    zip_path = tmp_path / \"no_dicoms.zip\"\n    _zip_dir(zip_path, payload)\n\n    slices, meta = load_case([str(zip_path)])\n    warnings = meta.get(\"warnings\", [])\n\n    assert slices == []\n    assert any(\"ZIP contained no DICOM files\" in w for w in warnings), f\"Expected no-dicom warning. Got: {warnings}\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:35:08.225582Z","iopub.execute_input":"2026-02-14T18:35:08.226020Z","iopub.status.idle":"2026-02-14T18:35:08.251045Z","shell.execute_reply.started":"2026-02-14T18:35:08.225951Z","shell.execute_reply":"2026-02-14T18:35:08.249849Z"}},"outputs":[{"name":"stdout","text":"Writing /kaggle/working/mri-mvp/tests/test_io_nested_zip.py\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"!pytest -q /kaggle/working/mri-mvp/tests/test_io_nested_zip.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:35:08.252411Z","iopub.execute_input":"2026-02-14T18:35:08.252674Z","iopub.status.idle":"2026-02-14T18:35:11.765691Z","shell.execute_reply.started":"2026-02-14T18:35:08.252650Z","shell.execute_reply":"2026-02-14T18:35:11.764434Z"}},"outputs":[{"name":"stdout","text":"\n==================================== ERRORS ====================================\n\u001b[31m\u001b[1m_____________ ERROR collecting mri-mvp/tests/test_io_nested_zip.py _____________\u001b[0m\n\u001b[31mImportError while importing test module '/kaggle/working/mri-mvp/tests/test_io_nested_zip.py'.\nHint: make sure your test modules/packages have valid Python names.\nTraceback:\n/usr/lib/python3.12/importlib/__init__.py:90: in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nmri-mvp/tests/test_io_nested_zip.py:15: in <module>\n    from mri.io.load_case import load_case\nE   ModuleNotFoundError: No module named 'mri'\u001b[0m\n\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n\u001b[31mERROR\u001b[0m mri-mvp/tests/test_io_nested_zip.py\n!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!\n\u001b[31m\u001b[31m\u001b[1m1 error\u001b[0m\u001b[31m in 1.00s\u001b[0m\u001b[0m\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"### 3.7 Tests — minimal smoke imports\n","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/mri-mvp/tests/test_smoke_inference_service.py\ndef test_import_service():\n    from mri.application.inference_service import InferenceService\n    assert InferenceService is not None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:35:11.767383Z","iopub.execute_input":"2026-02-14T18:35:11.767897Z","iopub.status.idle":"2026-02-14T18:35:11.775042Z","shell.execute_reply.started":"2026-02-14T18:35:11.767860Z","shell.execute_reply":"2026-02-14T18:35:11.773984Z"}},"outputs":[{"name":"stdout","text":"Writing /kaggle/working/mri-mvp/tests/test_smoke_inference_service.py\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"%%writefile /kaggle/working/mri-mvp/tests/test_smoke_serving.py\ndef test_import_demo():\n    from mri.serving.app import demo\n    assert demo is not None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:35:11.776369Z","iopub.execute_input":"2026-02-14T18:35:11.777298Z","iopub.status.idle":"2026-02-14T18:35:11.797153Z","shell.execute_reply.started":"2026-02-14T18:35:11.777224Z","shell.execute_reply":"2026-02-14T18:35:11.796129Z"}},"outputs":[{"name":"stdout","text":"Writing /kaggle/working/mri-mvp/tests/test_smoke_serving.py\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"## 4. Model checkpoint (copy into working dir)\n\nUpdate the `/kaggle/input/...` path to match your checkpoint dataset.\n","metadata":{}},{"cell_type":"code","source":"# CELL: 08_CHECKPOINT\n# Point the MVP to the policy-aligned model bundle dataset (preferred)\n\nimport os\nfrom pathlib import Path\n\nMODEL_BUNDLE_DIR = \"/kaggle/input/datasets/kabomolefe/mri-model-bundle-v1\"\nassert Path(MODEL_BUNDLE_DIR).exists(), f\"Missing bundle dir: {MODEL_BUNDLE_DIR}. Check Inputs.\"\n\nos.environ[\"MODEL_REF\"] = MODEL_BUNDLE_DIR\nprint(\"✅ MODEL_REF set to:\", os.environ[\"MODEL_REF\"])\n\n# prove the expected artifacts exist\nneed = [\"best_model.pth\", \"temperature_scaling.json\", \"final_policy_config.json\", \"domain_guard_lr.npz\"]\nprint({n: (Path(MODEL_BUNDLE_DIR)/n).exists() for n in need})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:35:11.798479Z","iopub.execute_input":"2026-02-14T18:35:11.798733Z","iopub.status.idle":"2026-02-14T18:35:11.820278Z","shell.execute_reply.started":"2026-02-14T18:35:11.798710Z","shell.execute_reply":"2026-02-14T18:35:11.818851Z"}},"outputs":[{"name":"stdout","text":"✅ MODEL_REF set to: /kaggle/input/datasets/kabomolefe/mri-model-bundle-v1\n{'best_model.pth': True, 'temperature_scaling.json': True, 'final_policy_config.json': True, 'domain_guard_lr.npz': True}\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"## 5. Install package (editable)\n\nWe install from `/kaggle/working/mri-mvp` so `import mri` resolves cleanly.\n","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/mri-mvp/setup.py\n# CELL: 09_SETUP_PY\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"mri-mvp\",\n    version=\"0.0.0\",\n    package_dir={\"\": \"src\"},\n    packages=find_packages(\"src\"),\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:35:11.821783Z","iopub.execute_input":"2026-02-14T18:35:11.822235Z","iopub.status.idle":"2026-02-14T18:35:11.834422Z","shell.execute_reply.started":"2026-02-14T18:35:11.822190Z","shell.execute_reply":"2026-02-14T18:35:11.833241Z"}},"outputs":[{"name":"stdout","text":"Writing /kaggle/working/mri-mvp/setup.py\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# CELL: 10_PIP_INSTALL_EDITABLE\n%pip install -e /kaggle/working/mri-mvp -q\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:35:11.836056Z","iopub.execute_input":"2026-02-14T18:35:11.836456Z","iopub.status.idle":"2026-02-14T18:35:20.179392Z","shell.execute_reply.started":"2026-02-14T18:35:11.836403Z","shell.execute_reply":"2026-02-14T18:35:20.178368Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"### 5.1 Kaggle path bootstrap (belt-and-suspenders)\n\nThis helps if Kaggle import paths get weird after restarts.\n","metadata":{}},{"cell_type":"code","source":"# CELL: 11_PATH_BOOTSTRAP (required for Kaggle)\nimport site\n\nsite.addsitedir(\"/kaggle/working/mri-mvp/src\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:35:20.181155Z","iopub.execute_input":"2026-02-14T18:35:20.182397Z","iopub.status.idle":"2026-02-14T18:35:20.187312Z","shell.execute_reply.started":"2026-02-14T18:35:20.182358Z","shell.execute_reply":"2026-02-14T18:35:20.186350Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"## 6. Smoke tests (fast)\n\nThese catch syntax/import issues before launch.\n","metadata":{}},{"cell_type":"code","source":"# CELL: 12_PY_COMPILE\n!python -m py_compile /kaggle/working/mri-mvp/src/mri/application/inference_service.py\n!python -m py_compile /kaggle/working/mri-mvp/src/mri/serving/app.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:35:20.189101Z","iopub.execute_input":"2026-02-14T18:35:20.189471Z","iopub.status.idle":"2026-02-14T18:35:20.729958Z","shell.execute_reply.started":"2026-02-14T18:35:20.189446Z","shell.execute_reply":"2026-02-14T18:35:20.728764Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# CELL: 12_SMOKE_TEST (fast)\nimport os\nfrom pathlib import Path\n\n# Confirm MODEL_REF is set BEFORE importing serving.app\nprint(\"MODEL_REF:\", os.getenv(\"MODEL_REF\"))\nassert os.getenv(\"MODEL_REF\"), \"MODEL_REF not set. Run CELL 08_CHECKPOINT first.\"\n\n# Import package + app\nimport mri\nprint(\"OK: imported mri package\")\n\nfrom mri.serving.app import demo\nprint(\"OK: imported demo\")\n\n# Confirm bundle files exist\nbundle = Path(os.environ[\"MODEL_REF\"])\nprint(\"Bundle dir:\", bundle)\nprint(\"best_model.pth size MB:\", (bundle/\"best_model.pth\").stat().st_size/1e6)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:35:20.731829Z","iopub.execute_input":"2026-02-14T18:35:20.732336Z","iopub.status.idle":"2026-02-14T18:35:39.493324Z","shell.execute_reply.started":"2026-02-14T18:35:20.732300Z","shell.execute_reply":"2026-02-14T18:35:39.492173Z"}},"outputs":[{"name":"stdout","text":"MODEL_REF: /kaggle/input/datasets/kabomolefe/mri-model-bundle-v1\nOK: imported mri package\nnew /\nOK: imported demo\nBundle dir: /kaggle/input/datasets/kabomolefe/mri-model-bundle-v1\nbest_model.pth size MB: 44.794315\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# CELL: 13_PYTEST_SMOKE\n!pytest -q /kaggle/working/mri-mvp/tests\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:35:39.494601Z","iopub.execute_input":"2026-02-14T18:35:39.495096Z","iopub.status.idle":"2026-02-14T18:35:57.731612Z","shell.execute_reply.started":"2026-02-14T18:35:39.495049Z","shell.execute_reply":"2026-02-14T18:35:57.730337Z"}},"outputs":[{"name":"stdout","text":"\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                    [100%]\u001b[0m\n\u001b[32m\u001b[32m\u001b[1m5 passed\u001b[0m\u001b[32m in 12.73s\u001b[0m\u001b[0m\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"## 7. Launch public demo (last section)\n\n**Rule:** Run `Reset` then `Launch`. If you need a new link, reset first.\n","metadata":{}},{"cell_type":"markdown","source":"**# Run Order**\nSetup/install\n\n(Optional) tests\n\nReset\n\nLaunch (keep running)* ","metadata":{}},{"cell_type":"code","source":"# CELL: 99_RESET_GRADIO (CANONICAL)\n# CANONICAL RESET CELL\n# Run this ONCE after kernel restart and BEFORE launch.\n# Do NOT import demo here.\n\nimport gradio as gr\n\n# Safe global reset (do NOT import demo here)\ngr.close_all()\nprint(\"✅ reset done (gr.close_all)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:35:57.733475Z","iopub.execute_input":"2026-02-14T18:35:57.734044Z","iopub.status.idle":"2026-02-14T18:35:57.741591Z","shell.execute_reply.started":"2026-02-14T18:35:57.733982Z","shell.execute_reply":"2026-02-14T18:35:57.740442Z"}},"outputs":[{"name":"stdout","text":"✅ reset done (gr.close_all)\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# CELL: 100_LAUNCH_PUBLIC_DEMO (CANONICAL)\n# CANONICAL LAUNCH CELL\n# Run this AFTER reset. Keep it running while testing the public link.\n# If you restart kernel, you must rerun setup → reset → launch.\n\nimport time, requests, importlib, subprocess\nimport gradio as gr\nimport gradio.networking as net\nimport mri.serving.app as serving_app\n\n# Clean start BEFORE launch\ngr.close_all()\n\n# Fresh demo object each run (prevents relaunching a previously-closed instance)\nimportlib.reload(serving_app)\ndemo = serving_app.demo\n\n# Kaggle workaround: bound Gradio share readiness checks\n_orig_url_ok = net.url_ok\ndef _url_ok_with_timeout(url: str) -> bool:\n    t0 = time.time()\n    while time.time() - t0 < 8:\n        try:\n            r = requests.get(url, timeout=2)\n            if r.status_code in (200, 302, 401, 404):\n                return True\n        except Exception:\n            pass\n        time.sleep(0.5)\n    return True\n\nnet.url_ok = _url_ok_with_timeout\n\napp, local_url, share_url = demo.launch(\n    debug=True,\n    share=True,\n    inline=False,\n    prevent_thread_lock=True,\n    show_error=True,\n)\n\nprint(\"✅ LOCAL:\", local_url)\nprint(\"✅ PUBLIC:\", share_url)\n\n# Prove the server is actually listening (truth signal)\nprint(subprocess.run([\"bash\",\"-lc\",\"ss -ltnp | grep 7860 || true\"], capture_output=True, text=True).stdout)\n\n# Local health check\ntry:\n    r = requests.get(local_url, timeout=2)\n    print(\"✅ local health:\", r.status_code)\nexcept Exception as e:\n    print(\"❌ local health failed:\", repr(e))\n\nprint(\"\\nKeep this cell running while testing the PUBLIC link. Interrupt to stop the server/tunnel.\")\nwhile True:\n    time.sleep(60)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:35:57.743235Z","iopub.execute_input":"2026-02-14T18:35:57.743900Z","iopub.status.idle":"2026-02-14T19:23:10.577895Z","shell.execute_reply.started":"2026-02-14T18:35:57.743866Z","shell.execute_reply":"2026-02-14T19:23:10.575713Z"}},"outputs":[{"name":"stdout","text":"new /\n* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://6db1504fbd45bf8eac.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/gradio/queueing.py\", line 759, in process_events\n    response = await route_utils.call_process_api(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 354, in call_process_api\n    output = await app.get_blocks().process_api(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 2116, in process_api\n    result = await self.call_function(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 1623, in call_function\n    prediction = await anyio.to_thread.run_sync(  # type: ignore\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/anyio/to_thread.py\", line 63, in run_sync\n    return await get_async_backend().run_sync_in_worker_thread(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 2502, in run_sync_in_worker_thread\n    return await future\n           ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 986, in run\n    result = context.run(func, *args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/gradio/utils.py\", line 915, in wrapper\n    response = f(*args, **kwargs)\n               ^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/mri-mvp/src/mri/serving/app.py\", line 55, in run_inference\n    raise gr.Error(f\"No usable slices. Loader warnings: {meta.get('warnings', [])}\")\ngradio.exceptions.Error: \"No usable slices. Loader warnings: ['No files provided.']\"\nERROR:    Exception in ASGI application\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n    result = await app(  # type: ignore[func-returns-value]\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n    return await self.app(scope, receive, send)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/fastapi/applications.py\", line 1139, in __call__\n    await super().__call__(scope, receive, send)\n  File \"/usr/local/lib/python3.12/dist-packages/starlette/applications.py\", line 107, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 186, in __call__\n    raise exc\n  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 164, in __call__\n    await self.app(scope, receive, _send)\n  File \"/usr/local/lib/python3.12/dist-packages/gradio/brotli_middleware.py\", line 74, in __call__\n    return await self.app(scope, receive, send)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 888, in __call__\n    await self.simple_response(scope, receive, send, request_headers=headers)\n  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 904, in simple_response\n    await self.app(scope, receive, send)\n  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/exceptions.py\", line 63, in __call__\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/usr/local/lib/python3.12/dist-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n    await self.app(scope, receive, send)\n  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 716, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 736, in app\n    await route.handle(scope, receive, send)\n  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 290, in handle\n    await self.app(scope, receive, send)\n  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 119, in app\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 105, in app\n    response = await f(request)\n               ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 385, in app\n    raw_response = await run_endpoint_function(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 284, in run_endpoint_function\n    return await dependant.call(**values)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/gradio/routes.py\", line 1708, in upload_file\n    form = await multipart_parser.parse()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 742, in parse\n    async for chunk in self.stream:\n  File \"/usr/local/lib/python3.12/dist-packages/starlette/requests.py\", line 237, in stream\n    raise ClientDisconnect()\nstarlette.requests.ClientDisconnect\n","output_type":"stream"},{"name":"stdout","text":"Keyboard interruption in main thread... closing server.\nKilling tunnel 127.0.0.1:7860 <> https://6db1504fbd45bf8eac.gradio.live\n✅ LOCAL: http://127.0.0.1:7860/\n✅ PUBLIC: https://6db1504fbd45bf8eac.gradio.live\n\n❌ local health failed: ConnectionError(MaxRetryError('HTTPConnectionPool(host=\\'127.0.0.1\\', port=7860): Max retries exceeded with url: / (Caused by NewConnectionError(\"HTTPConnection(host=\\'127.0.0.1\\', port=7860): Failed to establish a new connection: [Errno 111] Connection refused\"))'))\n\nKeep this cell running while testing the PUBLIC link. Interrupt to stop the server/tunnel.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/2696617753.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nKeep this cell running while testing the PUBLIC link. Interrupt to stop the server/tunnel.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":26},{"cell_type":"markdown","source":"## 8. Verification checklist\n\n✅ Multi-image JPEG upload (6+ slices)\n\n✅ DICOM.zip upload (works)\n\n✅ Nested/no-extension zip (works)\n\n⚠️ Non-MRI screenshots → should ideally be rejected/uncertain (known limitation until we add stronger input validity gating)\n","metadata":{}},{"cell_type":"markdown","source":"****Run cells top → bottom once\n\nAfter install cell, restart kernel if prompted\n\nAlways run RESET before LAUNCH\n\nPublic demo uses share=True, inline=False\n\nStopping launch cell kills the tunnel (404 is expected)","metadata":{}},{"cell_type":"code","source":"!python - << 'PY'\nimport json\nnb = json.load(open(\"/kaggle/working/mvp-notebook.ipynb\",\"r\",encoding=\"utf-8\"))\ncode_cells = [c for c in nb[\"cells\"] if c.get(\"cell_type\")==\"code\"]\nprint(\"code_cells:\", len(code_cells))\nPY\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T19:23:10.578821Z","iopub.status.idle":"2026-02-14T19:23:10.579345Z","shell.execute_reply.started":"2026-02-14T19:23:10.579099Z","shell.execute_reply":"2026-02-14T19:23:10.579129Z"}},"outputs":[],"execution_count":null}]}